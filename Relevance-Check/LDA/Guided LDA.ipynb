{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidedlda\n",
    "import numpy as np\n",
    "import os, sys, pickle\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [\n",
    "    '1',\n",
    "    '2',\n",
    "    '3'\n",
    "]\n",
    "policies = [\n",
    "    'aadhar',\n",
    "    'farmers',\n",
    "    'demon',\n",
    "    'gst'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_aadhar = ['Aadhaar', 'UIDAI', 'unique identity number', 'UID', \\\n",
    "            'unique Aadhaar number', 'Unique Identification Authority', \\\n",
    "            'Adhar', 'Aadhar', 'Adhaar', 'Adharcard', 'Aadharcard', \\\n",
    "            'Aadhaarcard', 'Aadhar Card','Aadhar', 'Aadhaar', 'Adhar',\\\n",
    "            'Adharcard', 'Aadharcard', 'Aadhaarcard', 'UIDAI', 'Aadhar Card']\n",
    "\n",
    "keywords_farmers = ['loan waiver', 'loan waivers', 'farmer loan', 'farmer suicide','farmer suicides',\\\n",
    "                    'pest infestation', 'farmer loans','drought','farmer', 'farmers', 'crop insurance',\\\n",
    "                    'Swaminathan Commission', 'National Commission on Farmer', 'kisan', 'agriculture',\\\n",
    "                    'monsoon failure', 'crop failure', 'fertilizers', 'Seeds Corporation', 'crop loss',\\\n",
    "                    'crop losses', 'unseasonal rains', 'irrigation facilities', 'debt traps',\\\n",
    "                   'loan waiver', 'farmer loan', 'farmer suicide', 'pest infestation', 'Swaminathan Commission',\\\n",
    "                    'National Commission on Farmer','kisan', 'monsoon failure', 'crop failure',\\\n",
    "                    'fertilizers', 'Seeds Corporation', 'farmer', 'agricultural']\n",
    "\n",
    "keywords_demon = ['Rs 1,000 notes', 'Rs 500 notes', 'lower denomination', 'Rs 500 and Rs 1,000 notes',\\\n",
    "                 'demonetisation', 'denomination note', 'cash withdrawal', 'swipe machine', 'unaccounted money',\\\n",
    "                 'withdrawal limit', 'black money', 'long queue', 'cashless transaction', 'cashless economy',\\\n",
    "                 'demonitis', 'demonitiz', 'swipe machine', 'pos machine', 'fake currency', 'digital payment',\\\n",
    "                 'digital transaction', 'cash transaction', 'cashless economy', 'cash crunch', 'currency switch'\\\n",
    "                 , 'demonetised note', 'cashless transaction', 'note ban', 'currency switch','ATMs', 'now-defunct currency',\\\n",
    "                 'demonitis', 'demonitiz', 'denomination note', 'cash withdrawal', 'swipe machine', 'unaccounted money', 'withdrawal limit', \\\n",
    "                  'pos machine', 'fake currency', 'digital payment', 'digital transaction', 'cash transaction', 'cashless economy',\\\n",
    "                  'black money', 'cash crunch', 'currency switch', 'long queue', 'demonetised note',\\\n",
    "                  'cashless transaction', 'note ban', 'currency switch', 'demonetis', 'demonetiz']\n",
    "\n",
    "keywords_gst = ['GST', 'Goods and Services Tax', 'Goods & Services Tax', 'excise duty',\\\n",
    "                'good and service tax', 'tax reform', 'goods and services tax', 'gst', 'taxpayers',\\\n",
    "               'GST', 'Goods and Services Tax', 'Goods & Services Tax', 'excise duty']\n",
    "\n",
    "keywords_tech = ['privacy', 'cashless', 'technology', 'technological', 'innovation', 'software', 'engineering', 'high technology',\\\n",
    "            'technical', 'tech']\n",
    "\n",
    "policies_dict = {\"aadhar\":keywords_aadhar, \"demon\":keywords_demon, \"farmers\":keywords_farmers, \"gst\":keywords_gst, \"tech\":keywords_tech}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = './guided-lda-count-vectors'\n",
    "curr_policy_idx = 3\n",
    "curr_vector_type = 2\n",
    "print(policies[curr_policy_idx], curr_vector_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Document-Term Matrix & Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector(count, policy):\n",
    "    file = open(folder_name + '/' + policy + '_X_' + str(count) + '_count.pickle', 'rb')\n",
    "    X = pickle.load(file)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectorizer(count, policy):\n",
    "    file = open(folder_name + '/' + policy + '_vectorizer_' + str(count) + '_count.pickle', 'rb')\n",
    "    X = pickle.load(file)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_vector(curr_vector_type, policies[curr_policy_idx]).toarray()\n",
    "vectorizer = load_vectorizer(curr_vector_type, policies[curr_policy_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.vocabulary_\n",
    "word2id = {}\n",
    "for v in vocab:\n",
    "    word2id[vocab[v]] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_normal = guidedlda.GuidedLDA(n_topics=2, n_iter=100, random_state=7, refresh=20)\n",
    "doc_topic_normal = model_normal.fit_transform(X)\n",
    "for i in range(20):\n",
    "    print(\"top topic: {} Document: {}\".format(\n",
    "            doc_topic_normal[i].argmax(), \n",
    "            ', '.join([word2id[v] for v in list(reversed(X[i,:].argsort()))[0:5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topics = {}\n",
    "for keyword in policies_dict[policies[curr_policy_idx].lower()]:\n",
    "    seed_topics[keyword.lower()] = 0\n",
    "    seed_topics[keyword] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_guided = guidedlda.GuidedLDA(n_topics=2, n_iter=100, random_state=7, refresh=20)\n",
    "model_guided.fit(X, seed_topics=seed_topics, seed_confidence=0.15)\n",
    "doc_topic_guided = model_guided.transform(X)\n",
    "for i in range(20):\n",
    "    print(\"top topic: {} Document: {}\".format(\n",
    "            doc_topic_guided[i].argmax(), \n",
    "            ', '.join([word2id[v] for v in list(reversed(X[i,:].argsort()))[0:5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Guided LDA v/s Normal LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_policy_data(policy):\n",
    "    file = open('./actual-tags/' + policy + '_df.pickle', 'rb')\n",
    "    X = pickle.load(file)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_labels = load_policy_data(policies[curr_policy_idx])['label']\n",
    "y_pred_normal = [doc_topic_normal[i].argmax() for i in range(doc_topic_normal.shape[0])]\n",
    "y_pred_guided = [doc_topic_guided[i].argmax() for i in range(doc_topic_guided.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Normal: {}, Guided: {}'.format(accuracy_score(actual_labels, y_pred_normal), accuracy_score(actual_labels, y_pred_guided)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(policies[curr_policy_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
