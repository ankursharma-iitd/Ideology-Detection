{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import re\n",
    "from spacy.pipeline import merge_entities\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.add_pipe(merge_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = './top-300/'\n",
    "my_dataset = base + 'all.txt'\n",
    "deepak_dataset = base + 'combined-raw-data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(keywords):\n",
    "    keywords_hypen = [x.replace(\" \", \"-\") for x in keywords]\n",
    "    total_keywords = list(set(keywords + keywords_hypen))\n",
    "    return total_keywords\n",
    "\n",
    "def is_phrase_in(phrase, text):\n",
    "    return re.search(r\"\\b{}(s|es)?\\b\".format(phrase), text, re.IGNORECASE) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_aadhar = ['Aadhaar', 'UIDAI', 'unique identity number', 'UID', \\\n",
    "                'unique Aadhaar number', 'Unique Identification', \\\n",
    "                'Adhar', 'Aadhar', 'Adhaar', 'Adharcard', 'Aadharcard', \\\n",
    "                'Aadhaarcard', 'Aadhar Card','Aadhar', 'Aadhaar', 'Adhar',\\\n",
    "                'Adharcard', 'Aadharcard', 'Aadhaarcard', 'UIDAI', 'Aadhar Card', \\\n",
    "                '12-digit', 'Enrolment ID', 'Enrollment ID', 'Enrolment Number', \\\n",
    "                'Enrollment Number', 'Personal Identification Number', 'E-Aadhaar', 'UID/EID', \\\n",
    "                'E-Adhar', 'E-Aadhar', 'E-Adhaar', 'Pankaj Kumar', 'uidai.gov.in', \\\n",
    "                'Aadhaar-link', 'Adhaar-link', 'Aadhar-link', 'Adhar-link', \\\n",
    "                'Aadhaar-enable', 'Adhaar-enable', 'Aadhar-enable', 'Adhar-enable'\n",
    "                ]\n",
    "\n",
    "keywords_digital_india = ['Digital India', 'Digital Swades', 'india digital', 'digit india', \\\n",
    "                          'digital desh', 'make india', 'digital divide', 'digital payment', 'free wifi service', \\\n",
    "                          'digital locker', 'digital transaction', 'wifi hotspot', 'budget cybersecurity', \\\n",
    "                          'internet connectivity', 'smart city', 'digital business', 'bharatnet project', \n",
    "                          'digital present', 'Bharat net', 'digitalised', 'digitalized', 'make in india', \\\n",
    "                         'connectivity', 'digital', 'free wifi']\n",
    "\n",
    "keywords_cashless = ['cashless', 'digital payment', 'mobikwik','Unified Payment interface','UPI','online transfer',\\\n",
    "                    'SBI pay','ICICI pocket','Payzapp','Paytm','freecharge','ewallet','mobile wallet',\\\n",
    "                     'internet banking','net banking','mobile banking ','PhonePe','physical-POS',\\\n",
    "                     'M-POS','V-POS','digital transaction','pos machine','swipe machine','digital wallet',\\\n",
    "                    'digital economy','card payment','BHIM','banking transaction','swiping machine','payment gateway', \\\n",
    "                    'm pos', 'v pos', 'physical pos']\n",
    "\n",
    "keywords_egov = ['e-governance', 'information and communication technology', 'e-govt','egovernment',\\\n",
    "                'electronic governance','paperless office','communication','technology','ict academy',\\\n",
    "                 'ict sector','ict information','ict tool','e-district','m-governance', 'e district', \\\n",
    "                'e governance', 'e govt', 'm governance', 'egov', 'e gov', 'information technology', \\\n",
    "                ]\n",
    "\n",
    "policies = {\"aadhar\":process(keywords_aadhar), \"digital_india\":process(keywords_digital_india), \"cashless\":process(keywords_cashless), \"egov\":process(keywords_egov)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_aadhar = ['Aadhaar', 'UIDAI', 'unique identity number', 'UID', \\\n",
    "            'unique Aadhaar number', 'Unique Identification', \\\n",
    "            'Adhar', 'Aadhar', 'Adhaar', 'Adharcard', 'Aadharcard', \\\n",
    "            'Aadhaarcard', 'Aadhar Card','Aadhar', 'Aadhaar', 'Adhar',\\\n",
    "            'Adharcard', 'Aadharcard', 'Aadhaarcard', 'UIDAI', 'Aadhar Card', \\\n",
    "            '12-digit', 'Enrolment ID', 'Enrollment ID', 'Enrolment Number', \\\n",
    "            'Enrollment Number', 'Personal Identification Number', 'E-Aadhaar', 'UID/EID', \\\n",
    "            'E-Adhar', 'E-Aadhar', 'E-Adhaar', 'Pankaj Kumar', 'uidai.gov.in', \\\n",
    "            'Aadhaar-link', 'Adhaar-link', 'Aadhar-link', 'Adhar-link', \\\n",
    "            'Aadhaar-enable', 'Adhaar-enable', 'Aadhar-enable', 'Adhar-enable'\n",
    "            ]\n",
    "\n",
    "keywords_farmers = ['loan waiver', 'loan waivers', 'farmer loan', 'farmer suicide','farmer suicides',\\\n",
    "                    'pest infestation', 'farmer loans','drought','farmer', 'farmers', 'crop insurance',\\\n",
    "                    'Swaminathan Commission', 'National Commission on Farmer', 'kisan', 'agriculture',\\\n",
    "                    'monsoon failure', 'crop failure', 'fertilizers', 'Seeds Corporation', 'crop loss',\\\n",
    "                    'crop losses', 'unseasonal rains', 'irrigation facilities', 'debt traps',\\\n",
    "                   'loan waiver', 'farmer loan', 'farmer suicide', 'pest infestation', 'Swaminathan Commission',\\\n",
    "                    'National Commission on Farmer','kisan', 'monsoon failure', 'crop failure',\\\n",
    "                    'fertilizer', 'Seeds Corporation', 'agricultural', 'crop prices', \\\n",
    "                   'agrarian', 'NCSTC', 'farming community', 'debt waiver', 'waiver scheme', 'farm loan', \\\n",
    "                   'crop loan', 'farmer suicide', 'farmers suicide', 'farmer agitation', 'plight farmer', \\\n",
    "                   'distressed farmer', 'farmer issue', 'farmers\\' protest', 'farmer protest', 'agrarian crisis', \\\n",
    "                   'agrarian unrest', 'agriculture protest', 'farmers\\' march', 'farmers march', 'farmer march']\n",
    "\n",
    "keywords_demon = ['Rs 1,000 notes', 'Rs 500 notes', 'lower denomination', 'Rs 500 and Rs 1,000 notes',\\\n",
    "                 'demonetisation', 'denomination note', 'cash withdrawal', 'swipe machine', 'unaccounted money',\\\n",
    "                 'withdrawal limit', 'black money', 'long queue', 'cashless transaction', 'cashless economy',\\\n",
    "                 'demonitis', 'demonitiz', 'swipe machine', 'pos machine', 'fake currency', 'digital payment',\\\n",
    "                 'digital transaction', 'cash transaction', 'cashless economy', 'cash crunch', 'currency switch'\\\n",
    "                 , 'demonetised note', 'cashless transaction', 'note ban', 'currency switch','ATMs', 'now-defunct currency',\\\n",
    "                 'demonitis', 'demonitiz', 'denomination note', 'cash withdrawal', 'swipe machine', 'unaccounted money', 'withdrawal limit', \\\n",
    "                  'pos machine', 'fake currency', 'digital payment', 'digital transaction', 'cash transaction', 'cashless economy',\\\n",
    "                  'black money', 'cash crunch', 'currency switch', 'long queue', 'demonetised note',\\\n",
    "                  'cashless transaction', 'note ban', 'currency switch', 'demonetis', 'demonetiz', 'denomination note']\n",
    "\n",
    "keywords_gst = ['GST', 'Goods and Services Tax', 'Goods & Services Tax', 'Excise Duty',\\\n",
    "                'good and service tax', 'tax reform', 'goods and services tax', 'gst', 'taxpayers',\\\n",
    "               'GST', 'Goods and Services Tax', 'Goods & Services Tax', 'excise duty', \\\n",
    "               'GSTIN', 'CGST', 'SGST', 'IGST', 'Reverse Charge', 'GSTR', 'GSP', 'Suvidha Provider', \\\n",
    "               'HSN', 'gabbar singh tax', 'goods service tax', 'good service tax']\n",
    "\n",
    "keywords_tech = ['Privacy', 'Cashless', 'Technology', 'Technological', 'Innovation', 'Software', 'Engineering', 'High Technology',\\\n",
    "            'Technical', 'Tech', 'Personal Data Protection', 'Big Data', 'Artificial Intelligence', 'Digital India', \\\n",
    "                'high speed internet', 'make in india', 'e governance', 'umang', 'digital literacy', 'national policy on electronics', \\\n",
    "                'npe', 'e gadget', 'entrepreneur', 'startup', 'tech', 'scientific', 'science', 'technologies', 'technical', 'cashless', \\\n",
    "                'technology-based', 'smart city']\n",
    "\n",
    "policies = {\"aadhar\":process(keywords_aadhar), \"demon\":process(keywords_demon), \"farmers\":process(keywords_farmers), \"gst\":process(keywords_gst), \"tech\":process(keywords_tech)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "with open(my_dataset, 'r') as file:\n",
    "    for line in file:\n",
    "        sentence = line.strip().lower()\n",
    "        if(len(sentence) > 0):\n",
    "            all_sentences.append(sentence)\n",
    "print('Num sentences in my dataset: ', len(all_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(deepak_dataset)\n",
    "deepak_sentences = list(set(df['statement']))\n",
    "print('Num sentences in Deepak dataset: ', len(deepak_sentences))\n",
    "for sentence in deepak_sentences:\n",
    "    refined_sentence = sentence.strip().lower()\n",
    "    if(len(refined_sentence) > 0):\n",
    "        all_sentences.append(refined_sentence)\n",
    "all_sentences = list(set(all_sentences))\n",
    "print('Num sentences in final dataset: ', len(all_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open(base + 'new-refined-all-by-stmnts.txt','w')\n",
    "for line in all_sentences:\n",
    "    output.write(line + '\\n')\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_policy(sent, policies):\n",
    "    p = []\n",
    "    words = []\n",
    "    for policy, keywords in policies.items():\n",
    "        for word in keywords:\n",
    "            if is_phrase_in(word, sent):\n",
    "                words.append(word)\n",
    "                p.append(policy)\n",
    "                break\n",
    "    if len(p)>1:\n",
    "        print(p)\n",
    "    return p, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open(base + 'new-out-all-by-stmnts.txt','w')\n",
    "with open(base + 'new-refined-all-by-stmnts.txt','r') as file:\n",
    "    for line in file:\n",
    "        p, words = check_policy(line, policies)\n",
    "        p = \" \".join(p)\n",
    "        words = \" \".join(words)\n",
    "        output.write(p+\" -- \"+words+\"\\n\")\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2): \n",
    "    lst3 = [value for value in lst1 if value in lst2] \n",
    "    return lst3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aadhar_lines = []\n",
    "digital_india_lines = []\n",
    "cashless_lines = []\n",
    "egov_lines = []\n",
    "with open(base + \"new-refined-all-by-stmnts.txt\",'r') as file_s, open(base + \"new-out-all-by-stmnts.txt\",'r') as file_o: \n",
    "    for x, y in zip(file_s, file_o):\n",
    "        y = y.strip().split(\" \")\n",
    "        if len(y)>1:\n",
    "            print(y)\n",
    "        for policy in y:\n",
    "            if policy==\"aadhar\":\n",
    "                aadhar_lines.append(x)\n",
    "            if policy==\"digital_india\":\n",
    "                digital_india_lines.append(x)\n",
    "            if policy==\"cashless\":\n",
    "                cashless_lines.append(x)\n",
    "            if policy==\"egov\":\n",
    "                egov_lines.append(x)\n",
    "aadhar_lines = list(set(aadhar_lines))\n",
    "digital_india_lines = list(set(digital_india_lines))\n",
    "cashless_lines = list(set(cashless_lines))\n",
    "egov_lines = list(set(egov_lines))\n",
    "print(len(aadhar_lines), len(digital_india_lines), len(cashless_lines), len(egov_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_a = open(base + 'aadhar_all.txt','w')\n",
    "file_d = open(base + 'digital_india_all.txt','w')\n",
    "file_c = open(base + 'cashless_all.txt','w')\n",
    "file_e = open(base + 'egov_all.txt','w')\n",
    "for line in aadhar_lines:\n",
    "    file_a.write(line)\n",
    "for line in digital_india_lines:\n",
    "    file_d.write(line)\n",
    "for line in cashless_lines:\n",
    "    file_c.write(line)\n",
    "for line in egov_lines:\n",
    "    file_e.write(line)\n",
    "file_a.close()\n",
    "file_d.close()\n",
    "file_c.close()\n",
    "file_e.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aadhar_lines = []\n",
    "demon_lines = []\n",
    "farmers_lines = []\n",
    "gst_lines = []\n",
    "tech_lines = []\n",
    "with open(base + \"refined-all-by-stmnts.txt\",'r') as file_s, open(base + \"out-all-by-stmnts.txt\",'r') as file_o: \n",
    "    for x, y in zip(file_s, file_o):\n",
    "        y = y.strip().split(\" \")\n",
    "        if len(y)>1:\n",
    "            print(y)\n",
    "        for policy in y:\n",
    "            if policy==\"aadhar\":\n",
    "                aadhar_lines.append(x)\n",
    "            if policy==\"farmers\":\n",
    "                farmers_lines.append(x)\n",
    "            if policy==\"demon\":\n",
    "                demon_lines.append(x)\n",
    "            if policy==\"tech\":\n",
    "                tech_lines.append(x)\n",
    "            if policy==\"gst\":\n",
    "                gst_lines.append(x)\n",
    "aadhar_lines = list(set(aadhar_lines))\n",
    "farmers_lines = list(set(farmers_lines))\n",
    "demon_lines = list(set(demon_lines))\n",
    "tech_lines = list(set(tech_lines))\n",
    "gst_lines = list(set(gst_lines))\n",
    "print(len(aadhar_lines), len(farmers_lines), len(demon_lines), len(tech_lines), len(gst_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_a = open(base + 'aadhar_all.txt','w')\n",
    "file_d = open(base + 'demon_all.txt','w')\n",
    "file_f = open(base + 'farmers_all.txt','w')\n",
    "file_g = open(base + 'gst_all.txt','w')\n",
    "file_t = open(base + 'tech_all.txt','w')\n",
    "for line in aadhar_lines:\n",
    "    file_a.write(line)\n",
    "for line in demon_lines:\n",
    "    file_d.write(line)\n",
    "for line in tech_lines:\n",
    "    file_t.write(line)\n",
    "for line in gst_lines:\n",
    "    file_g.write(line)\n",
    "for line in farmers_lines:\n",
    "    file_f.write(line)\n",
    "file_a.close()\n",
    "file_d.close()\n",
    "file_f.close()\n",
    "file_g.close()\n",
    "file_t.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using pobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5 # this indicates policy -- 0 -> aadhar, 1 -> demon, 2 -> farmers, 3 -> gst, 4 -> tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "nlp.add_pipe(merge_entities)\n",
    "\n",
    "def is_relevant(sent, keywords):\n",
    "    doc = nlp(sent)\n",
    "    for chunk in (doc.noun_chunks):\n",
    "        if chunk.root.dep_!='nsubj' and chunk.root.dep_!='nsubjpass':\n",
    "            continue\n",
    "        for word in keywords:\n",
    "            if is_phrase_in(word, chunk.text):\n",
    "                return True\n",
    "    return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process2(keywords):\n",
    "    keywords_small = [x.lower() for x in keywords]\n",
    "    keywords_big = [x.upper() for x in keywords]\n",
    "    keywords_hypen = [x.replace(\" \", \"-\") for x in keywords]\n",
    "    total_keywords = list(set(keywords + keywords_hypen + keywords_small + keywords_big))\n",
    "    return total_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['aadhar_all.txt', 'demon_all.txt', 'farmers_all.txt', 'gst_all.txt','tech_all.txt']\n",
    "f = open(base + files[idx], 'r+')\n",
    "dicti = {files[0]:keywords_aadhar, \n",
    "         files[1]:keywords_demon, \n",
    "         files[2]:keywords_farmers, \n",
    "         files[3]:keywords_gst,\n",
    "         files[4]:keywords_tech}\n",
    "print(files[idx])\n",
    "rel_count = 0\n",
    "total_count = 0\n",
    "total_keywords = process2(dicti[files[idx]])\n",
    "print(total_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = open(base + 'pobj_relevant_' + files[idx], 'w+')\n",
    "for line in f:\n",
    "    total_count += 1\n",
    "    if(is_relevant_by_pobj(line.strip(), total_keywords)):\n",
    "        rel_count += 1\n",
    "        f2.write(line)\n",
    "f2.close()\n",
    "print(rel_count, total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Supervised Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3 # this indicates policy -- 0 -> aadhar, 1 -> demon, 2 -> farmers, 3 -> gst, 4 -> tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_into_string(filename):\n",
    "    text_file = open(filename, 'r')\n",
    "    lines = []\n",
    "    for line in text_file:\n",
    "        sentence = line.strip().lower()\n",
    "        if(len(sentence) > 0):\n",
    "            lines.append(line)\n",
    "    text_file.close()\n",
    "    return lines\n",
    "print(files[idx])\n",
    "dataset = read_into_string(base + files[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_results(X_test, text_clf):\n",
    "    predicted = text_clf.predict(X_test)\n",
    "    f = open(base + 'supervised_relevant_' + files[idx], 'w+')\n",
    "    for sentence, prediction in zip(X_test, predicted):\n",
    "        if(prediction == 1):\n",
    "            f.write(sentence)\n",
    "    f.close()\n",
    "    print('relevant statements: {}, Total statements: {}'.format(sum(predicted), len(X_test))) # relevant-1, non-relevant-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_results(dataset, pkl.load(open(base + files[idx][:-8] + '-model.pkl', 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = ['aadhar', 'demon', 'farmers', 'gst', 'tech']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cv2] *",
   "language": "python",
   "name": "conda-env-cv2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
