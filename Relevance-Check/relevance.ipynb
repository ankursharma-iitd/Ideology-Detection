{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "# from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-95ef6260efea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge_entities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_entities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "from spacy.pipeline import merge_entities\n",
    "nlp = spacy.load(\"en\")\n",
    "nlp.add_pipe(merge_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = ['non_relevant_aadhar.txt',\n",
    "#          'relevant_aadhar.txt',\n",
    "#         'non_relevant_demon.txt',\n",
    "#           'relevant_demon.txt',\n",
    "#         'non_relevant_farmers.txt',\n",
    "#          'relevant_farmers.txt',\n",
    "#         'non_relevant_gst.txt',\n",
    "#         'relevant_gst.txt']\n",
    "files = ['non_relevant_gst.txt',\n",
    "         'relevant_gst.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_entity(sent):\n",
    "    doc = nlp(sent)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_=='PERSON':\n",
    "#             sent = sent.replace(ent.text,ent.label_)\n",
    "            sent = sent.replace(ent.text, \"\")\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_sents = []\n",
    "labels = []\n",
    "for i, file in enumerate(files):\n",
    "    with open(file,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        file_sents+=[l.strip() for l in lines]\n",
    "        labels+=[i]*len(lines)\n",
    "\n",
    "# for file in files:\n",
    "#     with open(file, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "#         file_sents.append(\"\\n\".join(lines))\n",
    "\n",
    "# df = pd.DataFrame({\"file\":files,\"text\":file_sents})\n",
    "df = pd.DataFrame({\"text\":file_sents, \"label\":labels})\n",
    "# Remove punctuation\n",
    "df['text_processed'] = df['text'].map(lambda x: re.sub('[,\\.!?|;]', ' ', x))\n",
    "df['text_processed'] = df['text_processed'].map(lambda x: re.sub('[0-9]', ' ', x))\n",
    "# remove person entity\n",
    "df['text_processed'] = df['text_processed'].map(lambda x: replace_entity(x))\n",
    "# Convert the titles to lowercase\n",
    "df['text_processed'] = df['text_processed'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the different processed titles together.\n",
    "long_string = ','.join(list(df['text_processed'].values))\n",
    "# print(df['file'][7])\n",
    "# long_string = df['text_processed'][7]\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helper function\n",
    "# def plot_10_most_common_words(count_data, count_vectorizer):\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     words = count_vectorizer.get_feature_names()\n",
    "#     total_counts = np.zeros(len(words))\n",
    "#     for t in count_data:\n",
    "#         total_counts+=t.toarray()[0]\n",
    "    \n",
    "#     count_dict = (zip(words, total_counts))\n",
    "#     count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
    "#     words = [w[0] for w in count_dict]\n",
    "#     counts = [w[1] for w in count_dict]\n",
    "#     x_pos = np.arange(len(words)) \n",
    "    \n",
    "#     plt.figure(2, figsize=(15, 15/1.6180))\n",
    "#     plt.subplot(title='10 most common words')\n",
    "#     sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "#     sns.barplot(x_pos, counts, palette='husl')\n",
    "#     plt.xticks(x_pos, words, rotation=90) \n",
    "#     plt.xlabel('words')\n",
    "#     plt.ylabel('counts')\n",
    "#     plt.show()\n",
    "\n",
    "# Initialise the count vectorizer with the English stop words\n",
    "# count_vectorizer = CountVectorizer()\n",
    "count_vectorizer = TfidfVectorizer(ngram_range=(1,1), stop_words=set(stopwords.words('english')))\n",
    "# Fit and transform the processed titles\n",
    "count_data = count_vectorizer.fit_transform(df['text_processed'])\n",
    "\n",
    "# Visualise the 10 most common words\n",
    "# plot_10_most_common_words(count_data, count_vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# Tweak the two parameters below (use int values below 15)\n",
    "number_topics = 2\n",
    "number_words = 10\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(count_data)\n",
    "\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)\n",
    "\n",
    "# kmeans = KMeans(n_clusters=2)\n",
    "# kmeans.fit(count_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    pred = lda.transform(count_vectorizer.transform([df['text_processed'][i]])).tolist()[0]\n",
    "    topic = pred.index(max(pred))\n",
    "    print(topic, df['label'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lda, open('lda_gst_model.pickle', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump({\"relevant\":1, \"non-relevant\":0}, open('lda_gst_dict.pickle','wb'))\n",
    "pickle.dump(count_vectorizer, open('lda_gst_vectorizer.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = kmeans.predict(count_data)\n",
    "# clf = SVC()\n",
    "clf = BernoulliNB()\n",
    "clf.fit(count_data, df['label'])\n",
    "y_pred = clf.predict(count_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(count_data,df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(df['label'],y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = count_data.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2D = TSNE(n_components=2).fit_transform(x)\n",
    "# pca = PCA(n_components=2).fit(x)\n",
    "# data2D = pca.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centers2D = pca.transform(kmeans.cluster_centers_)\n",
    "# plt.hold(True)\n",
    "plt.scatter(data2D[:,0], data2D[:,1], c=df.label, edgecolor='k')\n",
    "# plt.scatter(centers2D[:,0], centers2D[:,1], \n",
    "#             marker='x', s=200, linewidths=3, c='b')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data2D[:,0], data2D[:,1], c=y_pred, edgecolor='k')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i,y in enumerate(y_pred):\n",
    "    if y==1:\n",
    "        print(df['text_processed'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from pyLDAvis import sklearn as sklearn_lda\n",
    "# import pickle \n",
    "# import pyLDAvis\n",
    "# import os\n",
    "\n",
    "# # Visualize the topics\n",
    "# pyLDAvis.enable_notebook()\n",
    "\n",
    "# LDAvis_data_filepath = os.path.join('./ldavis_prepared_'+str(number_topics))\n",
    "# # # this is a bit time consuming - make the if statement True\n",
    "# # # if you want to execute visualization prep yourself\n",
    "# if 1 == 1:\n",
    "\n",
    "#     LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)\n",
    "\n",
    "#     with open(LDAvis_data_filepath, 'w') as f:\n",
    "#         pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "# # load the pre-prepared pyLDAvis data from disk\n",
    "# with open(LDAvis_data_filepath) as f:\n",
    "#     LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "# pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(number_topics) +'.html')\n",
    "\n",
    "# LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
