{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re, string, os, itertools\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import subprocess\n",
    "import shlex\n",
    "from pymongo import MongoClient\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolved_entity_table = 'entities_resolved_overall'\n",
    "entity_name = 'Reetika Khera'\n",
    "article_table = 'articles'\n",
    "res_folder = './RESULTS/'\n",
    "directory3 = os.path.dirname(res_folder)\n",
    "if not os.path.exists(directory3):\n",
    "\tos.makedirs(directory3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Method to parse all the entities\n",
    "'''\n",
    "def get_all_entities(collection, types):\n",
    "    pipeline = [{\"$project\":{\"stdName\":1,\"type\":1,\"aliases\":1,\"articleIds\":1,\"num\":{\"$size\":\"$articleIds\"}}}]\n",
    "    cursor = list(collection.aggregate(pipeline))\n",
    "    top_n_entities = {}\n",
    "    entities = {type:[] for type in types}\n",
    "    for ent in cursor:\n",
    "        if(ent['type'] in types):\n",
    "            entities[ent['type']].append(ent)\n",
    "\n",
    "    for type in entities.keys():\n",
    "        entities[type].sort(key=lambda x: x['num'], reverse=True)\n",
    "        top_n_entities[type] = [{\"name\":obj['stdName'],\"coverage\":obj['num'],\"aliases\":obj['aliases'],\"articleIds\":obj['articleIds']} for obj in entities[type]]\n",
    "    return top_n_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Configurations required for By-Statement Extraction\n",
    "'''\n",
    "#do not change the order\n",
    "requiredCategs = ['FRONT_PAGE', 'REGIONAL_NEWS','NATIONAL_NEWS', 'INTERNATIONAL_NEWS', 'SPORTS', 'BUSINESS', 'OPINION']               \n",
    "                \n",
    "mongoConfigs = {\n",
    "'host':'10.237.26.159',\n",
    "'port':27017,\n",
    "'db':'media-db'\n",
    "}\n",
    "\n",
    "esConfigs = {\n",
    "'host':'10.237.26.25',\n",
    "'port':9200,\n",
    "'db':'media-db'\n",
    "}\n",
    "\n",
    "proxy_server='https://act4d.iitd.ernet.in:3128'\n",
    "#proxy_server='https://10.10.78.62:3128'\n",
    "\n",
    "entity_types = ['Person']\n",
    "short_sources_list = [\"Hindu\", \"TOI\", \"HT\", \"IE\", \"DecH\", \"Telegraph\", \"NIE\"]\n",
    "sources_list = [\"The Hindu\", \"The Times Of India\", \"Hindustan Times\", \"Indian Express\", \"Deccan Herald\", \"Telegraph\", \"The New Indian Express\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractSentences:\n",
    "    \n",
    "    caps = \"([A-Z])\"\n",
    "    prefixes = \"(Mr|St|Mrs|Ms|Dr|Rs)[.]\"\n",
    "    suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "    starters = \"(Rs|Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "    acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "    websites = \"[.](com|net|org|io|gov)\"\n",
    "    \n",
    "    def split_into_sentences(self,text):\n",
    "        text = \" \" + text + \"  \"\n",
    "        text = text.replace(\"\\n\",\" \")\n",
    "        text = re.sub(self.prefixes,\"\\\\1<prd>\",text)\n",
    "        text = re.sub(self.websites,\"<prd>\\\\1\",text)\n",
    "        if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "        text = re.sub(\"\\s\" + self.caps + \"[.] \",\" \\\\1<prd> \",text)\n",
    "        text = re.sub(self.acronyms+\" \"+self.starters,\"\\\\1<stop> \\\\2\",text)\n",
    "        text = re.sub(self.caps + \"[.]\" + self.caps + \"[.]\" + self.caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "        text = re.sub(self.caps + \"[.]\" + self.caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "        text = re.sub(\" \"+self.suffixes+\"[.] \"+self.starters,\" \\\\1<stop> \\\\2\",text)\n",
    "        text = re.sub(\" \"+self.suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "        text = re.sub(\" \" + self.caps + \"[.]\",\" \\\\1<prd>\",text)\n",
    "        if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "        if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "        if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "        text = text.replace(\".\",\".<stop>\")\n",
    "        text = text.replace(\"?\",\"?<stop>\")\n",
    "        text = text.replace(\"!\",\"!<stop>\")\n",
    "        text = text.replace(\"<prd>\",\".\")\n",
    "        sentences = text.split(\"<stop>\")\n",
    "        sentences = sentences[:-1]\n",
    "        sentences = [s.strip() for s in sentences]\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(mongoConfigs['host'], mongoConfigs['port'])\n",
    "db = client[mongoConfigs['db']]\n",
    "collection = db[resolved_entity_table]  # collection having resolved entities\n",
    "art_collection = db[article_table]  # collection having articles\n",
    "\n",
    "entity_types = entity_types\n",
    "short_sources_list = short_sources_list\n",
    "sources_list = sources_list\n",
    "fixed_keywords = ['says', 'said', 'asks', 'asked', 'told', 'announced', 'announce', 'claimed', 'claim']\n",
    "\n",
    "extractor = ExtractSentences()  # object for extracting sentences from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanfordNLP:\n",
    "    def __init__(self, host='http://localhost', port=9000):\n",
    "        self.nlp = StanfordCoreNLP(host, port=port,\n",
    "                                   timeout=30000)\n",
    "        self.props = {\n",
    "            'annotators': 'tokenize,ssplit,pos,lemma,ner,parse,depparse,dcoref,relation',\n",
    "            'pipelineLanguage': 'en',\n",
    "            'outputFormat': 'json'\n",
    "        }\n",
    "\n",
    "    def word_tokenize(self, sentence):\n",
    "        return self.nlp.word_tokenize(sentence)\n",
    "\n",
    "    def pos(self, sentence):\n",
    "        return self.nlp.pos_tag(sentence)\n",
    "\n",
    "    def ner(self, sentence):\n",
    "        return self.nlp.ner(sentence)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        return self.nlp.parse(sentence)\n",
    "\n",
    "    def dependency_parse(self, sentence):\n",
    "        return self.nlp.dependency_parse(sentence)\n",
    "\n",
    "    def annotate(self, sentence):\n",
    "        return json.loads(self.nlp.annotate(sentence, properties=self.props))\n",
    "\n",
    "    @staticmethod\n",
    "    def tokens_to_dict(_tokens):\n",
    "        tokens = defaultdict(dict)\n",
    "        for token in _tokens:\n",
    "            tokens[int(token['index'])] = {\n",
    "                'word': token['word'],\n",
    "                'lemma': token['lemma'],\n",
    "                'pos': token['pos'],\n",
    "                'ner': token['ner']\n",
    "            }\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSentiment(sentiString):\n",
    "    '''\n",
    "    return Sentiment by Vader\n",
    "    :param sentiString:\n",
    "    :return:\n",
    "    '''\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment = analyzer.polarity_scores(sentiString)\n",
    "    a_sent = (sentiment[\"compound\"])\n",
    "    return a_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "entitySpecificSentimentAnalysis:\n",
    "    takes two argument\n",
    "    1. Input File : set of sentences\n",
    "    2. Keywords associated with target\n",
    "\n",
    "    and output two list\n",
    "    1. Articles on target\n",
    "    2. Articles by target\n",
    "    3. Articles not about target\n",
    "'''\n",
    "\n",
    "\n",
    "def preprocesstext(doc_set):\n",
    "    text = doc_set.lower()\n",
    "    text = text.replace('\\r', '')\n",
    "    text = text.replace('\\r\\n', '')\n",
    "    text = text.replace('\\n', '')\n",
    "    text = text.replace('\"', '')\n",
    "    text = text.replace('%', ' ')\n",
    "    return text\n",
    "\n",
    "\n",
    "def entitySpecificCoverageAnalysis(doc_set, entity_keywords, entity_name, e_aliases):\n",
    "    '''\n",
    "    Finds the sentences that are about or by the entity\n",
    "    :param doc_set: set of sentences\n",
    "    :param entity_keywords: keywords as to which entity to identify in the sentence.\n",
    "    :return: onTarget_sentences, byTarget_sentences, removed_sentences, onTargetTopic, byTargetTopic\n",
    "    '''\n",
    "    sNLP = StanfordNLP()\n",
    "    onTargetArticles = []\n",
    "    byTargetArticles = []\n",
    "    removedArticles = []\n",
    "    short_entity_name = ''.join(entity_name.split()).lower()\n",
    "    entity_keywords.append(short_entity_name)\n",
    "    for i in range(len(doc_set)):\n",
    "        print('Document: {}'.format(i))\n",
    "        text = preprocesstext(doc_set[i])\n",
    "        for alis in e_aliases:\n",
    "            text = text.replace(' ' + alis.lower() + ' ', ' ' + short_entity_name + ' ')\n",
    "            text = text.replace(' ' + alis.lower() + '. ', ' ' + short_entity_name + ' . ')\n",
    "            text = text.replace(' ' + alis.lower() + ', ', ' ' + short_entity_name + ' , ')\n",
    "        try:\n",
    "            pos_text = sNLP.pos(text)\n",
    "        except json.decoder.JSONDecodeError:\n",
    "            print('JSON_Decode_Error: ', text)\n",
    "            continue\n",
    "        parse_text = sNLP.dependency_parse(text)\n",
    "        state1 = False\n",
    "        state2 = False\n",
    "        for pt in parse_text:\n",
    "            if ((pt[0] == 'nsubj') or (pt[0] == 'nmod') or (pt[0] == 'amod') or (pt[0] == 'dobj')) and (\n",
    "                        (pos_text[pt[1] - 1][0] in entity_keywords) or (pos_text[pt[2] - 1][0] in entity_keywords)):\n",
    "                if ((pt[0] == 'nsubj') and (\n",
    "                                pos_text[pt[1] - 1][0] in fixed_keywords or pos_text[pt[2] - 1][0] in fixed_keywords)):\n",
    "                    state2 = True\n",
    "                else:\n",
    "                    state1 = True\n",
    "        if state1:\n",
    "            onTargetArticles.append(text)\n",
    "        if state2:\n",
    "            byTargetArticles.append(text)\n",
    "        else:\n",
    "            removedArticles.append(text)\n",
    "    return (onTargetArticles, byTargetArticles, removedArticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names_aliases_articles(entities):\n",
    "    e_names = []\n",
    "    e_aliases = []\n",
    "    e_articleIds = []\n",
    "    indices = []\n",
    "    for type in entities.keys():\n",
    "        for entity in entities[type]:\n",
    "            e_names.append(entity['name'])\n",
    "            e_aliases.append(entity['aliases'])\n",
    "            e_articleIds.append(entity[\"articleIds\"])\n",
    "    return (e_names, e_aliases, e_articleIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = get_all_entities(collection, N, entity_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_names, e_aliases, e_articleIds = get_names_aliases_articles(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Narendra Modi.Besides',\n",
       " 'Targeting Narendra Modi',\n",
       " 'Manmohan Singh.Then',\n",
       " 'Virat Kohli Dhoni',\n",
       " 'Donald Trump']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findPowerEliteIndex(entity_name, e_names, e_aliases):\n",
    "    '''\n",
    "    FInd all the entity resolution which may contain the given entity.\n",
    "    :param entity_name: Given entity\n",
    "    :param e_names: list of all entity names\n",
    "    :param e_aliases: list of all entity aliases\n",
    "    :return: set of indices\n",
    "    '''\n",
    "    print('Search ', entity_name, ' : ', len(e_names))\n",
    "    indices = []\n",
    "    for i in range(len(e_names)):\n",
    "        name = e_names[i].replace('.', '')\n",
    "        alias = ','.join(e_aliases[i])\n",
    "        if entity_name.lower() in name.lower() or entity_name.lower() in alias.lower():\n",
    "            indices.append(i)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search  Reetika Khera  :  2785864\n",
      "[20002, 169699, 188364, 301878, 358022, 1542608, 1572147, 1665674, 1903404, 2498496]\n"
     ]
    }
   ],
   "source": [
    "entity_ind = findPowerEliteIndex(entity_name, e_names, e_aliases)\n",
    "print(entity_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reetika Khera : READ ARTICLE IDS ...\n",
      "[20002, 169699, 188364, 301878, 358022, 1542608, 1572147, 1665674, 1903404, 2498496]\n"
     ]
    }
   ],
   "source": [
    "articles = {s: [] for s in sources_list}\n",
    "print(entity_name + ' : READ ARTICLE IDS ...')\n",
    "\n",
    "print(entity_ind)\n",
    "new_entity_alias = []\n",
    "for l in range(len(entity_ind)):\n",
    "    new_entity_alias.extend(e_aliases[entity_ind[l]])\n",
    "    for article_id in e_articleIds[entity_ind[l]]:\n",
    "        article = art_collection.find({\"_id\": article_id})[0]\n",
    "        # url = article[\"articleUrl\"]\n",
    "        text = article[\"text\"]\n",
    "        source = article[\"sourceName\"]\n",
    "        if source in sources_list:\n",
    "            articles[source].append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printToFile(about_entity, by_entity, entity, source):\n",
    "    '''\n",
    "    :param about_entity: set of sentences that are about the entity\n",
    "    :param by_entity:  set of sentences that are statements made by the entity\n",
    "    :param entity: Entity name\n",
    "    :param source: News source Short URL to be used in file naming\n",
    "    :return: None\n",
    "    '''\n",
    "    about_sent = 0\n",
    "    by_sent = 0\n",
    "    if len(about_entity):\n",
    "        fname = './' + res_folder + '/about_' + '_'.join(entity.split()) + '_' + source + '.txt'\n",
    "        outfile = open(fname, 'w')\n",
    "        for l in range(len(about_entity)):\n",
    "            line = about_entity[l]\n",
    "            line_sent = findSentiment(line)\n",
    "            outfile.write(';;' + str(line_sent) + ';;' + line + '\\n')\n",
    "            about_sent = about_sent + line_sent\n",
    "        outfile.close()\n",
    "    if len(by_entity):\n",
    "        fname = './' + res_folder + '/by_' + '_'.join(entity.split()) + '_' + source + '.txt'\n",
    "        outfile = open(fname, 'w')\n",
    "        for l in range(len(by_entity)):\n",
    "            line = by_entity[l]\n",
    "            line_sent = findSentiment(line)\n",
    "            outfile.write(';;' + str(line_sent) + ';;' + line + '\\n')\n",
    "            by_sent = by_sent + line_sent\n",
    "        outfile.close()\n",
    "    return (about_sent, by_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printRemovedToFile(entity, source, sentences):\n",
    "    '''\n",
    "    Write the removed sentences to a file\n",
    "    :param entity: Entity Name\n",
    "    :param source:  News source Short URL to be used in file naming\n",
    "    :param sentences: set of removed sentences\n",
    "    :return: None\n",
    "    '''\n",
    "    fname = './' + res_folder + '/removed_' + '_'.join(entity.split()) + '_' + source + '.txt'\n",
    "    outfile = open(fname, 'w')\n",
    "    for s in sentences:\n",
    "        outfile.write(s + '\\n')\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Times Of India  , #articles :  0\n",
      "Extract sentences...\n",
      "Find entity specific sentences...\n",
      "Print about and by entity sentences to file...\n",
      "Reetika Khera The Times Of India done...\n",
      "About:  0  By:  0 Removed:  0\n",
      "About:  0  By:  0 \n",
      "\n",
      "Hindustan Times  , #articles :  3\n",
      "Extract sentences...\n",
      "Find entity specific sentences...\n",
      "Document: 0\n",
      "Document: 1\n",
      "Document: 2\n",
      "Document: 3\n",
      "Document: 4\n",
      "Document: 5\n",
      "Document: 6\n",
      "Document: 7\n",
      "Document: 8\n",
      "Document: 9\n",
      "Document: 10\n",
      "Document: 11\n",
      "Document: 12\n",
      "Document: 13\n",
      "Document: 14\n",
      "Document: 15\n",
      "Document: 16\n",
      "Document: 17\n",
      "Document: 18\n",
      "Document: 19\n",
      "Document: 20\n",
      "Document: 21\n",
      "Document: 22\n",
      "Document: 23\n",
      "Document: 24\n",
      "Document: 25\n",
      "Document: 26\n",
      "Document: 27\n",
      "Document: 28\n",
      "Document: 29\n",
      "Document: 30\n",
      "Document: 31\n",
      "Document: 32\n",
      "Document: 33\n",
      "Document: 34\n",
      "Document: 35\n",
      "Document: 36\n",
      "Document: 37\n",
      "Document: 38\n",
      "Document: 39\n",
      "Document: 40\n",
      "Document: 41\n",
      "Document: 42\n",
      "Document: 43\n",
      "Document: 44\n",
      "Document: 45\n",
      "Document: 46\n",
      "Document: 47\n",
      "Document: 48\n",
      "Document: 49\n",
      "Document: 50\n",
      "Document: 51\n",
      "Document: 52\n",
      "Document: 53\n",
      "Document: 54\n",
      "Document: 55\n",
      "Document: 56\n",
      "Document: 57\n",
      "Document: 58\n",
      "Document: 59\n",
      "Document: 60\n",
      "Document: 61\n",
      "Document: 62\n",
      "Document: 63\n",
      "Document: 64\n",
      "Document: 65\n",
      "Document: 66\n",
      "Document: 67\n",
      "Document: 68\n",
      "Document: 69\n",
      "Document: 70\n",
      "Document: 71\n",
      "Document: 72\n",
      "Document: 73\n",
      "Document: 74\n",
      "Document: 75\n",
      "Document: 76\n",
      "Document: 77\n",
      "Document: 78\n",
      "Document: 79\n",
      "Document: 80\n",
      "Document: 81\n",
      "Document: 82\n",
      "Document: 83\n",
      "Document: 84\n",
      "Document: 85\n",
      "Document: 86\n",
      "Document: 87\n",
      "Document: 88\n",
      "Document: 89\n",
      "Document: 90\n",
      "Document: 91\n",
      "Document: 92\n",
      "Document: 93\n",
      "Document: 94\n",
      "Document: 95\n",
      "Document: 96\n",
      "Document: 97\n",
      "Document: 98\n",
      "Document: 99\n",
      "Document: 100\n",
      "Document: 101\n",
      "Document: 102\n",
      "Document: 103\n",
      "Document: 104\n",
      "Print about and by entity sentences to file...\n",
      "Reetika Khera Hindustan Times done...\n",
      "About:  0  By:  1 Removed:  104\n",
      "About:  0  By:  0.1027 \n",
      "\n",
      "Indian Express  , #articles :  6\n",
      "Extract sentences...\n",
      "Find entity specific sentences...\n",
      "Document: 0\n",
      "Document: 1\n",
      "Document: 2\n",
      "Document: 3\n",
      "Document: 4\n",
      "Document: 5\n",
      "Document: 6\n",
      "Document: 7\n",
      "Document: 8\n",
      "Document: 9\n",
      "Document: 10\n",
      "Document: 11\n",
      "Document: 12\n",
      "Document: 13\n",
      "Document: 14\n",
      "Document: 15\n",
      "Document: 16\n",
      "Document: 17\n",
      "Document: 18\n",
      "Document: 19\n",
      "Document: 20\n",
      "Document: 21\n",
      "Document: 22\n",
      "Document: 23\n",
      "Document: 24\n",
      "Document: 25\n",
      "Document: 26\n",
      "Document: 27\n",
      "Document: 28\n",
      "Document: 29\n",
      "Document: 30\n",
      "Document: 31\n",
      "Document: 32\n",
      "Document: 33\n",
      "Document: 34\n",
      "Document: 35\n",
      "Document: 36\n",
      "Document: 37\n",
      "Document: 38\n",
      "Document: 39\n",
      "Document: 40\n",
      "Document: 41\n",
      "Document: 42\n",
      "Document: 43\n",
      "Document: 44\n",
      "Document: 45\n",
      "Document: 46\n",
      "Document: 47\n",
      "Document: 48\n",
      "Document: 49\n",
      "Document: 50\n",
      "Document: 51\n",
      "Document: 52\n",
      "Document: 53\n",
      "Document: 54\n",
      "Document: 55\n",
      "Document: 56\n",
      "Document: 57\n",
      "Document: 58\n",
      "Document: 59\n",
      "Document: 60\n",
      "Document: 61\n",
      "Document: 62\n",
      "Document: 63\n",
      "Document: 64\n",
      "Document: 65\n",
      "Document: 66\n",
      "Document: 67\n",
      "Document: 68\n",
      "Document: 69\n",
      "Document: 70\n",
      "Document: 71\n",
      "Document: 72\n",
      "Document: 73\n",
      "Document: 74\n",
      "Document: 75\n",
      "Document: 76\n",
      "Document: 77\n",
      "Document: 78\n",
      "Document: 79\n",
      "Document: 80\n",
      "Document: 81\n",
      "Document: 82\n",
      "Document: 83\n",
      "Document: 84\n",
      "Document: 85\n",
      "Document: 86\n",
      "Document: 87\n",
      "Document: 88\n",
      "Document: 89\n",
      "Document: 90\n",
      "Document: 91\n",
      "Document: 92\n",
      "Document: 93\n",
      "Document: 94\n",
      "Document: 95\n",
      "Document: 96\n",
      "Document: 97\n",
      "Document: 98\n",
      "Document: 99\n",
      "Document: 100\n",
      "Document: 101\n",
      "Document: 102\n",
      "Document: 103\n",
      "Document: 104\n",
      "Document: 105\n",
      "Document: 106\n",
      "Document: 107\n",
      "Document: 108\n",
      "Document: 109\n",
      "Document: 110\n",
      "Document: 111\n",
      "Document: 112\n",
      "Document: 113\n",
      "Document: 114\n",
      "Document: 115\n",
      "Document: 116\n",
      "Document: 117\n",
      "Document: 118\n",
      "Document: 119\n",
      "Document: 120\n",
      "Document: 121\n",
      "Document: 122\n",
      "Document: 123\n",
      "Document: 124\n",
      "Document: 125\n",
      "Document: 126\n",
      "Document: 127\n",
      "Document: 128\n",
      "Document: 129\n",
      "Document: 130\n",
      "Document: 131\n",
      "Document: 132\n",
      "Document: 133\n",
      "Document: 134\n",
      "Document: 135\n",
      "Document: 136\n",
      "Document: 137\n",
      "Document: 138\n",
      "Document: 139\n",
      "Document: 140\n",
      "Document: 141\n",
      "Document: 142\n",
      "Document: 143\n",
      "Document: 144\n",
      "Document: 145\n",
      "Document: 146\n",
      "Document: 147\n",
      "Document: 148\n",
      "Document: 149\n",
      "Document: 150\n",
      "Document: 151\n",
      "Document: 152\n",
      "Document: 153\n",
      "Document: 154\n",
      "Document: 155\n",
      "Document: 156\n",
      "Document: 157\n",
      "Document: 158\n",
      "Document: 159\n",
      "Document: 160\n",
      "Document: 161\n",
      "Document: 162\n",
      "Document: 163\n",
      "Document: 164\n",
      "Document: 165\n",
      "Document: 166\n",
      "Document: 167\n",
      "Document: 168\n",
      "Document: 169\n",
      "Document: 170\n",
      "Document: 171\n",
      "Document: 172\n",
      "Document: 173\n",
      "Document: 174\n",
      "Document: 175\n",
      "Document: 176\n",
      "Document: 177\n",
      "Document: 178\n",
      "Document: 179\n",
      "Document: 180\n",
      "Document: 181\n",
      "Document: 182\n",
      "Document: 183\n",
      "Document: 184\n",
      "Document: 185\n",
      "Document: 186\n",
      "Document: 187\n",
      "Document: 188\n",
      "Document: 189\n",
      "Document: 190\n",
      "Document: 191\n",
      "Document: 192\n",
      "Document: 193\n",
      "Document: 194\n",
      "Document: 195\n",
      "Document: 196\n",
      "Document: 197\n",
      "Document: 198\n",
      "Document: 199\n",
      "Document: 200\n",
      "Document: 201\n",
      "Document: 202\n",
      "Document: 203\n",
      "Document: 204\n",
      "Document: 205\n",
      "Document: 206\n",
      "Document: 207\n",
      "Print about and by entity sentences to file...\n",
      "Reetika Khera Indian Express done...\n",
      "About:  1  By:  1 Removed:  207\n",
      "About:  -0.296  By:  0.0 \n",
      "\n",
      "Deccan Herald  , #articles :  1\n",
      "Extract sentences...\n",
      "Find entity specific sentences...\n",
      "Document: 0\n",
      "Document: 1\n",
      "Document: 2\n",
      "Document: 3\n",
      "Document: 4\n",
      "Document: 5\n",
      "Document: 6\n",
      "Document: 7\n",
      "Document: 8\n",
      "Document: 9\n",
      "Document: 10\n",
      "Document: 11\n",
      "Document: 12\n",
      "Document: 13\n",
      "Document: 14\n",
      "Document: 15\n",
      "Document: 16\n",
      "Document: 17\n",
      "Document: 18\n",
      "Document: 19\n",
      "Document: 20\n",
      "Document: 21\n",
      "Document: 22\n",
      "Document: 23\n",
      "Document: 24\n",
      "Document: 25\n",
      "Document: 26\n",
      "Document: 27\n",
      "Document: 28\n",
      "Document: 29\n",
      "Document: 30\n",
      "Document: 31\n",
      "Document: 32\n",
      "Document: 33\n",
      "Document: 34\n",
      "Document: 35\n",
      "Document: 36\n",
      "Document: 37\n",
      "Document: 38\n",
      "Document: 39\n",
      "Document: 40\n",
      "Document: 41\n",
      "Document: 42\n",
      "Document: 43\n",
      "Document: 44\n",
      "Document: 45\n",
      "Document: 46\n",
      "Document: 47\n",
      "Document: 48\n",
      "Document: 49\n",
      "Document: 50\n",
      "Document: 51\n",
      "Document: 52\n",
      "Document: 53\n",
      "Document: 54\n",
      "Document: 55\n",
      "Document: 56\n",
      "Document: 57\n",
      "Document: 58\n",
      "Document: 59\n",
      "Document: 60\n",
      "Document: 61\n",
      "Document: 62\n",
      "Document: 63\n",
      "Document: 64\n",
      "Document: 65\n",
      "Document: 66\n",
      "Document: 67\n",
      "Document: 68\n",
      "Document: 69\n",
      "Document: 70\n",
      "Document: 71\n",
      "Document: 72\n",
      "Print about and by entity sentences to file...\n",
      "Reetika Khera Deccan Herald done...\n",
      "About:  1  By:  0 Removed:  73\n",
      "About:  0.4767  By:  0 \n",
      "\n",
      "Telegraph  , #articles :  2\n",
      "Extract sentences...\n",
      "Find entity specific sentences...\n",
      "Document: 0\n",
      "Document: 1\n",
      "Document: 2\n",
      "Document: 3\n",
      "Document: 4\n",
      "Document: 5\n",
      "Document: 6\n",
      "Document: 7\n",
      "Document: 8\n",
      "Document: 9\n",
      "Document: 10\n",
      "Document: 11\n",
      "Document: 12\n",
      "Document: 13\n",
      "Document: 14\n",
      "Document: 15\n",
      "Document: 16\n",
      "Document: 17\n",
      "Document: 18\n",
      "Document: 19\n",
      "Document: 20\n",
      "Document: 21\n",
      "Document: 22\n",
      "Document: 23\n",
      "Document: 24\n",
      "Document: 25\n",
      "Document: 26\n",
      "Document: 27\n",
      "Document: 28\n",
      "Document: 29\n",
      "Document: 30\n",
      "Document: 31\n",
      "Document: 32\n",
      "Document: 33\n",
      "Document: 34\n",
      "Document: 35\n",
      "Document: 36\n",
      "Document: 37\n",
      "Document: 38\n",
      "Document: 39\n",
      "Document: 40\n",
      "Document: 41\n",
      "Document: 42\n",
      "Document: 43\n",
      "Document: 44\n",
      "Document: 45\n",
      "Document: 46\n",
      "Document: 47\n",
      "Document: 48\n",
      "Document: 49\n",
      "Document: 50\n",
      "Document: 51\n",
      "Document: 52\n",
      "Document: 53\n",
      "Document: 54\n",
      "Document: 55\n",
      "Print about and by entity sentences to file...\n",
      "Reetika Khera Telegraph done...\n",
      "About:  0  By:  0 Removed:  56\n",
      "About:  0  By:  0 \n",
      "\n",
      "The New Indian Express  , #articles :  0\n",
      "Extract sentences...\n",
      "Find entity specific sentences...\n",
      "Print about and by entity sentences to file...\n",
      "Reetika Khera The New Indian Express done...\n",
      "About:  0  By:  0 Removed:  0\n",
      "About:  0  By:  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for j in range(1, len(sources_list)):\n",
    "    source = sources_list[j]\n",
    "    print(source, ' , #articles : ', len(articles[source]))\n",
    "    sentences = []\n",
    "\n",
    "    print('Extract sentences...')\n",
    "    for text in list(set(articles[source])):\n",
    "        ext_sentences = extractor.split_into_sentences(str(text))\n",
    "        sentences.extend(ext_sentences)\n",
    "\n",
    "    print('Find entity specific sentences...')\n",
    "    onTargetArticles, byTargetArticles, removedArticles = entitySpecificCoverageAnalysis \\\n",
    "        (sentences, new_entity_alias, entity_name, new_entity_alias)\n",
    "\n",
    "    print('Print about and by entity sentences to file...')\n",
    "    about_sent, by_sent = printToFile(onTargetArticles, byTargetArticles,\n",
    "                                      entity_name, short_sources_list[j])\n",
    "    printRemovedToFile(entity_name, short_sources_list[j], removedArticles)\n",
    "\n",
    "    print(entity_name, source, \"done...\")\n",
    "    print('About: ', len(onTargetArticles), ' By: ', len(byTargetArticles), 'Removed: ', len(removedArticles))\n",
    "    print('About: ', about_sent, ' By: ', by_sent, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
