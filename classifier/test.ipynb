{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import random\n",
    "import progressbar\n",
    "import torch\n",
    "import pickle\n",
    "from mytree import *\n",
    "from utils import *\n",
    "from treeUtil import *\n",
    "import tqdm\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import functools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import string\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import nltk.tree\n",
    "from ast import literal_eval\n",
    "import pptree\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import os\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from spacy.pipeline import merge_entities\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.add_pipe(merge_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False if in-domain; True if general\n",
    "proanti = True\n",
    "w2vec = False\n",
    "ner = True\n",
    "blackout = False\n",
    "balanced = True\n",
    "undersample = False\n",
    "non_trainable = True\n",
    "economic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'economic_w2v_1ProAnti_0General_1NER_0Blackout_1Balance_0Undersample_1Fixed'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = {0:'ProAnti', 1:'General', 2:'NER', 3:'Blackout', 4:'Balance', 5:'Undersample', 6:'Fixed'}\n",
    "bool_list = [proanti, w2vec, ner, blackout, balanced, undersample, non_trainable]\n",
    "namecode = 'economic_w2v'\n",
    "for index, bb in enumerate(bool_list):\n",
    "    namecode += '_'\n",
    "    if bb:\n",
    "        namecode += '1'\n",
    "    else:\n",
    "        namecode += '0'\n",
    "    namecode += dic[index]\n",
    "namecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA=False\n",
    "def Var(v):\n",
    "    if CUDA: return Variable(v.cuda())\n",
    "    else: return Variable(v)\n",
    "widgets = [progressbar.Percentage(), ' ', progressbar.Bar(), ' ', progressbar.ETA()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveNN(nn.Module):\n",
    "    def __init__(self, word_embeddings, vocab, embedSize=300, numClasses=2, beta = 0.3, use_weight = True, non_trainable = non_trainable):\n",
    "        super(RecursiveNN, self).__init__()\n",
    "#             if (w2vec):\n",
    "#                 self.embedding = nn.Embedding(len(vocab), embedSize)\n",
    "#                 self.embedding.load_state_dict({'weight': w2vec_weights})\n",
    "        self.embedding = nn.Embedding.from_pretrained(word_embeddings)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        if non_trainable:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(len(vocab), embedSize)\n",
    "        self.embedding = nn.Embedding(len(vocab), embedSize)\n",
    "        self.W = nn.Linear(2*embedSize, embedSize, bias=True)\n",
    "        self.nonLinear = torch.tanh\n",
    "        self.projection = nn.Linear(embedSize, numClasses, bias=True)\n",
    "        self.nodeProbList = []\n",
    "        self.labelList = []\n",
    "        self.loss = Var(torch.FloatTensor([0]))\n",
    "        self.V = vocab\n",
    "        self.beta = beta\n",
    "        self.use_weight = use_weight\n",
    "        self.total_rep = None #\n",
    "        self.count_rep = 0 #\n",
    "        self.numClasses = numClasses\n",
    "\n",
    "    def traverse(self, node):\n",
    "        if node.isLeaf:\n",
    "            if node.getLeafWord() in self.V:  # check if right word is in vocabulary\n",
    "                word = node.getLeafWord()\n",
    "            else:  # otherwise use the unknown token\n",
    "                word = 'UNK'\n",
    "            # print(self.V[word],len(self.V),word,(torch.LongTensor([int(self.V[word])])))\n",
    "            currentNode = (self.embedding(Var(torch.LongTensor([int(self.V[word])]))))\n",
    "        else: currentNode = self.nonLinear(self.W(torch.cat((self.traverse(node.left),self.traverse(node.right)),1)))\n",
    "        currentNode = currentNode/(torch.norm(currentNode))\n",
    "\n",
    "        assert node.label!=None\n",
    "        self.nodeProbList.append(self.projection(currentNode))\n",
    "#         print(node.label)\n",
    "#         print(torch.LongTensor([node.label]))\n",
    "        self.labelList.append(torch.LongTensor([node.label]))\n",
    "        loss_weight = 1-self.beta if node.annotated else self.beta\n",
    "        self.loss += (loss_weight*F.cross_entropy(input=torch.cat([self.projection(currentNode)]),target=Var(torch.cat([torch.LongTensor([node.label])]))))\n",
    "\n",
    "        #\n",
    "        if not node.isRoot():\n",
    "            if self.total_rep is None:\n",
    "                self.total_rep = currentNode.data.clone()\n",
    "            else:\n",
    "                self.total_rep += currentNode.data.clone()\n",
    "            self.count_rep += 1\n",
    "        #\n",
    "        return currentNode   \n",
    "    \n",
    "    \n",
    "    \n",
    "    def traverse_test(self, node):\n",
    "        if node.isLeaf:\n",
    "            if node.getLeafWord() in self.V:  # check if right word is in vocabulary\n",
    "                word = node.getLeafWord()\n",
    "            else:  # otherwise use the unknown token\n",
    "                word = 'UNK'\n",
    "            # print(self.V[word],len(self.V),word,(torch.LongTensor([int(self.V[word])])))\n",
    "            currentNode = (self.embedding(Var(torch.LongTensor([int(self.V[word])]))))\n",
    "        else: currentNode = self.nonLinear(self.W(torch.cat((self.traverse_test(node.left),self.traverse_test(node.right)),1)))\n",
    "        currentNode = currentNode/(torch.norm(currentNode))\n",
    "\n",
    "#         assert node.label!=None\n",
    "        self.nodeProbList.append(self.projection(currentNode))\n",
    "        loss_weight = 1-self.beta if node.annotated else self.beta\n",
    "\n",
    "        if not node.isRoot():\n",
    "            if self.total_rep is None:\n",
    "                self.total_rep = currentNode.data.clone()\n",
    "            else:\n",
    "                self.total_rep += currentNode.data.clone()\n",
    "            self.count_rep += 1\n",
    "        #\n",
    "        return currentNode\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.nodeProbList = []\n",
    "        self.labelList = []\n",
    "        self.loss = Var(torch.FloatTensor([0]))\n",
    "#         self.traverse(x)\n",
    "        self.traverse_test(x)\n",
    "#         self.labelList = Var(torch.cat(self.labelList))\n",
    "        return torch.cat(self.nodeProbList)\n",
    "\n",
    "    def getLoss(self, tree):\n",
    "        nodes = self.forward(tree)\n",
    "        predictions = nodes.max(dim=1)[1]\n",
    "        loss = self.loss\n",
    "        return predictions,loss\n",
    "    \n",
    "    \n",
    "    def predict(self, trees):\n",
    "        pbar = progressbar.ProgressBar(widgets=widgets, maxval=len(trees)).start()\n",
    "        preds = []\n",
    "        for j, tree in enumerate(trees):\n",
    "            nodes = self.forward(tree.root)\n",
    "            predictions = nodes.max(dim=1)[1]\n",
    "#                     print((predictions.cpu().data).numpy(),(predictions.cpu().data).numpy().shape)\n",
    "#                     print((self.labelList.cpu().data).numpy(), (self.labelList.cpu().data).numpy().shape)\n",
    "            preds.append(predictions)\n",
    "            pbar.update(j)\n",
    "        pbar.finish()\n",
    "        return preds\n",
    "    \n",
    "\n",
    "    def getRep(self, tree):\n",
    "        self.count_rep = 0\n",
    "        self.total_rep = None\n",
    "        self.nodeProbList = []\n",
    "#         self.labelList = []\n",
    "        self.loss = Var(torch.FloatTensor([0]))\n",
    "\n",
    "        root_rep = self.traverse(tree)\n",
    "\n",
    "        return (torch.cat((root_rep,self.total_rep/self.count_rep),1)).data.numpy().T.flatten()\n",
    "\n",
    "    \n",
    "#     def evaluate(self, trees):\n",
    "#         pbar = progressbar.ProgressBar(widgets=widgets, maxval=len(trees)).start()\n",
    "#         n = nAll = correctRoot = correctAll = 0.0\n",
    "#         tp = [1e-2]*self.numClasses\n",
    "#         fp = [1e-2]*self.numClasses\n",
    "#         fn = [1e-2]*self.numClasses\n",
    "#         f1 = [0.]*self.numClasses\n",
    "#         for j, tree in enumerate(trees):\n",
    "#             predictions,_ = self.getLoss(tree.root)\n",
    "# #                     print((predictions.cpu().data).numpy(),(predictions.cpu().data).numpy().shape)\n",
    "# #                     print((self.labelList.cpu().data).numpy(), (self.labelList.cpu().data).numpy().shape)\n",
    "#             correct = ((predictions.cpu().data).numpy()==(self.labelList.cpu().data).numpy())\n",
    "# #                     print(correct)\n",
    "#             correctAll += correct.sum()\n",
    "#             nAll += np.shape(correct.squeeze())[0] if np.size(correct)!=1 else 1 \n",
    "#             correctRoot += correct.squeeze()[-1] if np.size(correct)!=1 else correct[-1]\n",
    "# #                     print(correct.squeeze()[-1] if np.size(correct)!=1 else correct[-1])\n",
    "# #                     print('actual: {}'.format(tree.root.label))\n",
    "#             for i in range(self.numClasses):\n",
    "#                 size = np.size((predictions.cpu().data).numpy())\n",
    "#                 if size!=1:\n",
    "#                     pred = (predictions.cpu().data).numpy().squeeze()[-1]\n",
    "#                     actual = (self.labelList.cpu().data).numpy().squeeze()[-1]\n",
    "#                 else:\n",
    "#                     pred = (predictions.cpu().data).numpy()[-1]\n",
    "#                     actual = (self.labelList.cpu().data).numpy()[-1]\n",
    "#                 if pred==i and actual==i:\n",
    "#                     tp[i]+=1\n",
    "#                 elif pred==i and actual!=i:\n",
    "#                     fn[i]+=1\n",
    "#                 elif pred==i and actual!=i:\n",
    "#                     fp[i]+=1\n",
    "#             n += 1\n",
    "#             pbar.update(j)\n",
    "# #             print(tp,fp,fn)\n",
    "#         for i in range(self.numClasses):\n",
    "#             p =(1.0*tp[i]/(tp[i]+fp[i]))\n",
    "#             r =(1.0*tp[i]/(tp[i]+fn[i]))\n",
    "#             f1[i] = (2*p*r)/(p+r)\n",
    "#         pbar.finish()\n",
    "#         return correctRoot / n, correctAll/nAll, f1\n",
    "\n",
    "    def eval_sent_lvl(self,trees,clf):\n",
    "        pbar = progressbar.ProgressBar(widgets=widgets, maxval=len(trees)).start()\n",
    "        n = nAll = correctRoot = correctAll = 0.0\n",
    "        X_predict = []\n",
    "        Y_gold = []\n",
    "        for j, tree in enumerate(trees):\n",
    "            tree_rep = model.getRep(tree.root)\n",
    "            X_predict.append(tree_rep)\n",
    "            Y_gold.append(tree.root.label)\n",
    "        acc = clf.score(np.array(X_predict),np.array(Y_gold))\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_entity(sent):\n",
    "    doc = nlp(sent)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_=='PERSON':\n",
    "            sent = sent.replace(ent.text,ent.label_)\n",
    "    return sent\n",
    "\n",
    "def blackout_entity(sent):\n",
    "    doc = nlp(sent)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_=='PERSON':\n",
    "            sent = sent.replace(ent.text,\"\")\n",
    "    return sent\n",
    "\n",
    "def read_into_string(filename):\n",
    "    text_file = open(filename, 'r')\n",
    "    lines = text_file.read().split('\\n')\n",
    "    if ner:\n",
    "        lines = [replace_entity(sent) for sent in lines]\n",
    "    if blackout:\n",
    "        lines = [blackout_entity(sent) for sent in lines]\n",
    "    text_file.close()\n",
    "    return lines\n",
    "\n",
    "def process(file):\n",
    "    sents = read_into_string(file)\n",
    "    sents = [s for s in sents if len(s)>14 and len(s)<500]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tree(text):\n",
    "#     print(text)\n",
    "    output = nlp.annotate(text, properties={\n",
    "        'annotators': 'tokenize,ssplit,pos,depparse,parse',\n",
    "        'outputFormat': 'json'\n",
    "    })\n",
    "    output = literal_eval(output)\n",
    "    try:\n",
    "        tree = str(output['sentences'][0]['parse'])\n",
    "    except:\n",
    "        print(output,text)\n",
    "        return\n",
    "    # print (tree)\n",
    "    parse_string = ' '.join(str(tree).split())\n",
    "    # print(parse_string)\n",
    "    # print (\"\\n\\n\")\n",
    "    tree = nltk.tree.Tree.fromstring(parse_string)\n",
    "    tree.chomsky_normal_form()\n",
    "    tree.collapse_unary(collapseRoot=True,collapsePOS=True)\n",
    "    nt = convertNLTK_tree(tree)\n",
    "    return nt\n",
    "\n",
    "def printLabelTree(tree):\n",
    "    def inorder(node,nnode):\n",
    "        if node.isLeaf:\n",
    "            newnode = pptree.Node('H',nnode)\n",
    "            wnode = pptree.Node(node.word,newnode)\n",
    "        elif nnode is not None:\n",
    "            newnode = pptree.Node('H',nnode)\n",
    "            inorder(node.left,newnode)\n",
    "            inorder(node.right,newnode)\n",
    "        elif node.isRoot():\n",
    "            newnode = pptree.Node('H')\n",
    "            inorder(node.left,newnode)\n",
    "            inorder(node.right,newnode)\n",
    "            return newnode\n",
    "        return None\n",
    "    pptree.print_tree(inorder(tree.root,None))\n",
    "\n",
    "def create_trees_using_df(df):\n",
    "    tree = []\n",
    "    for tokens in list(df['tokens']):\n",
    "        if len(tokens)==0:\n",
    "            continue\n",
    "        line = ' '.join(tokens)\n",
    "        line += '\\n'\n",
    "        tree.append(make_tree(line))\n",
    "    return tree\n",
    "\n",
    "def printlabel(root,l):\n",
    "    if root:\n",
    "        l.append(root.label)\n",
    "#         print(root.label)\n",
    "        if root.left:\n",
    "            l+=printlabel(root.left,[])\n",
    "#             print(printlabel(root.left))\n",
    "        if root.right:\n",
    "            l+=printlabel(root.right,[])\n",
    "#             print(printlabel(root.right))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path of text file to process\n",
    "file_path = 'sample.txt'\n",
    "sentences = process(file_path)\n",
    "# length of a sentence should be between 14 and 500, else parse trees cannot be formed - stanford parser throws error\n",
    "sentences = [sent for sent in sentences if len(sent)>14 and len(sent)<500]\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this command in the terminal, from stanford-corenlp-full-2018-10-05 directory\n",
    "\n",
    "- java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -annotators \"tokenize,ssplit,pos,lemma,parse,sentiment\" -port 9000 -timeout 90000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP('http://localhost', port=9000,timeout=90000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This makes it not only one of the most accurat...</td>\n",
       "      <td>[This, makes, it, not, only, one, of, the, mos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acknowledging slip-ups in the Aadhaar enrolmen...</td>\n",
       "      <td>[Acknowledging, slip, ups, in, the, Aadhaar, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Briefing reporters here , UIDAI chairman PERSO...</td>\n",
       "      <td>[Briefing, reporters, here, UIDAI, chairman, P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Modi said the banks should redouble efforts in...</td>\n",
       "      <td>[Modi, said, the, banks, should, redouble, eff...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement  \\\n",
       "0  This makes it not only one of the most accurat...   \n",
       "1  Acknowledging slip-ups in the Aadhaar enrolmen...   \n",
       "2  Briefing reporters here , UIDAI chairman PERSO...   \n",
       "3  Modi said the banks should redouble efforts in...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [This, makes, it, not, only, one, of, the, mos...  \n",
       "1  [Acknowledging, slip, ups, in, the, Aadhaar, e...  \n",
       "2  [Briefing, reporters, here, UIDAI, chairman, P...  \n",
       "3  [Modi, said, the, banks, should, redouble, eff...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'statement':sentences})\n",
    "df['tokens'] = [word_tokenize(re.sub(r'[^\\w\\s]|[\\d]+',' ',sent)) for sent in df['statement']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = create_trees_using_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the model in a subfolder 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statement</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This makes it not only one of the most accurat...</td>\n",
       "      <td>PRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acknowledging slip-ups in the Aadhaar enrolmen...</td>\n",
       "      <td>ANTI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Briefing reporters here , UIDAI chairman PERSO...</td>\n",
       "      <td>PRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Modi said the banks should redouble efforts in...</td>\n",
       "      <td>ANTI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Statement Label\n",
       "0  This makes it not only one of the most accurat...   PRO\n",
       "1  Acknowledging slip-ups in the Aadhaar enrolmen...  ANTI\n",
       "2  Briefing reporters here , UIDAI chairman PERSO...   PRO\n",
       "3  Modi said the banks should redouble efforts in...  ANTI"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pickle.load(open(\"./models/\"+namecode+'.pkl','rb'))\n",
    "preds = model.predict(trees)\n",
    "preds = [p.numpy()[-1] for p in preds]\n",
    "labels = []\n",
    "i=0\n",
    "for p in preds:\n",
    "    if p==0:\n",
    "        labels.append('PRO')\n",
    "    else:\n",
    "        labels.append('ANTI')\n",
    "        i+=1\n",
    "\n",
    "df_preds = pd.DataFrame({\"Statement\": df['statement'], \"Label\":labels})\n",
    "df_preds.to_csv('predictions.csv')\n",
    "df_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
