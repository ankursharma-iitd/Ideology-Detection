{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import random\n",
    "import progressbar\n",
    "import torch\n",
    "import pickle\n",
    "from mytree import *\n",
    "from utils import *\n",
    "from treeUtil import *\n",
    "import tqdm\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import functools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import string\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import nltk.tree\n",
    "from ast import literal_eval\n",
    "import pptree\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import os\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from spacy.pipeline import merge_entities\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.add_pipe(merge_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False if in-domain; True if general\n",
    "proanti = False\n",
    "w2vec = False\n",
    "ner = True\n",
    "blackout = False\n",
    "balanced = True\n",
    "undersample = False\n",
    "non_trainable = True\n",
    "economic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'econ_w2v_0ProAnti_0General_1NER_0Blackout_1Balance_0Undersample_1Fixed'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = {0:'ProAnti', 1:'General', 2:'NER', 3:'Blackout', 4:'Balance', 5:'Undersample', 6:'Fixed'}\n",
    "bool_list = [proanti, w2vec, ner, blackout, balanced, undersample, non_trainable]\n",
    "corpus_path = '../data/new/econ'\n",
    "namecode = 'econ_w2v'\n",
    "for index, bb in enumerate(bool_list):\n",
    "    namecode += '_'\n",
    "    if bb:\n",
    "        namecode += '1'\n",
    "    else:\n",
    "        namecode += '0'\n",
    "    namecode += dic[index]\n",
    "namecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For logging\n",
    "import logging\n",
    "\n",
    "#Remove all the previous handlers\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "#Create the file for logging purposes -> CHANGE THE LEVEL TYPE TO logging.DEBUG when debugging or finding faults\n",
    "logging.basicConfig(filename='log_'+namecode+'.log',\n",
    "                            filemode='a',\n",
    "                            format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                            datefmt='%m/%d/%Y %I:%M:%S %p',\n",
    "                            level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger('./logs_'+namecode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveNN(nn.Module):\n",
    "    def __init__(self, word_embeddings, vocab, embedSize=300, numClasses=2, beta = 0.3, use_weight = True, non_trainable = non_trainable):\n",
    "        super(RecursiveNN, self).__init__()\n",
    "#             if (w2vec):\n",
    "#                 self.embedding = nn.Embedding(len(vocab), embedSize)\n",
    "#                 self.embedding.load_state_dict({'weight': w2vec_weights})\n",
    "        self.embedding = nn.Embedding.from_pretrained(word_embeddings)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        if non_trainable:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(len(vocab), embedSize)\n",
    "        self.embedding = nn.Embedding(len(vocab), embedSize)\n",
    "        self.W = nn.Linear(2*embedSize, embedSize, bias=True)\n",
    "        self.nonLinear = torch.tanh\n",
    "        self.projection = nn.Linear(embedSize, numClasses, bias=True)\n",
    "        self.nodeProbList = []\n",
    "        self.labelList = []\n",
    "        self.loss = Var(torch.FloatTensor([0]))\n",
    "        self.V = vocab\n",
    "        self.beta = beta\n",
    "        self.use_weight = use_weight\n",
    "        self.total_rep = None #\n",
    "        self.count_rep = 0 #\n",
    "        self.numClasses = numClasses\n",
    "\n",
    "    def traverse(self, node):\n",
    "        if node.isLeaf:\n",
    "            if node.getLeafWord() in self.V:  # check if right word is in vocabulary\n",
    "                word = node.getLeafWord()\n",
    "            else:  # otherwise use the unknown token\n",
    "                word = 'UNK'\n",
    "            # print(self.V[word],len(self.V),word,(torch.LongTensor([int(self.V[word])])))\n",
    "            currentNode = (self.embedding(Var(torch.LongTensor([int(self.V[word])]))))\n",
    "        else: currentNode = self.nonLinear(self.W(torch.cat((self.traverse(node.left),self.traverse(node.right)),1)))\n",
    "        currentNode = currentNode/(torch.norm(currentNode))\n",
    "\n",
    "        assert node.label!=None\n",
    "        self.nodeProbList.append(self.projection(currentNode))\n",
    "        # print (node.label)\n",
    "        self.labelList.append(torch.LongTensor([node.label]))\n",
    "        loss_weight = 1-self.beta if node.annotated else self.beta\n",
    "        self.loss += (loss_weight*F.cross_entropy(input=torch.cat([self.projection(currentNode)]),target=Var(torch.cat([torch.LongTensor([node.label])]))))\n",
    "\n",
    "        #\n",
    "        if not node.isRoot():\n",
    "            if self.total_rep is None:\n",
    "                self.total_rep = currentNode.data.clone()\n",
    "            else:\n",
    "                self.total_rep += currentNode.data.clone()\n",
    "            self.count_rep += 1\n",
    "        #\n",
    "\n",
    "        return currentNode        \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.nodeProbList = []\n",
    "        self.labelList = []\n",
    "        self.loss = Var(torch.FloatTensor([0]))\n",
    "        self.traverse(x)\n",
    "        self.labelList = Var(torch.cat(self.labelList))\n",
    "        return torch.cat(self.nodeProbList)\n",
    "\n",
    "    def getLoss(self, tree):\n",
    "        nodes = self.forward(tree)\n",
    "        predictions = nodes.max(dim=1)[1]\n",
    "        loss = self.loss\n",
    "        return predictions,loss\n",
    "\n",
    "    def getRep(self, tree):\n",
    "        self.count_rep = 0\n",
    "        self.total_rep = None\n",
    "        self.nodeProbList = []\n",
    "        self.labelList = []\n",
    "        self.loss = Var(torch.FloatTensor([0]))\n",
    "\n",
    "        root_rep = self.traverse(tree)\n",
    "\n",
    "        return (torch.cat((root_rep,self.total_rep/self.count_rep),1)).data.numpy().T.flatten()\n",
    "\n",
    "\n",
    "    def evaluate(self, trees):\n",
    "            pbar = progressbar.ProgressBar(widgets=widgets, maxval=len(trees)).start()\n",
    "            n = nAll = correctRoot = correctAll = 0.0\n",
    "            tp = [1e-2]*self.numClasses\n",
    "            fp = [1e-2]*self.numClasses\n",
    "            fn = [1e-2]*self.numClasses\n",
    "            f1 = [0.]*self.numClasses\n",
    "            for j, tree in enumerate(trees):\n",
    "                predictions,_ = self.getLoss(tree.root)\n",
    "#                     print((predictions.cpu().data).numpy(),(predictions.cpu().data).numpy().shape)\n",
    "#                     print((self.labelList.cpu().data).numpy(), (self.labelList.cpu().data).numpy().shape)\n",
    "                correct = ((predictions.cpu().data).numpy()==(self.labelList.cpu().data).numpy())\n",
    "#                     print(correct)\n",
    "                correctAll += correct.sum()\n",
    "                nAll += np.shape(correct.squeeze())[0] if np.size(correct)!=1 else 1 \n",
    "                correctRoot += correct.squeeze()[-1] if np.size(correct)!=1 else correct[-1]\n",
    "#                     print(correct.squeeze()[-1] if np.size(correct)!=1 else correct[-1])\n",
    "#                     print('actual: {}'.format(tree.root.label))\n",
    "                for i in range(self.numClasses):\n",
    "                    size = np.size((predictions.cpu().data).numpy())\n",
    "                    if size!=1:\n",
    "                        pred = (predictions.cpu().data).numpy().squeeze()[-1]\n",
    "                        actual = (self.labelList.cpu().data).numpy().squeeze()[-1]\n",
    "                    else:\n",
    "                        pred = (predictions.cpu().data).numpy()[-1]\n",
    "                        actual = (self.labelList.cpu().data).numpy()[-1]\n",
    "                    if pred==i and actual==i:\n",
    "                        tp[i]+=1\n",
    "                    elif pred==i and actual!=i:\n",
    "                        fn[i]+=1\n",
    "                    elif pred==i and actual!=i:\n",
    "                        fp[i]+=1\n",
    "                n += 1\n",
    "                pbar.update(j)\n",
    "#             print(tp,fp,fn)\n",
    "            for i in range(self.numClasses):\n",
    "                p =(1.0*tp[i]/(tp[i]+fp[i]))\n",
    "                r =(1.0*tp[i]/(tp[i]+fn[i]))\n",
    "                f1[i] = (2*p*r)/(p+r)\n",
    "            pbar.finish()\n",
    "            return correctRoot / n, correctAll/nAll, f1\n",
    "\n",
    "    def eval_sent_lvl(self,trees,clf):\n",
    "        pbar = progressbar.ProgressBar(widgets=widgets, maxval=len(trees)).start()\n",
    "        n = nAll = correctRoot = correctAll = 0.0\n",
    "        X_predict = []\n",
    "        Y_gold = []\n",
    "        for j, tree in enumerate(trees):\n",
    "            tree_rep = model.getRep(tree.root)\n",
    "            X_predict.append(tree_rep)\n",
    "            Y_gold.append(tree.root.label)\n",
    "        acc = clf.score(np.array(X_predict),np.array(Y_gold))\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_trees = 'econ_step1_trees.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pickle.load(open(\"./trees/df_\"+file_trees, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = set(sum(df_train['tokens'],[]))\n",
    "\n",
    "word2idx = {}\n",
    "word2idx['UNK']=0\n",
    "i = 1\n",
    "for token in list(all_tokens):\n",
    "    word2idx[token] = i\n",
    "    i+=1\n",
    "\n",
    "idx2word = {v: k for k, v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: 'stood',\n",
       " 2: 'of',\n",
       " 3: 'made',\n",
       " 4: 'properties',\n",
       " 5: 'Kaiser',\n",
       " 6: 'Tamluk',\n",
       " 7: 'welcomed',\n",
       " 8: 'Thanks',\n",
       " 9: 'MLC',\n",
       " 10: 'vaccination',\n",
       " 11: 'bench',\n",
       " 12: 'Ramesh',\n",
       " 13: 'Ajitpawar',\n",
       " 14: 'affordable',\n",
       " 15: 'generations',\n",
       " 16: 'criticizing',\n",
       " 17: 'agencies',\n",
       " 18: 'afresh',\n",
       " 19: 'token',\n",
       " 20: 'Emphasising',\n",
       " 21: 'players',\n",
       " 22: 'driven',\n",
       " 23: 'Mulayam',\n",
       " 24: 'entrepreneurial',\n",
       " 25: 'encouraging',\n",
       " 26: 'conducting',\n",
       " 27: 'clinched',\n",
       " 28: 'multinational',\n",
       " 29: 'relaxed',\n",
       " 30: 'investing',\n",
       " 31: 'conceal',\n",
       " 32: 'milk',\n",
       " 33: 'conversion',\n",
       " 34: 'living',\n",
       " 35: 'realisation',\n",
       " 36: 'Canada',\n",
       " 37: 'diluted',\n",
       " 38: 'coal',\n",
       " 39: 'Raizada',\n",
       " 40: 'Guidelinesfarmer',\n",
       " 41: 'weekly',\n",
       " 42: 'store',\n",
       " 43: 'evasion',\n",
       " 44: 'extended',\n",
       " 45: 'determined',\n",
       " 46: 'ministerial',\n",
       " 47: 'hotbed',\n",
       " 48: 'brought',\n",
       " 49: 'magisterial',\n",
       " 50: 'clipping',\n",
       " 51: 'rightful',\n",
       " 52: 'inclusive',\n",
       " 53: 'separatism',\n",
       " 54: 'delivers',\n",
       " 55: 'totally',\n",
       " 56: 'Pandey',\n",
       " 57: 'nutritious',\n",
       " 58: 'participation',\n",
       " 59: 'Chandravana',\n",
       " 60: 'spreading',\n",
       " 61: 'institute',\n",
       " 62: 'constitutional',\n",
       " 63: 'thanks',\n",
       " 64: 'Led',\n",
       " 65: 'terminate',\n",
       " 66: 'indeed',\n",
       " 67: 'combined',\n",
       " 68: 'Sangathan',\n",
       " 69: 'adopting',\n",
       " 70: 'thus',\n",
       " 71: 'human',\n",
       " 72: 'needs',\n",
       " 73: 'indication',\n",
       " 74: 'national',\n",
       " 75: 'recourse',\n",
       " 76: 'bothered',\n",
       " 77: 'initiative',\n",
       " 78: 'earnings',\n",
       " 79: 'overview',\n",
       " 80: 'acquired',\n",
       " 81: 'kings',\n",
       " 82: 'inconclusive',\n",
       " 83: 'copying',\n",
       " 84: 'industry',\n",
       " 85: 'Akkayyapalem',\n",
       " 86: 'STS',\n",
       " 87: 'invoices',\n",
       " 88: 'aspiring',\n",
       " 89: 'attaining',\n",
       " 90: 'resign',\n",
       " 91: 'United',\n",
       " 92: 'Medinipur',\n",
       " 93: 'income',\n",
       " 94: 'MRP',\n",
       " 95: 'address',\n",
       " 96: 'affected',\n",
       " 97: 'disbursement',\n",
       " 98: 'chairman',\n",
       " 99: 'Badly',\n",
       " 100: 'another',\n",
       " 101: 'alluding',\n",
       " 102: 'amnesty',\n",
       " 103: 'Insurance',\n",
       " 104: 'curbs',\n",
       " 105: 'create',\n",
       " 106: 'served',\n",
       " 107: 'Mandal',\n",
       " 108: 'micro',\n",
       " 109: 'garlic',\n",
       " 110: 'spurious',\n",
       " 111: 'assurances',\n",
       " 112: 'pressing',\n",
       " 113: 'displays',\n",
       " 114: 'cot',\n",
       " 115: 'Denomination',\n",
       " 116: 'protocols',\n",
       " 117: 'dealt',\n",
       " 118: 'adjournment',\n",
       " 119: 'vote',\n",
       " 120: 'Padayatra',\n",
       " 121: 'whereas',\n",
       " 122: 'joins',\n",
       " 123: 'irks',\n",
       " 124: 'declared',\n",
       " 125: 'Gfpc',\n",
       " 126: 'justified',\n",
       " 127: 'midnight',\n",
       " 128: 'charged',\n",
       " 129: 'dialogue',\n",
       " 130: 'Dussehra',\n",
       " 131: 'scared',\n",
       " 132: 'Clean',\n",
       " 133: 'gardens',\n",
       " 134: 'Madukkarai',\n",
       " 135: 'contesting',\n",
       " 136: 'invented',\n",
       " 137: 'insured',\n",
       " 138: 'colleagues',\n",
       " 139: 'former',\n",
       " 140: 'Dharna',\n",
       " 141: 'adopted',\n",
       " 142: 'inherited',\n",
       " 143: 'redefined',\n",
       " 144: 'elections',\n",
       " 145: 'Palaniswami',\n",
       " 146: 'Pms',\n",
       " 147: 'pace',\n",
       " 148: 'fee',\n",
       " 149: 'locked',\n",
       " 150: 'villages',\n",
       " 151: 'Gurdaspur',\n",
       " 152: 'compared',\n",
       " 153: 'timeline',\n",
       " 154: 'forever',\n",
       " 155: 'case',\n",
       " 156: 'validity',\n",
       " 157: 'concentrate',\n",
       " 158: 'intermittent',\n",
       " 159: 'endorsing',\n",
       " 160: 'alive',\n",
       " 161: 'sudden',\n",
       " 162: 'Jawans',\n",
       " 163: 'enhancement',\n",
       " 164: 'acres',\n",
       " 165: 'openly',\n",
       " 166: 'Highlighting',\n",
       " 167: 'pending',\n",
       " 168: 'focusing',\n",
       " 169: 'Linking',\n",
       " 170: 'exists',\n",
       " 171: 'rectify',\n",
       " 172: 'mimicked',\n",
       " 173: 'restructured',\n",
       " 174: 'So',\n",
       " 175: 'April',\n",
       " 176: 'inflicted',\n",
       " 177: 'Treasury',\n",
       " 178: 'custodian',\n",
       " 179: 'Countering',\n",
       " 180: 'comment',\n",
       " 181: 'global',\n",
       " 182: 'Deshpande',\n",
       " 183: 'Kullu',\n",
       " 184: 'And',\n",
       " 185: 'morally',\n",
       " 186: 'using',\n",
       " 187: 'hike',\n",
       " 188: 'general',\n",
       " 189: 'reflect',\n",
       " 190: 'labourers',\n",
       " 191: 'along',\n",
       " 192: 'potential',\n",
       " 193: 'exports',\n",
       " 194: 'genuine',\n",
       " 195: 'shaped',\n",
       " 196: 'relating',\n",
       " 197: 'hopefully',\n",
       " 198: 'aggregate',\n",
       " 199: 'improve',\n",
       " 200: 'UDF',\n",
       " 201: 'Pressing',\n",
       " 202: 'furnish',\n",
       " 203: 'sops',\n",
       " 204: 'control',\n",
       " 205: 'finalised',\n",
       " 206: 'party',\n",
       " 207: 'rupee',\n",
       " 208: 'transforming',\n",
       " 209: 'agrarian',\n",
       " 210: 'longer',\n",
       " 211: 'predicted',\n",
       " 212: 'cues',\n",
       " 213: 'Lfegtkkfb',\n",
       " 214: 'plots',\n",
       " 215: 'Poor',\n",
       " 216: 'junior',\n",
       " 217: 'series',\n",
       " 218: 'replenished',\n",
       " 219: 'Ptinagpur',\n",
       " 220: 'Continuing',\n",
       " 221: 'faith',\n",
       " 222: 'embed',\n",
       " 223: 'feedback',\n",
       " 224: 'He',\n",
       " 225: 'overreach',\n",
       " 226: 'Rajasthan',\n",
       " 227: 'Rafale',\n",
       " 228: 'moderate',\n",
       " 229: 'platform',\n",
       " 230: 'away',\n",
       " 231: 'crossing',\n",
       " 232: 'director',\n",
       " 233: 'recalling',\n",
       " 234: 'relation',\n",
       " 235: 'politically',\n",
       " 236: 'indicates',\n",
       " 237: 'intends',\n",
       " 238: 'speculations',\n",
       " 239: 'fertilizers',\n",
       " 240: 'agreement',\n",
       " 241: 'successfully',\n",
       " 242: 'mitigating',\n",
       " 243: 'birth',\n",
       " 244: 'check',\n",
       " 245: 'progress',\n",
       " 246: 'necessitate',\n",
       " 247: 'CST',\n",
       " 248: 'wallets',\n",
       " 249: 'model',\n",
       " 250: 'Harkishan',\n",
       " 251: 'dramatic',\n",
       " 252: 'grabbed',\n",
       " 253: 'Surat',\n",
       " 254: 'visited',\n",
       " 255: 'Gehlot',\n",
       " 256: 'preparing',\n",
       " 257: 'proposed',\n",
       " 258: 'recommended',\n",
       " 259: 'toll',\n",
       " 260: 'Arjun',\n",
       " 261: 'liabilities',\n",
       " 262: 'were',\n",
       " 263: 'topic',\n",
       " 264: 'CAG',\n",
       " 265: 'sacrilege',\n",
       " 266: 'infrastructural',\n",
       " 267: 'leveraging',\n",
       " 268: 'turf',\n",
       " 269: 'operational',\n",
       " 270: 'Rahulanantapur',\n",
       " 271: 'Morenew',\n",
       " 272: 'Ftapcci',\n",
       " 273: 'eased',\n",
       " 274: 'seriousness',\n",
       " 275: 'sole',\n",
       " 276: 'cricketer',\n",
       " 277: 'Videoaddressing',\n",
       " 278: 'gave',\n",
       " 279: 'beat',\n",
       " 280: 'rest',\n",
       " 281: 'disappointing',\n",
       " 282: 'haemorrhage',\n",
       " 283: 'Shirdi',\n",
       " 284: 'raise',\n",
       " 285: 'three',\n",
       " 286: 'None',\n",
       " 287: 'revise',\n",
       " 288: 'throwing',\n",
       " 289: 'Clearancethe',\n",
       " 290: 'attendance',\n",
       " 291: 'organisers',\n",
       " 292: 'bars',\n",
       " 293: 'programmes',\n",
       " 294: 'scientists',\n",
       " 295: 'raising',\n",
       " 296: 'km',\n",
       " 297: 'saints',\n",
       " 298: 'They',\n",
       " 299: 'failed',\n",
       " 300: 'Kenyan',\n",
       " 301: 'foes',\n",
       " 302: 'smoking',\n",
       " 303: 'entities',\n",
       " 304: 'Mandis',\n",
       " 305: 'economical',\n",
       " 306: 'Omarabdullah',\n",
       " 307: 'attorney',\n",
       " 308: 'digital',\n",
       " 309: 'chilly',\n",
       " 310: 'admitted',\n",
       " 311: 'Japan',\n",
       " 312: 'Ani_News',\n",
       " 313: 'insurance',\n",
       " 314: 'Months',\n",
       " 315: 'actual',\n",
       " 316: 'minds',\n",
       " 317: 'scheduled',\n",
       " 318: 'Yogiadityanath',\n",
       " 319: 'hoarded',\n",
       " 320: 'produces',\n",
       " 321: 'economy',\n",
       " 322: 'similar',\n",
       " 323: 'transactions',\n",
       " 324: 'gets',\n",
       " 325: 'Whitefly',\n",
       " 326: 'Pmksy',\n",
       " 327: 'refunding',\n",
       " 328: 'Demonetisations',\n",
       " 329: 'reviewing',\n",
       " 330: 'IHS',\n",
       " 331: 'anchor',\n",
       " 332: 'governance',\n",
       " 333: 'stalemate',\n",
       " 334: 'I',\n",
       " 335: 'Rera',\n",
       " 336: 'flop',\n",
       " 337: 'Istseeking',\n",
       " 338: 'housing',\n",
       " 339: 'settled',\n",
       " 340: 'Crisils',\n",
       " 341: 'Newsat',\n",
       " 342: 'Micro',\n",
       " 343: 'robbery',\n",
       " 344: 'UID',\n",
       " 345: 'gain',\n",
       " 346: 'good',\n",
       " 347: 'PM',\n",
       " 348: 'distributing',\n",
       " 349: 'Bahujan',\n",
       " 350: 'occupation',\n",
       " 351: 'immature',\n",
       " 352: 'goes',\n",
       " 353: 'lawfully',\n",
       " 354: 'Paribas',\n",
       " 355: 'system',\n",
       " 356: 'staunchly',\n",
       " 357: 'affect',\n",
       " 358: 'mentors',\n",
       " 359: 'quarter',\n",
       " 360: 'jokes',\n",
       " 361: 'accommodate',\n",
       " 362: 'Ptijaipur',\n",
       " 363: 'lawyer',\n",
       " 364: 'Majdoor',\n",
       " 365: 'Agricultural',\n",
       " 366: 'Mamata',\n",
       " 367: 'editors',\n",
       " 368: 'slyly',\n",
       " 369: 'appointed',\n",
       " 370: 'Ruing',\n",
       " 371: 'outside',\n",
       " 372: 'Initiating',\n",
       " 373: 'greater',\n",
       " 374: 'NPR',\n",
       " 375: 'allotted',\n",
       " 376: 'failures',\n",
       " 377: 'fishing',\n",
       " 378: 'flexibility',\n",
       " 379: 'exclusion',\n",
       " 380: 'maximum',\n",
       " 381: 'rationalised',\n",
       " 382: 'emerged',\n",
       " 383: 'sad',\n",
       " 384: 'PERSONwith',\n",
       " 385: 'Defaulters',\n",
       " 386: 'BMW',\n",
       " 387: 'Karnataka',\n",
       " 388: 'NRC',\n",
       " 389: 'minutes',\n",
       " 390: 'Pradhikaran',\n",
       " 391: 'trade',\n",
       " 392: 'Prime',\n",
       " 393: 'ID',\n",
       " 394: 'Noting',\n",
       " 395: 'Polyhouses',\n",
       " 396: 'liquidity',\n",
       " 397: 'roil',\n",
       " 398: 'Targeting',\n",
       " 399: 'landless',\n",
       " 400: 'overseas',\n",
       " 401: 'granting',\n",
       " 402: 'sincerely',\n",
       " 403: 'usurious',\n",
       " 404: 'keen',\n",
       " 405: 'pockets',\n",
       " 406: 'decreased',\n",
       " 407: 'Faridkot',\n",
       " 408: 'consume',\n",
       " 409: 'confirmation',\n",
       " 410: 'freedom',\n",
       " 411: 'marks',\n",
       " 412: 'forum',\n",
       " 413: 'ruined',\n",
       " 414: 'comptroller',\n",
       " 415: 'PMI',\n",
       " 416: 'depended',\n",
       " 417: 'Neel',\n",
       " 418: 'bathing',\n",
       " 419: 'heed',\n",
       " 420: 'explore',\n",
       " 421: 'notification',\n",
       " 422: 'Kushinagar',\n",
       " 423: 'testimonials',\n",
       " 424: 'facility',\n",
       " 425: 'huge',\n",
       " 426: 'potato',\n",
       " 427: 'acceptable',\n",
       " 428: 'inconveniences',\n",
       " 429: 'limitations',\n",
       " 430: 'amount',\n",
       " 431: 'platinum',\n",
       " 432: 'then',\n",
       " 433: 'Environment',\n",
       " 434: 'analysis',\n",
       " 435: 'historic',\n",
       " 436: 'Droughtchief',\n",
       " 437: 'adopts',\n",
       " 438: 'almost',\n",
       " 439: 'US',\n",
       " 440: 'Fcv',\n",
       " 441: 'First',\n",
       " 442: 'clearer',\n",
       " 443: 'double',\n",
       " 444: 'et',\n",
       " 445: 'costs',\n",
       " 446: 'hurriedly',\n",
       " 447: 'recommendation',\n",
       " 448: 'chairmen',\n",
       " 449: 'juices',\n",
       " 450: 'The',\n",
       " 451: 'selective',\n",
       " 452: 'stay',\n",
       " 453: 're',\n",
       " 454: 'banker',\n",
       " 455: 'dearer',\n",
       " 456: 'Maharashtra',\n",
       " 457: 'moneys',\n",
       " 458: 'entity',\n",
       " 459: 'unsustainable',\n",
       " 460: 'presenting',\n",
       " 461: 'Mills',\n",
       " 462: 'present',\n",
       " 463: 'bolster',\n",
       " 464: 'broken',\n",
       " 465: 'agreeable',\n",
       " 466: 'Radhakrishnan',\n",
       " 467: 'Unthought',\n",
       " 468: 'computerised',\n",
       " 469: 'still',\n",
       " 470: 'watchers',\n",
       " 471: 'vigilantism',\n",
       " 472: 'NGOs',\n",
       " 473: 'as',\n",
       " 474: 'inward',\n",
       " 475: 'Fm',\n",
       " 476: 'incremental',\n",
       " 477: 'ministry',\n",
       " 478: 'recoveries',\n",
       " 479: 'accessible',\n",
       " 480: 'O',\n",
       " 481: 'agreeing',\n",
       " 482: 'Oct',\n",
       " 483: 'wholesale',\n",
       " 484: 'Benami',\n",
       " 485: 'juvenile',\n",
       " 486: 'alibi',\n",
       " 487: 'showpiece',\n",
       " 488: 'frequent',\n",
       " 489: 'serve',\n",
       " 490: 'Cag',\n",
       " 491: 'RBI',\n",
       " 492: 'IPL',\n",
       " 493: 'pitched',\n",
       " 494: 'Rajnathsingh',\n",
       " 495: 'New',\n",
       " 496: 'Cash',\n",
       " 497: 'inaugurating',\n",
       " 498: 'sufficiency',\n",
       " 499: 'Lauding',\n",
       " 500: 'Director',\n",
       " 501: 'prices',\n",
       " 502: 'television',\n",
       " 503: 'Apart',\n",
       " 504: 'accused',\n",
       " 505: 'expensive',\n",
       " 506: 'who',\n",
       " 507: 'Dbtl',\n",
       " 508: 'purview',\n",
       " 509: 'Paise',\n",
       " 510: 'Pakistan',\n",
       " 511: 'Aadhaar',\n",
       " 512: 'Sub',\n",
       " 513: 'Similarly',\n",
       " 514: 'alleged',\n",
       " 515: 'revised',\n",
       " 516: 'annual',\n",
       " 517: 'remain',\n",
       " 518: 'Jd',\n",
       " 519: 'Revanth',\n",
       " 520: 'rapturous',\n",
       " 521: 'scourges',\n",
       " 522: 'scanty',\n",
       " 523: 'ownership',\n",
       " 524: 'thrust',\n",
       " 525: 'Bjp',\n",
       " 526: 'Inclusionmumbai',\n",
       " 527: 'Siddaramaiahbangalore',\n",
       " 528: 'hrs',\n",
       " 529: 'array',\n",
       " 530: 'secretary',\n",
       " 531: 'inform',\n",
       " 532: 'North',\n",
       " 533: 'forged',\n",
       " 534: 'auctioning',\n",
       " 535: 'whole',\n",
       " 536: 'Amountearlier',\n",
       " 537: 'Istterming',\n",
       " 538: 'Lokpal',\n",
       " 539: 'dubbed',\n",
       " 540: 'counted',\n",
       " 541: 'updated',\n",
       " 542: 'employees',\n",
       " 543: 'addition',\n",
       " 544: 'Utsav',\n",
       " 545: 'Border',\n",
       " 546: 'clubs',\n",
       " 547: 'Corporation',\n",
       " 548: 'Myanmar',\n",
       " 549: 'Satya',\n",
       " 550: 'revenues',\n",
       " 551: 'deserving',\n",
       " 552: 'vain',\n",
       " 553: 'item',\n",
       " 554: 'ensuing',\n",
       " 555: 'Bathinda',\n",
       " 556: 'Yes',\n",
       " 557: 'prove',\n",
       " 558: 'controversial',\n",
       " 559: 'Immolated',\n",
       " 560: 'needy',\n",
       " 561: 'hogwash',\n",
       " 562: 'prefers',\n",
       " 563: 'changer',\n",
       " 564: 'lower',\n",
       " 565: 'colluding',\n",
       " 566: 'win',\n",
       " 567: 'neighbouring',\n",
       " 568: 'spike',\n",
       " 569: 'harvest',\n",
       " 570: 'principal',\n",
       " 571: 'election',\n",
       " 572: 'deduction',\n",
       " 573: 'district',\n",
       " 574: 'advance',\n",
       " 575: 'patterns',\n",
       " 576: 'identifier',\n",
       " 577: 'refresh',\n",
       " 578: 'undertake',\n",
       " 579: 'Tmc',\n",
       " 580: 'all',\n",
       " 581: 'legislators',\n",
       " 582: 'usurp',\n",
       " 583: 'handles',\n",
       " 584: 'task',\n",
       " 585: 'Marathwada',\n",
       " 586: 'fairly',\n",
       " 587: 'isolated',\n",
       " 588: 'busted',\n",
       " 589: 'Sunday',\n",
       " 590: 'Ajmer',\n",
       " 591: 'Lawyer',\n",
       " 592: 'Sangli',\n",
       " 593: 'Jalandhar',\n",
       " 594: 'blunder',\n",
       " 595: 'Raigaon',\n",
       " 596: 'house',\n",
       " 597: 'Christmas',\n",
       " 598: 'instant',\n",
       " 599: 'dates',\n",
       " 600: 'failure',\n",
       " 601: 'negative',\n",
       " 602: 'witnessed',\n",
       " 603: 'restriction',\n",
       " 604: 'lauding',\n",
       " 605: 'Apcthe',\n",
       " 606: 'Etcwe',\n",
       " 607: 'repaying',\n",
       " 608: 'govts',\n",
       " 609: 'type',\n",
       " 610: 'encryption',\n",
       " 611: 'children',\n",
       " 612: 'honesty',\n",
       " 613: 'eastern',\n",
       " 614: 'Apologising',\n",
       " 615: 'circle',\n",
       " 616: 'queue',\n",
       " 617: 'arrives',\n",
       " 618: 'incomes',\n",
       " 619: 'Shrikant',\n",
       " 620: 'Otherwise',\n",
       " 621: 'convened',\n",
       " 622: 'inputs',\n",
       " 623: 'ceiling',\n",
       " 624: 'Mantri',\n",
       " 625: 'takes',\n",
       " 626: 'blue',\n",
       " 627: 'felt',\n",
       " 628: 'visible',\n",
       " 629: 'Action',\n",
       " 630: 'litres',\n",
       " 631: 'truth',\n",
       " 632: 'length',\n",
       " 633: 'Old',\n",
       " 634: 'generated',\n",
       " 635: 'Formalisation',\n",
       " 636: 'spared',\n",
       " 637: 'easy',\n",
       " 638: 'discusses',\n",
       " 639: 'charge',\n",
       " 640: 'bond',\n",
       " 641: 'Sgst',\n",
       " 642: 'bound',\n",
       " 643: 'institutional',\n",
       " 644: 'stated',\n",
       " 645: 'Capt',\n",
       " 646: 'static',\n",
       " 647: 'odds',\n",
       " 648: 'Reform',\n",
       " 649: 'Urgently',\n",
       " 650: 'reason',\n",
       " 651: 'respectable',\n",
       " 652: 'changes',\n",
       " 653: 'rich',\n",
       " 654: 'rising',\n",
       " 655: 'Siachen',\n",
       " 656: 'HRD',\n",
       " 657: 'Almatti',\n",
       " 658: 'PERSONAc',\n",
       " 659: 'Communitys',\n",
       " 660: 'inter',\n",
       " 661: 'Left',\n",
       " 662: 'effectively',\n",
       " 663: 'checking',\n",
       " 664: 'outdated',\n",
       " 665: 'arising',\n",
       " 666: 'abroad',\n",
       " 667: 'asset',\n",
       " 668: 'tally',\n",
       " 669: 'lentils',\n",
       " 670: 'Thakur',\n",
       " 671: 'carry',\n",
       " 672: 'forming',\n",
       " 673: 'terrorists',\n",
       " 674: 'wounds',\n",
       " 675: 'calibrated',\n",
       " 676: 'Astodia',\n",
       " 677: 'Issuenew',\n",
       " 678: 'PERSONnew',\n",
       " 679: 'question',\n",
       " 680: 'cities',\n",
       " 681: 'drug',\n",
       " 682: 'brining',\n",
       " 683: 'picture',\n",
       " 684: 'televised',\n",
       " 685: 'Rampant',\n",
       " 686: 'keynote',\n",
       " 687: 'aimless',\n",
       " 688: 'veteran',\n",
       " 689: 'assent',\n",
       " 690: 'restricted',\n",
       " 691: 'avail',\n",
       " 692: 'fruitful',\n",
       " 693: 'Cdms',\n",
       " 694: 'shift',\n",
       " 695: 'Kamal',\n",
       " 696: 'assailing',\n",
       " 697: 'managed',\n",
       " 698: 'employed',\n",
       " 699: 'backlash',\n",
       " 700: 'Timenew',\n",
       " 701: 'doled',\n",
       " 702: 'portfolio',\n",
       " 703: 'Maidur',\n",
       " 704: 'China',\n",
       " 705: 'miseries',\n",
       " 706: 'World',\n",
       " 707: 'socially',\n",
       " 708: 'Kalasa',\n",
       " 709: 'fruits',\n",
       " 710: 'abundance',\n",
       " 711: 'misery',\n",
       " 712: 'Noteban',\n",
       " 713: 'volatility',\n",
       " 714: 'specifically',\n",
       " 715: 'eventually',\n",
       " 716: 'Bmr',\n",
       " 717: 'Ysr',\n",
       " 718: 'irreversible',\n",
       " 719: 'proliferation',\n",
       " 720: 'independent',\n",
       " 721: 'facilitating',\n",
       " 722: 'brainchild',\n",
       " 723: 'interact',\n",
       " 724: 'Slamming',\n",
       " 725: 'incurred',\n",
       " 726: 'auditor',\n",
       " 727: 'archaic',\n",
       " 728: 'roughly',\n",
       " 729: 'repricing',\n",
       " 730: 'appealed',\n",
       " 731: 'compensation',\n",
       " 732: 'preserving',\n",
       " 733: 'lip',\n",
       " 734: 'expeditiously',\n",
       " 735: 'prevent',\n",
       " 736: 'Bjps',\n",
       " 737: 'crime',\n",
       " 738: 'U',\n",
       " 739: 'proved',\n",
       " 740: 'spent',\n",
       " 741: 'improvement',\n",
       " 742: 'corresponding',\n",
       " 743: 'creation',\n",
       " 744: 'equal',\n",
       " 745: 'victory',\n",
       " 746: 'databases',\n",
       " 747: 'attack',\n",
       " 748: 'cells',\n",
       " 749: 'concerns',\n",
       " 750: 'troubles',\n",
       " 751: 'shame',\n",
       " 752: 'emphasized',\n",
       " 753: 'producing',\n",
       " 754: 'originally',\n",
       " 755: 'scaled',\n",
       " 756: 'LCD',\n",
       " 757: 'extremism',\n",
       " 758: 'partial',\n",
       " 759: 'came',\n",
       " 760: 'advantages',\n",
       " 761: 'Demonetise',\n",
       " 762: 'caps',\n",
       " 763: 'printing',\n",
       " 764: 'file',\n",
       " 765: 'presented',\n",
       " 766: 'serious',\n",
       " 767: 'veterinary',\n",
       " 768: 'passing',\n",
       " 769: 'Pmnew',\n",
       " 770: 'institutions',\n",
       " 771: 'advertise',\n",
       " 772: 'variety',\n",
       " 773: 'formalising',\n",
       " 774: 'unfazed',\n",
       " 775: 'IMF',\n",
       " 776: 'Registered',\n",
       " 777: 'coordination',\n",
       " 778: 'farmland',\n",
       " 779: 'nurture',\n",
       " 780: 'trick',\n",
       " 781: 'compel',\n",
       " 782: 'Agartala',\n",
       " 783: 'conveniently',\n",
       " 784: 'empower',\n",
       " 785: 'fin',\n",
       " 786: 'induce',\n",
       " 787: 'management',\n",
       " 788: 'false',\n",
       " 789: 'Press',\n",
       " 790: 'POS',\n",
       " 791: 'suppliers',\n",
       " 792: 'Whatever',\n",
       " 793: 'Tnieterming',\n",
       " 794: 'makers',\n",
       " 795: 'subsidized',\n",
       " 796: 'Further',\n",
       " 797: 'TMCs',\n",
       " 798: 'value',\n",
       " 799: 'renting',\n",
       " 800: 'Dave',\n",
       " 801: 'began',\n",
       " 802: 'Open',\n",
       " 803: 'prioritise',\n",
       " 804: 'pan',\n",
       " 805: 'defended',\n",
       " 806: 'intention',\n",
       " 807: 'whipped',\n",
       " 808: 'role',\n",
       " 809: 'British',\n",
       " 810: 'Dr',\n",
       " 811: 'Germany',\n",
       " 812: 'neglecting',\n",
       " 813: 'accountants',\n",
       " 814: 'Platform',\n",
       " 815: 'below',\n",
       " 816: 'curbed',\n",
       " 817: 'extract',\n",
       " 818: 'VAT',\n",
       " 819: 'nationality',\n",
       " 820: 'besides',\n",
       " 821: 'click',\n",
       " 822: 'exploration',\n",
       " 823: 'Stone',\n",
       " 824: 'migration',\n",
       " 825: 'criticised',\n",
       " 826: 'Istif',\n",
       " 827: 'Backing',\n",
       " 828: 'Dh',\n",
       " 829: 'Enrolment',\n",
       " 830: 'wings',\n",
       " 831: 'logistic',\n",
       " 832: 'kicking',\n",
       " 833: 'techniques',\n",
       " 834: 'Policiesin',\n",
       " 835: 'field',\n",
       " 836: 'repository',\n",
       " 837: 'Magisterial',\n",
       " 838: 'backtracked',\n",
       " 839: 'endangering',\n",
       " 840: 'uncomfortable',\n",
       " 841: 'Nilanga',\n",
       " 842: 'Srivastavwhile',\n",
       " 843: 'convener',\n",
       " 844: 'forest',\n",
       " 845: 'Council',\n",
       " 846: 'Jalyukt',\n",
       " 847: 'hugely',\n",
       " 848: 'Crisis',\n",
       " 849: 'enigma',\n",
       " 850: 'closer',\n",
       " 851: 'Kohar',\n",
       " 852: 'presence',\n",
       " 853: 's',\n",
       " 854: 'channels',\n",
       " 855: 'Markit',\n",
       " 856: 'contribution',\n",
       " 857: 'unlikely',\n",
       " 858: 'determination',\n",
       " 859: 'veiled',\n",
       " 860: 'Many',\n",
       " 861: 'Pandeynaik',\n",
       " 862: 'R',\n",
       " 863: 'Holding',\n",
       " 864: 'unfair',\n",
       " 865: 'lack',\n",
       " 866: 'snatched',\n",
       " 867: 'disenchanted',\n",
       " 868: 'Angadi',\n",
       " 869: 'chasing',\n",
       " 870: 'deepen',\n",
       " 871: 'ordered',\n",
       " 872: 'centuries',\n",
       " 873: 'Private',\n",
       " 874: 'coupled',\n",
       " 875: 'doubled',\n",
       " 876: 'educate',\n",
       " 877: 'Fundndrf',\n",
       " 878: 'commerce',\n",
       " 879: 'participating',\n",
       " 880: 'until',\n",
       " 881: 'Jknew',\n",
       " 882: 'Dgps',\n",
       " 883: 'Contrary',\n",
       " 884: 'quality',\n",
       " 885: 'Compensation',\n",
       " 886: 'Had',\n",
       " 887: 'affects',\n",
       " 888: 'acceleration',\n",
       " 889: 'Bengals',\n",
       " 890: 'Trinmool',\n",
       " 891: 'archive',\n",
       " 892: 'By',\n",
       " 893: 'soft',\n",
       " 894: 'pass',\n",
       " 895: 'Liquidity',\n",
       " 896: 'reportedly',\n",
       " 897: 'prominently',\n",
       " 898: 'breakthrough',\n",
       " 899: 'Europe',\n",
       " 900: 'sometimes',\n",
       " 901: 'fields',\n",
       " 902: 'wrongdoers',\n",
       " 903: 'private',\n",
       " 904: 'fisheries',\n",
       " 905: 'fateful',\n",
       " 906: 'Dismissing',\n",
       " 907: 'Which',\n",
       " 908: 'worsening',\n",
       " 909: 'machine',\n",
       " 910: 'filers',\n",
       " 911: 'Communist',\n",
       " 912: 'availed',\n",
       " 913: 'Byexpress',\n",
       " 914: 'rampant',\n",
       " 915: 'Realti',\n",
       " 916: 'Bound',\n",
       " 917: 'infusion',\n",
       " 918: 'tapped',\n",
       " 919: 'why',\n",
       " 920: 'withdraw',\n",
       " 921: 'Mitra',\n",
       " 922: 'professor',\n",
       " 923: 'road',\n",
       " 924: 'aware',\n",
       " 925: 'jam',\n",
       " 926: 'Cmchief',\n",
       " 927: 'sums',\n",
       " 928: 'accept',\n",
       " 929: 'college',\n",
       " 930: 'earmarked',\n",
       " 931: 'scrutiny',\n",
       " 932: 'Ghaziabad',\n",
       " 933: 'replace',\n",
       " 934: 'Saugata',\n",
       " 935: 'forces',\n",
       " 936: 'denominations',\n",
       " 937: 'apparent',\n",
       " 938: 'immediately',\n",
       " 939: 'leakages',\n",
       " 940: 'Soon',\n",
       " 941: 'flaws',\n",
       " 942: 'considered',\n",
       " 943: 'pointed',\n",
       " 944: 'measure',\n",
       " 945: 'privilege',\n",
       " 946: 'issue',\n",
       " 947: 'feed',\n",
       " 948: 'period',\n",
       " 949: 'petitions',\n",
       " 950: 'finalise',\n",
       " 951: 'awaiting',\n",
       " 952: 'quit',\n",
       " 953: 'extravagant',\n",
       " 954: 'layer',\n",
       " 955: 'dying',\n",
       " 956: 'accusing',\n",
       " 957: 'top',\n",
       " 958: 'West',\n",
       " 959: 'ruining',\n",
       " 960: 'landmark',\n",
       " 961: 'heeds',\n",
       " 962: 'could',\n",
       " 963: 'readjust',\n",
       " 964: 'Arundhatibhattacharya',\n",
       " 965: 'functioning',\n",
       " 966: 'Urban',\n",
       " 967: 'monetary',\n",
       " 968: 'pioneering',\n",
       " 969: 'DCs',\n",
       " 970: 'Cr',\n",
       " 971: 'mentioned',\n",
       " 972: 'Berlin',\n",
       " 973: 'paralysis',\n",
       " 974: 'protests',\n",
       " 975: 'outsourcing',\n",
       " 976: 'lollipops',\n",
       " 977: 'authorities',\n",
       " 978: 'Ashrama',\n",
       " 979: 'rationale',\n",
       " 980: 'DY',\n",
       " 981: 'full',\n",
       " 982: 'services',\n",
       " 983: 'physical',\n",
       " 984: 'serving',\n",
       " 985: 'financing',\n",
       " 986: 'Wednesday',\n",
       " 987: 'biting',\n",
       " 988: 'airlifted',\n",
       " 989: 'article',\n",
       " 990: 'SEBI',\n",
       " 991: 'Prithipal',\n",
       " 992: 'Pranabmukherjee',\n",
       " 993: 'respected',\n",
       " 994: 'exist',\n",
       " 995: 'encourage',\n",
       " 996: 'existence',\n",
       " 997: 'quintal',\n",
       " 998: 'Prem',\n",
       " 999: 'waters',\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fine tuned + blacked out embeddings . .\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Users/navreetkaur/MTP/finetune-word2vec/w2v-models/economic-word2vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-690231ec0f6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Using fine tuned + blacked out embeddings . .\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meconomic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0munbiased_in_domain_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Users/navreetkaur/MTP/finetune-word2vec/w2v-models/economic-word2vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0munbiased_in_domain_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/navreetkaur/MTP/finetune-word2vec/w2v-models/tech-word2vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWordEmbeddingsKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastTextKeyedVectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'compatible_hash'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m     \"\"\"\n\u001b[0;32m-> 1381\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mode should be a string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mfobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shortcut_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Users/navreetkaur/MTP/finetune-word2vec/w2v-models/economic-word2vec'"
     ]
    }
   ],
   "source": [
    "if not w2vec:\n",
    "    print(\"Using fine tuned + blacked out embeddings . .\")\n",
    "    if economic:\n",
    "        unbiased_in_domain_model = KeyedVectors.load('Users/navreetkaur/MTP/finetune-word2vec/w2v-models/economic-word2vec')\n",
    "    else:\n",
    "        unbiased_in_domain_model = KeyedVectors.load('/Users/navreetkaur/MTP/finetune-word2vec/w2v-models/tech-word2vec')\n",
    "    if ner:\n",
    "        unbiased_in_domain_model+='-ner'\n",
    "    if blackout:\n",
    "        unbiased_in_domain_model+='-blackout'\n",
    "    \n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('/Users/navreetkaur/MTP/finetune-word2vec/w2v-models/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = np.zeros((len(idx2word), word2vec_model.vector_size))\n",
    "\n",
    "for idx, word in idx2word.items():\n",
    "    if not w2vec:\n",
    "        try: \n",
    "            embed[idx] = unbiased_in_domain_model[word]\n",
    "        except KeyError:\n",
    "            try:\n",
    "                embed[idx] = word2vec_model[word]\n",
    "            except:\n",
    "                embed[idx] = np.random.normal(size=(word2vec_model.vector_size, ))\n",
    "    else:\n",
    "        try:\n",
    "            embed[idx] = word2vec_model[word]\n",
    "        except:\n",
    "            embed[idx] = np.random.normal(size=(word2vec_model.vector_size, ))\n",
    "        \n",
    "embed = torch.from_numpy(embed)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    2495\n",
       "1    2495\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby('target').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.shape, len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tree(text):\n",
    "#     print(text)\n",
    "    output = nlp.annotate(text, properties={\n",
    "        'annotators': 'tokenize,ssplit,pos,depparse,parse',\n",
    "        'outputFormat': 'json'\n",
    "    })\n",
    "    output = literal_eval(output)\n",
    "    try:\n",
    "        tree = str(output['sentences'][0]['parse'])\n",
    "    except:\n",
    "        print(output,text)\n",
    "        return\n",
    "    # print (tree)\n",
    "    parse_string = ' '.join(str(tree).split())\n",
    "    # print(parse_string)\n",
    "    # print (\"\\n\\n\")\n",
    "    tree = nltk.tree.Tree.fromstring(parse_string)\n",
    "    tree.chomsky_normal_form()\n",
    "    tree.collapse_unary(collapseRoot=True,collapsePOS=True)\n",
    "    nt = convertNLTK_tree(tree)\n",
    "    return nt\n",
    "\n",
    "def printLabelTree(tree):\n",
    "    def inorder(node,nnode):\n",
    "        if node.isLeaf:\n",
    "            newnode = pptree.Node('H',nnode)\n",
    "            wnode = pptree.Node(node.word,newnode)\n",
    "        elif nnode is not None:\n",
    "            newnode = pptree.Node('H',nnode)\n",
    "            inorder(node.left,newnode)\n",
    "            inorder(node.right,newnode)\n",
    "        elif node.isRoot():\n",
    "            newnode = pptree.Node('H')\n",
    "            inorder(node.left,newnode)\n",
    "            inorder(node.right,newnode)\n",
    "            return newnode\n",
    "        return None\n",
    "    pptree.print_tree(inorder(tree.root,None))\n",
    "\n",
    "def create_trees_using_df(df):\n",
    "    tree = []\n",
    "    for tokens in list(df['tokens']):\n",
    "        if len(tokens)==0:\n",
    "            continue\n",
    "        line = ' '.join(tokens)\n",
    "        line += '\\n'\n",
    "        tree.append(make_tree(line))\n",
    "    return tree\n",
    "\n",
    "def printlabel(root,l):\n",
    "    if root:\n",
    "        l.append(root.label)\n",
    "#         print(root.label)\n",
    "        if root.left:\n",
    "            l+=printlabel(root.left,[])\n",
    "#             print(printlabel(root.left))\n",
    "        if root.right:\n",
    "            l+=printlabel(root.right,[])\n",
    "#             print(printlabel(root.right))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = StanfordCoreNLP('http://localhost', port=9000,timeout=90000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neutral = create_trees_using_df(df_train[df_train.target == 0])\n",
    "# anti = create_trees_using_df(df_train[df_train.target == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_neutral = create_trees_using_df(df_train[df_train.target == 1])\n",
    "# pro = create_trees_using_df(df_train[df_train.target == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neutral_test = create_trees_using_df(df_test[df_test.target == 0])\n",
    "# non_neutral_test = create_trees_using_df(df_test[df_test.target == 1])\n",
    "# anti_test = create_trees_using_df(df_test[df_test.target == 0])\n",
    "# pro_test = create_trees_using_df(df_test[df_test.target == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fout = open(file_trees,'wb')\n",
    "# pickle.dump([pro, anti],fout)\n",
    "# fout.close()\n",
    "# fout = open(file_trees+\"_test\",'wb')\n",
    "# pickle.dump([pro_test, anti_test],fout)\n",
    "# fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[pro,anti] = pickle.load(open(os.path.join('./trees',file_trees),'rb'))\n",
    "[pro_test,anti_test] = pickle.load(open(os.path.join('./trees',file_trees+\"_test\"),'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fout = open(file_trees,'wb')\n",
    "# pickle.dump([neutral, non_neutral],fout)\n",
    "# fout.close()\n",
    "# fout = open(file_trees+'_test','wb')\n",
    "# pickle.dump([neutral_test, non_neutral_test],fout)\n",
    "# fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA=False\n",
    "def Var(v):\n",
    "    if CUDA: return Variable(v.cuda())\n",
    "    else: return Variable(v)\n",
    "    \n",
    "trees = []\n",
    "raw_words = []\n",
    "vocab = []\n",
    "\n",
    "# print(\"Loading trees...\")\n",
    "# [neutral, non_neutral] = pickle.load(open(file_trees,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for neutral_tree in neutral:\n",
    "#     neutral_tree.root.set_label('neutral')\n",
    "# for non_neutral_tree in non_neutral:\n",
    "#     non_neutral_tree.root.set_label('non_neutral')\n",
    "    \n",
    "for pro_tree in pro:\n",
    "    pro_tree.root.set_label('pro')\n",
    "for anti_tree in anti:\n",
    "    anti_tree.root.set_label('anti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for neutral_tree in neutral_test:\n",
    "#     neutral_tree.root.set_label('neutral')\n",
    "# for non_neutral_tree in non_neutral_test:\n",
    "#     non_neutral_tree.root.set_label('non_neutral')\n",
    "    \n",
    "for pro_tree in pro_test:\n",
    "    pro_tree.root.set_label('pro')\n",
    "for anti_tree in anti_test:\n",
    "    anti_tree.root.set_label('anti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(neutral,non_neutral):\n",
    "    trees = []\n",
    "    trees.extend(neutral)\n",
    "    trees.extend(non_neutral)\n",
    "    random.shuffle(trees)\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mytree import *\n",
    "from treeUtil import *\n",
    "# from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "val = {'pro':0,'anti':1,'default':-1}\n",
    "\n",
    "# pro:1, anti:0, neutral:2\n",
    "val_all = {'pro':1,'anti':0,'default':-1,'neutral':2}\n",
    "# neutral:0, non-neutral:1\n",
    "val_neutral = {'neutral':0, 'non_neutral':1, 'default':-1}\n",
    "\n",
    "\n",
    "def convert(T):\n",
    "    label = val[T.label] if (hasattr(T,'label')) else None\n",
    "    print(label)\n",
    "    newTree = convert_primary_new(T,label)\n",
    "    annotate_all(newTree)\n",
    "\n",
    "    return newTree\n",
    "\n",
    "def convert_neutral(T):\n",
    "    label = val_neutral[T.label] if (hasattr(T,'label')) else None\n",
    "    print(label)\n",
    "    newTree = convert_primary_new(T,label)\n",
    "    annotate_all(newTree)\n",
    "\n",
    "    return newTree\n",
    "\n",
    "def convert_all(T):\n",
    "    label = val_all[T.label] if (hasattr(T,'label')) else None\n",
    "    print(label)\n",
    "    newTree = convert_primary_new(T,label)\n",
    "    annotate_all(newTree)\n",
    "\n",
    "    return newTree\n",
    "\n",
    "\n",
    "def convert_primary(T):\n",
    "    if (hasattr(T,'label')):\n",
    "        print(T.label) \n",
    "    label = val[T.label] if (hasattr(T,'label')) else None\n",
    "    # label = val[T.label] if (hasattr(T,'label') ) else None # changed for ignoring neutral\n",
    "\n",
    "    if isinstance(T,leafObj):\n",
    "        newTree = Node(label,T.word,T.pos)\n",
    "        newTree.isLeaf = True\n",
    "        return newTree\n",
    "    else:\n",
    "        newTree = Node(label)\n",
    "    \n",
    "    leftChild = convert_primary(T.c1)\n",
    "    rightChild = convert_primary(T.c2)\n",
    "    leftChild.parent = newTree\n",
    "    rightChild.parent = newTree\n",
    "\n",
    "    newTree.left = leftChild\n",
    "    newTree.right = rightChild\n",
    "\n",
    "    return newTree\n",
    "\n",
    "def convert_primary_new(T,label):\n",
    "    # from IPython import embed; embed()\n",
    "    if T is None:\n",
    "        return None\n",
    "    # label = val[T.label] if (hasattr(T,'label') ) else None # changed for ignoring neutral\n",
    "    T.set_label(label)\n",
    "    # if (T.isLeaf) : print (T.word)\n",
    "\n",
    "    T.left = convert_primary_new(T.left,label)\n",
    "    T.right = convert_primary_new(T.right,label)\n",
    "\n",
    "    return T\n",
    "\n",
    "    # if T.isLeaf:\n",
    "    #     newTree = Node(label,T.word,T.pos)\n",
    "    #     newTree.isLeaf = True\n",
    "    #     return newTree\n",
    "    # else:\n",
    "    #     newTree = Node(label)\n",
    "    \n",
    "    # leftChild = convert_primary_new(T.left)\n",
    "    # rightChild = convert_primary_new(T.right)\n",
    "    # leftChild.parent = newTree\n",
    "    # rightChild.parent = newTree\n",
    "\n",
    "    # newTree.left = leftChild\n",
    "    # newTree.right = rightChild\n",
    "\n",
    "    # return newTree\n",
    "\n",
    "def convertNLTK_tree_primary(tree):\n",
    "    if tree.height()==2:\n",
    "        newTree = Node('default',tree[0],None)\n",
    "        newTree.isLeaf = True\n",
    "        return newTree\n",
    "    newTree = Node('default')\n",
    "    leftChild = convertNLTK_tree_primary(tree[0])\n",
    "    rightChild = convertNLTK_tree_primary(tree[1])\n",
    "    \n",
    "    leftChild.parent = newTree\n",
    "    rightChild.parent = newTree\n",
    "\n",
    "    newTree.left = leftChild\n",
    "    newTree.right = rightChild\n",
    "\n",
    "    return newTree\n",
    "\n",
    "def convertNLTK_tree(tree):\n",
    "    return Tree(convertNLTK_tree_primary(tree))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def annotate_all(T):\n",
    "    if T == None: return\n",
    "    if T.label != None : \n",
    "        T.annotated = True\n",
    "    else:\n",
    "        T.annotated = False\n",
    "        T.set_label(T.parent.label)\n",
    "    annotate_all(T.left)\n",
    "    annotate_all(T.right)\n",
    "\n",
    "def buildBalTree(sent):\n",
    "    words = sent.split(' ')\n",
    "\n",
    "    nodes = words\n",
    "\n",
    "    while len(nodes)>1:\n",
    "        temp = []\n",
    "        for i in range(0,len(nodes),2):\n",
    "            lChild = Node(None,nodes[i],None) if isinstance(nodes[i],str) else nodes[i]\n",
    "            if i+1<len(nodes):\n",
    "                rChild = Node(None,nodes[i+1],None) if isinstance(nodes[i+1],str) else nodes[i+1]\n",
    "            else:\n",
    "                rChild = None\n",
    "            if isinstance(nodes[i],str):\n",
    "                lChild.isLeaf = True\n",
    "                if rChild is not None:\n",
    "                    rChild.isLeaf = True\n",
    "            newNode = Node(None)\n",
    "            lChild.parent = newNode\n",
    "            newNode.left = lChild\n",
    "            newNode.right = rChild\n",
    "            if rChild is not None:\n",
    "                rChild.parent = newNode\n",
    "            temp.append(newNode)\n",
    "        nodes=temp\n",
    "    return Tree(nodes[0])\n",
    "\n",
    "def readFile2Trees(filename):\n",
    "    trees = []\n",
    "    with open(filename,'r') as file:\n",
    "        for line in file:\n",
    "            if line=='\\n':\n",
    "                continue\n",
    "            else:\n",
    "                [labelname,sent] = line.split(': ',1)\n",
    "                tree = buildBalTree(sent)\n",
    "                tree.root.set_label(val[labelname])\n",
    "                if val[labelname]!=2:\n",
    "                    trees.append(tree)\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trees = combine(neutral, non_neutral)\n",
    "trees = combine(pro,anti)\n",
    "data = []\n",
    "for i in range(len(trees)):\n",
    "    data.append(Tree(convert(trees[i].root)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trees_test = combine(neutral_test, non_neutral_test)\n",
    "trees_test = combine(pro_test,anti_test)\n",
    "data_test = []\n",
    "for i in range(len(trees_test)):\n",
    "    data_test.append(Tree(convert(trees_test[i].root)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA: model = RecursiveNN(embed,word2idx).cuda()\n",
    "else: model = RecursiveNN(embed,word2idx)\n",
    "max_epochs = 50\n",
    "widgets = [progressbar.Percentage(), ' ', progressbar.Bar(), ' ', progressbar.ETA()]\n",
    "l2_reg = {  'embedding.weight' : 1e-6,'W.weight' : 1e-4,'W.bias' : 1e-4,'projection.weight' : 1e-3,'projection.bias' : 1e-3}\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trn,dev = data[:int((len(data)+1)*.85)],data[int(len(data)*.85+1):]\n",
    "# len(data), len(trn), len(dev)\n",
    "trn = data\n",
    "dev = data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trn), len(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Start logging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, dampening=0.0)\n",
    "# bestAll=bestRoot=0.0\n",
    "global count\n",
    "count=0\n",
    "BATCH_SIZE = 128\n",
    "# optimizer = torch.optim.LBFGS(model.parameters(), lr=0.5, max_iter=10, history_size = 10)\n",
    "bestAll=bestRoot=0.0\n",
    "bestF1_0=bestF1_1=best_trn_F1_0=best_trn_F1_1=0.0\n",
    "best_trn_All = best_trn_Root = 0.0 \n",
    "for epoch in range(max_epochs):\n",
    "#     trn = get_trn()\n",
    "    print(\"\\n\\nEpoch %d\" % epoch)\n",
    "    logging.info('Epoch: '+str(epoch))\n",
    "#     pbar = progressbar.ProgressBar(widgets=widgets, maxval=len(trn)/BATCH_SIZE).start()\n",
    "    params = []\n",
    "    for i in range(0,len(trn),BATCH_SIZE):\n",
    "        count+=1\n",
    "        batch = trn[i:min(i+BATCH_SIZE,len(trn))]\n",
    "        def closure():\n",
    "            global count\n",
    "            optimizer.zero_grad()\n",
    "            _,total_loss = model.getLoss(trn[0].root)\n",
    "            for tree in batch:\n",
    "                _, loss = model.getLoss(tree.root)\n",
    "                total_loss += loss\n",
    "\n",
    "            total_loss = total_loss/len(batch)\n",
    "            #L2 reg\n",
    "            param_dict = dict()\n",
    "            for name, param in model.named_parameters():\n",
    "                param_dict[name] = param.data.clone()\n",
    "                if param.requires_grad:\n",
    "                        total_loss += 0.5*l2_reg[name]*(torch.norm(param)**2)\n",
    "            params.append(param_dict)\n",
    "            print('Loss = ',total_loss.data)\n",
    "            logging.info('Loss = '+str(total_loss.data))\n",
    "            logger.scalar_summary('loss', total_loss.data, count)\n",
    "            total_loss.backward()\n",
    "            clip_grad_norm_(model.parameters(),5,2)\n",
    "            return total_loss\n",
    "#         pbar.update(i/BATCH_SIZE)\n",
    "        optimizer.step(closure)\n",
    "\n",
    "#     pbar.finish()\n",
    "\n",
    "    avg_param = dict()\n",
    "    for name, param1 in model.named_parameters():\n",
    "            avg_param[name] = param1.data.clone()\n",
    "\n",
    "    for i in range(1,len(params)):\n",
    "        for name, param in params[i].items():\n",
    "            avg_param[name] += param.clone()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name == 'embedding.weight':\n",
    "            continue\n",
    "        param.data = avg_param[name]/len(params)\n",
    "\n",
    "    correctRoot, correctAll, f1 = model.evaluate(dev)\n",
    "    # correctRoot = model.eval_sent_lvl(dev,LR_clf)\n",
    "    if bestAll<correctAll: bestAll=correctAll\n",
    "    if bestRoot<correctRoot: bestRoot=correctRoot\n",
    "    if bestF1_0<f1[0]: bestF1_0=f1[0]\n",
    "    if bestF1_1<f1[1]: bestF1_1=f1[1]\n",
    "    print(\"\\nValidation All-nodes accuracy:\"+str(round(correctAll,2))+\"(best:\"+str(round(bestAll,2))+\")\")\n",
    "    print(\"Validation Root accuracy:\" + str(round(correctRoot,2))+\"(best:\"+str(round(bestRoot,2))+\")\")\n",
    "    print(\"F1:\"+str([round(x,2) for x in f1])+\"(best:\"+str(round(bestF1_0,2))+\" , \"+str(round(bestF1_1,2))+\")\")\n",
    "    logging.info(\"Validation All-nodes accuracy:\"+str(round(correctAll,2))+\"(best:\"+str(round(bestAll,2))+\")\")\n",
    "    logging.info(\"Validation Root accuracy:\" + str(round(correctRoot,2))+\"(best:\"+str(round(bestRoot,2))+\")\")\n",
    "    logging.info(\"F1:\"+str([round(x,2) for x in f1])+\"(best:\"+str(round(bestF1_0,2))+\" , \"+str(round(bestF1_1,2))+\")\")\n",
    "    correct_trn_Root, correct_trn_All, f1_trn = model.evaluate(trn)\n",
    "    # correctRoot = model.eval_sent_lvl(dev,LR_clf)\n",
    "    if best_trn_All<correct_trn_All: best_trn_All=correct_trn_All\n",
    "    if best_trn_Root<correct_trn_Root: best_trn_Root=correct_trn_Root\n",
    "    if best_trn_F1_0<f1_trn[0]: best_trn_F1_0=f1_trn[0]\n",
    "    if best_trn_F1_1<f1_trn[1]: best_trn_F1_1=f1_trn[1]\n",
    "    print(\"\\nTraining All-nodes accuracy:\"+str(round(correct_trn_All,2))+\"(best:\"+str(round(best_trn_All,2))+\")\")\n",
    "    print(\"Training Root// accuracy:\" + str(round(correct_trn_Root,2))+\"(best:\"+str(round(best_trn_Root,2))+\")\")\n",
    "    print(\"Training F1:\"+str([round(x,2) for x in f1_trn])+\"(best:\"+str(round(best_trn_F1_0,2))+\" , \"+str(round(best_trn_F1_1,2))+\")\")\n",
    "    logging.info(\"Training All-nodes accuracy:\"+str(round(correct_trn_All,2))+\"(best:\"+str(round(best_trn_All,2))+\")\")\n",
    "    logging.info(\"Training Root accuracy:\" + str(round(correct_trn_Root,2))+\"(best:\"+str(round(best_trn_Root,2))+\")\")\n",
    "    logging.info(\"Training F1:\"+str([round(x,2) for x in f1_trn])+\"(best:\"+str(round(best_trn_F1_0,2))+\" , \"+str(round(best_trn_F1_1,2))+\")\")\n",
    "    info = {'valid_root_acc':correctRoot, 'valid_tree_acc':correctAll, 'train_root_acc':correct_trn_Root, 'train_tree_acc':correct_trn_All}\n",
    "    for tag, value in info.items():\n",
    "        logger.scalar_summary(tag,value,epoch)\n",
    "    random.shuffle(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model,open(\"./models/\"+namecode+'.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
