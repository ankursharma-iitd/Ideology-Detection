{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import progressbar\n",
    "import torch\n",
    "import pickle\n",
    "from mytree import *\n",
    "from utils import *\n",
    "from treeUtil import *\n",
    "import tqdm\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import functools\n",
    "# from lbfgs_utils import compute_stats, get_grad\n",
    "# from LBFGS import LBFGS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time,gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec = False\n",
    "embedDim = 300\n",
    "\n",
    "def arg_parse():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-t', '--trees', dest='trees', required=True)\n",
    "    parser.add_argument('-e', '--epochs', dest='epochs', default=1, required=True)\n",
    "    parser.add_argument('-d', '--dim', dest='dim', default=300, required=True)\n",
    "    \n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveNN(nn.Module):\n",
    "\tdef __init__(self, vocab, embedSize=300, numClasses=2, beta = 0.3, use_weight = True, non_trainable = False):\n",
    "\t\tsuper(RecursiveNN, self).__init__()\n",
    "\t\tif (w2vec):\n",
    "\t\t\t# self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(w2vec_weights), freeze = False)\n",
    "\t\t\t# self.embedding = nn.Embedding.from_pretrained(w2vec_weights, freeze = False)\n",
    "\t\t\tself.embedding = nn.Embedding(len(vocab), embedSize)\n",
    "\t\t\tself.embedding.load_state_dict({'weight': w2vec_weights})\n",
    "\t\t\tself.embedding.weight.requires_grad = True\n",
    "\t\t\tif non_trainable:\n",
    "\t\t\t\tself.embedding.weight.requires_grad = False\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.embedding = nn.Embedding(len(vocab), embedSize)\n",
    "\t\tself.embedding = nn.Embedding(len(vocab), embedSize)\n",
    "\t\tself.W = nn.Linear(2*embedSize, embedSize, bias=True)\n",
    "\t\tself.nonLinear = torch.tanh\n",
    "\t\tself.projection = nn.Linear(embedSize, numClasses, bias=True)\n",
    "\t\tself.nodeProbList = []\n",
    "\t\tself.labelList = []\n",
    "\t\tself.loss = Var(torch.FloatTensor([0]))\n",
    "\t\tself.V = vocab\n",
    "\t\tself.beta = beta\n",
    "\t\tself.use_weight = use_weight\n",
    "\t\tself.total_rep = None #\n",
    "\t\tself.count_rep = 0 #\n",
    "\n",
    "\tdef traverse(self, node):\n",
    "        \"\"\"\n",
    "        get embedding of current node\n",
    "        \"\"\"\n",
    "\t\tif node.isLeaf:\n",
    "\t\t\tif node.getLeafWord() in self.V:  # check if right word is in vocabulary\n",
    "\t\t\t\tword = node.getLeafWord()\n",
    "\t\t\telse:  # otherwise use the unknown token\n",
    "\t\t\t\tword = 'UNK'\n",
    "\t\t\t# print(self.V[word],len(self.V),word,(torch.LongTensor([int(self.V[word])])))\n",
    "\t\t\tcurrentNode = (self.embedding(Var(torch.LongTensor([int(self.V[word])]))))\n",
    "\t\telse: currentNode = self.nonLinear(self.W(torch.cat((self.traverse(node.left),self.traverse(node.right)),1)))\n",
    "\t\tcurrentNode = currentNode/(torch.norm(currentNode))\n",
    "\n",
    "\t\tassert node.label!=None\n",
    "\t\tself.nodeProbList.append(self.projection(currentNode))\n",
    "\t\t# print (node.label)\n",
    "\t\tself.labelList.append(torch.LongTensor([node.label]))\n",
    "\t\tloss_weight = 1-self.beta if node.annotated else self.beta\n",
    "\t\tself.loss += (loss_weight*F.cross_entropy(input=torch.cat([self.projection(currentNode)]),target=Var(torch.cat([torch.LongTensor([node.label])]))))\n",
    "\t\t\n",
    "\t\t#\n",
    "\t\tif not node.isRoot():\n",
    "\t\t\tif self.total_rep is None:\n",
    "\t\t\t\tself.total_rep = currentNode.data.clone()\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.total_rep += currentNode.data.clone()\n",
    "\t\t\tself.count_rep += 1\n",
    "\t\t#\n",
    "\t\t\n",
    "\t\treturn currentNode        \n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tself.nodeProbList = []\n",
    "\t\tself.labelList = []\n",
    "\t\tself.loss = Var(torch.FloatTensor([0]))\n",
    "\t\tself.traverse(x)\n",
    "\t\tself.labelList = Var(torch.cat(self.labelList))\n",
    "\t\treturn torch.cat(self.nodeProbList)\n",
    "\n",
    "\tdef getLoss(self, tree):\n",
    "\t\tnodes = self.forward(tree)\n",
    "\t\tpredictions = nodes.max(dim=1)[1]\n",
    "\t\tloss = self.loss\n",
    "\t\treturn predictions,loss\n",
    "\n",
    "\tdef getRep(self, tree):\n",
    "\t\tself.count_rep = 0\n",
    "\t\tself.total_rep = None\n",
    "\t\tself.nodeProbList = []\n",
    "\t\tself.labelList = []\n",
    "\t\tself.loss = Var(torch.FloatTensor([0]))\n",
    "\t\t\n",
    "\t\troot_rep = self.traverse(tree)\n",
    "\n",
    "\t\treturn (torch.cat((root_rep,self.total_rep/self.count_rep),1)).data.numpy().T.flatten()\n",
    "\n",
    "\n",
    "\tdef evaluate(self, trees):\n",
    "\t\t\tpbar = progressbar.ProgressBar(widgets=widgets, maxval=len(trees)).start()\n",
    "\t\t\tn = nAll = correctRoot = correctAll = 0.0\n",
    "\t\t\tfor j, tree in enumerate(trees):\n",
    "\t\t\t\t\tpredictions,_ = self.getLoss(tree.root)\n",
    "\t\t\t\t\tcorrect = ((predictions.cpu().data).numpy()==(self.labelList.cpu().data).numpy())\n",
    "\t\t\t\t\tcorrectAll += correct.sum()\n",
    "\t\t\t\t\tnAll += np.shape(correct.squeeze())[0] if np.size(correct)!=1 else 1 \n",
    "\t\t\t\t\tcorrectRoot += correct.squeeze()[-1] if np.size(correct)!=1 else correct[-1]\n",
    "\t\t\t\t\tn += 1\n",
    "\t\t\t\t\tpbar.update(j)\n",
    "\t\t\tpbar.finish()\n",
    "\t\t\treturn correctRoot / n, correctAll/nAll\n",
    "\t\n",
    "\tdef eval_sent_lvl(self,trees,clf):\n",
    "\t\tpbar = progressbar.ProgressBar(widgets=widgets, maxval=len(trees)).start()\n",
    "\t\tn = nAll = correctRoot = correctAll = 0.0\n",
    "\t\tX_predict = []\n",
    "\t\tY_gold = []\n",
    "\t\tfor j, tree in enumerate(trees):\n",
    "\t\t\ttree_rep = model.getRep(tree.root)\n",
    "\t\t\tX_predict.append(tree_rep)\n",
    "\t\t\tY_gold.append(tree.root.label)\n",
    "\t\tacc = clf.score(np.array(X_predict),np.array(Y_gold))\n",
    "\t\treturn acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA=False\n",
    "def Var(v):\n",
    "    if CUDA: return Variable(v.cuda())\n",
    "    else: return Variable(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trees...\n"
     ]
    }
   ],
   "source": [
    "trees = []\n",
    "raw_words = []\n",
    "vocab = []\n",
    "print(\"Loading trees...\")\n",
    "[pro, anti] = pickle.load(open('tech_trees.pkl','rb'))\n",
    "random.shuffle(pro)\n",
    "for tree in pro:\n",
    "    tree.root.set_label('pro')\n",
    "for tree in anti:\n",
    "    tree.root.set_label('anti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_trees = []\n",
    "temp_trees.extend(pro[:84])\n",
    "temp_trees.extend(anti)\n",
    "trees.append([Tree(convert(tree.root)) for tree in temp_trees])\n",
    "\n",
    "trees = [s for l in trees for s in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [00:00<00:00, 6573.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168 trees loaded!\n",
      "Building vocab...\n",
      "4142 words found with a vocabulary size of 1341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"{} trees loaded!\".format(len(trees)))\n",
    "print(\"Building vocab...\")\n",
    "for tree in tqdm.tqdm(trees):\n",
    "\twords = tree.get_words()\n",
    "\traw_words.append(words)\n",
    "\tfor word in words:\n",
    "\t\tif word not in vocab: # default_dict can be used\n",
    "\t\t\tvocab.append(word)\n",
    "\n",
    "raw_words = [s for l in raw_words for s in l]  # chain the sublists in raw_words\n",
    "vocab = {w: i for (i, w) in enumerate(vocab)}\n",
    "if 'UNK' not in vocab:\n",
    "    vocab['UNK'] = max(vocab.values()) + 1  # Set UNK token\n",
    "\n",
    "print(\"{} words found with a vocabulary size of {}\".format(len(raw_words), len(vocab)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2vec embeddings Loading\n",
    "if (w2vec):\n",
    "  # Needs to be done just once, stores in pickle file then\n",
    " \t'''\n",
    "  print (\"Loading Word2vec Weights\")\n",
    "  w2vec_start = time.time()\n",
    "  w2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\"./GoogleNews-vectors-negative300.bin\", binary = True)\n",
    "  w2vec_end = time.time()\n",
    "  print (\"Loaded Word2vec Weights! \" + str (round((w2vec_end-w2vec_start)/60,2)))\n",
    "\n",
    "  w2vec_weights_ = np.zeros((len(vocab),embedDim))\n",
    "  for word in vocab.keys():\n",
    "    word_index = vocab[word]\n",
    "    # print (word,word_index)\n",
    "    try: \n",
    "      word_embedding = np.array(w2vec_model.wv[word], dtype = 'float32')\n",
    "    except: #if word not in google word2vec\n",
    "      # word_embedding = np.zeros(embedDim, dtype = 'float32')\n",
    "      word_embedding = np.random.normal(scale=0.6, size=(embedDim, ))\n",
    "    # print (word_embedding)\n",
    "    w2vec_weights_[word_index,:] = torch.FloatTensor(word_embedding)\n",
    "\n",
    "  w2vec_weights_ = torch.from_numpy(w2vec_weights_)\n",
    "  print (\"vocab specific embedding object created\")\n",
    "\n",
    "  print (\"Saving to pickle object\")\n",
    "  fout = open(\"vocab1314_w2vec_weights.pkl\",'wb')  # for testing just 40 trees\n",
    "  pickle.dump(w2vec_weights_,fout)\n",
    "  fout.close()\n",
    "  print (\"Saved to pickle object\")\n",
    "  '''\n",
    "\n",
    "  # fin = open (\"vocab1314_w2vec_weights.pkl\",'rb')\n",
    "  # w2vec_weights = pickle.load(fin)\n",
    "  # fin.close()\n",
    "  # print (\"\\nWord2vec weights loaded from pickle object\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA: model = RecursiveNN(vocab).cuda()\n",
    "else: model = RecursiveNN(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "widgets = [progressbar.Percentage(), ' ', progressbar.Bar(), ' ', progressbar.ETA()]\n",
    "l2_reg = {  'embedding.weight' : 1e-6,'W.weight' : 1e-4,'W.bias' : 1e-4,'projection.weight' : 1e-3,'projection.bias' : 1e-3}\n",
    "random.shuffle(trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn,dev = trees[:int((len(trees)+1)*.90)],trees[int(len(trees)*.90+1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 0\n",
      "Loss =  tensor([12.9389])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:12\n",
      "100% |##########################################################| Time: 0:00:00\n",
      "  2% |#                                                         | ETA:  0:00:06\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.45(best:0.45)\n",
      "Validation Root accuracy:0.44(best:0.44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:05\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.52(best:0.52)\n",
      "Training Root accuracy:0.51(best:0.51)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, dampening=0.0)\n",
    "# bestAll=bestRoot=0.0\n",
    "BATCH_SIZE = len(trn)\n",
    "# optimizer = torch.optim.LBFGS(model.parameters(), lr=0.5, max_iter=10, history_size = 10)\n",
    "bestAll=bestRoot=0.0\n",
    "best_trn_All = best_trn_Root = 0.0 \n",
    "for epoch in range(max_epochs):\n",
    "\tprint(\"\\n\\nEpoch %d\" % epoch)\n",
    "\tpbar = progressbar.ProgressBar(widgets=widgets, maxval=len(trn)/BATCH_SIZE).start()\n",
    "\tparams = []\n",
    "\tfor i in range(0,len(trn),BATCH_SIZE):\n",
    "\t\tbatch = trn[i:min(i+BATCH_SIZE,len(trn))]\n",
    "\t\tdef closure():\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\t_,total_loss = model.getLoss(trn[0].root)\n",
    "\t\t\tfor tree in batch:\n",
    "\t\t\t\t_, loss = model.getLoss(tree.root)\n",
    "\t\t\t\ttotal_loss += loss\n",
    "\t\t\t\t\n",
    "\t\t\ttotal_loss = total_loss/len(batch)\n",
    "\t\t\t#L2 reg\n",
    "\t\t\tparam_dict = dict()\n",
    "\t\t\tfor name, param in model.named_parameters():\n",
    "\t\t\t\tparam_dict[name] = param.data.clone()\n",
    "\t\t\t\tif param.requires_grad:\n",
    "\t\t\t\t\t\ttotal_loss += 0.5*l2_reg[name]*(torch.norm(param)**2)\n",
    "\t\t\tparams.append(param_dict)\n",
    "\t\t\tprint('Loss = ',total_loss.data)\n",
    "\t\t\ttotal_loss.backward()\n",
    "\t\t\tclip_grad_norm_(model.parameters(),5,2)\n",
    "\t\t\treturn total_loss\n",
    "\t\tpbar.update(i/BATCH_SIZE)\n",
    "\t\toptimizer.step(closure)\n",
    "\t\t\n",
    "\tpbar.finish()\n",
    "\n",
    "\tavg_param = dict()\n",
    "\tfor name, param1 in model.named_parameters():\n",
    "\t\t\tavg_param[name] = param1.data.clone()\n",
    "\t\t\t\n",
    "\tfor i in range(1,len(params)):\n",
    "\t\tfor name, param in params[i].items():\n",
    "\t\t\tavg_param[name] += param.clone()\n",
    "\tfor name, param in model.named_parameters():\n",
    "\t\tif name == 'embedding.weight':\n",
    "\t\t\tcontinue\n",
    "\t\tparam.data = avg_param[name]/len(params)\n",
    "\n",
    "\n",
    "\n",
    "\t# X_train, Y_train = [],[]\n",
    "\t# for tree in trn:\n",
    "\t# \tX_train.append(model.getRep(tree.root))\n",
    "\t# \tY_train.append(tree.root.label)\n",
    "\t# X = np.array(X_train)\n",
    "\t# Y = np.array(Y_train)\n",
    "\t# LR_clf = LogisticRegression().fit(X,Y)\n",
    "\n",
    "\n",
    "\tcorrectRoot, correctAll = model.evaluate(dev)\n",
    "\t# correctRoot = model.eval_sent_lvl(dev,LR_clf)\n",
    "\tif bestAll<correctAll: bestAll=correctAll\n",
    "\tif bestRoot<correctRoot: bestRoot=correctRoot\n",
    "\tprint(\"\\nValidation All-nodes accuracy:\"+str(round(correctAll,2))+\"(best:\"+str(round(bestAll,2))+\")\")\n",
    "\tprint(\"Validation Root accuracy:\" + str(round(correctRoot,2))+\"(best:\"+str(round(bestRoot,2))+\")\")\n",
    "\n",
    "\tcorrect_trn_Root, correct_trn_All = model.evaluate(trn)\n",
    "\t# correctRoot = model.eval_sent_lvl(dev,LR_clf)\n",
    "\tif best_trn_All<correct_trn_All: best_trn_All=correct_trn_All\n",
    "\tif best_trn_Root<correct_trn_Root: best_trn_Root=correct_trn_Root\n",
    "\tprint(\"\\nTraining All-nodes accuracy:\"+str(round(correct_trn_All,2))+\"(best:\"+str(round(best_trn_All,2))+\")\")\n",
    "\tprint(\"Training Root accuracy:\" + str(round(correct_trn_Root,2))+\"(best:\"+str(round(best_trn_Root,2))+\")\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\trandom.shuffle(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
