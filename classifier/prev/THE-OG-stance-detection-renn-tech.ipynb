{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import random\n",
    "import progressbar\n",
    "import torch\n",
    "import pickle\n",
    "from mytree import *\n",
    "from utils import *\n",
    "from treeUtil import *\n",
    "import tqdm\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import functools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import string\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import nltk.tree\n",
    "from ast import literal_eval\n",
    "import pptree\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import os\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from spacy.pipeline import merge_entities\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.add_pipe(merge_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "proanti = False # True if doing pro/anti (ideology classification); False if doing nuetral/non-neutral classification (Step1: Stance Detection)\n",
    "w2vec = False # False if in-domain; True if general\n",
    "ner = True # if replacing named entities with their entity tags\n",
    "blackout = False # if blacking out the named entities\n",
    "balanced = True # if dataset has to be balanced or not\n",
    "undersample = True # True if undersampling, False if oversampling\n",
    "non_trainable = True # if first layer of classifier is fixed\n",
    "economic = True # economic or tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'economic_w2v_0ProAnti_0General_1NER_0Blackout_1Balance_1Undersample_1Fixed'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a namecode for the model with set parameters\n",
    "dic = {0:'ProAnti', 1:'General', 2:'NER', 3:'Blackout', 4:'Balance', 5:'Undersample', 6:'Fixed'}\n",
    "bool_list = [proanti, w2vec, ner, blackout, balanced, undersample, non_trainable]\n",
    "if economic:\n",
    "    policy = 'economic'\n",
    "else:\n",
    "    policy = 'tech'\n",
    "corpus_path = '../data/new/'+policy\n",
    "namecode = policy+'_w2v'\n",
    "for index, bb in enumerate(bool_list):\n",
    "    namecode += '_'\n",
    "    if bb:\n",
    "        namecode += '1'\n",
    "    else:\n",
    "        namecode += '0'\n",
    "    namecode += dic[index]\n",
    "namecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For logging\n",
    "import logging\n",
    "\n",
    "#Remove all the previous handlers\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "#Create the file for logging purposes -> CHANGE THE LEVEL TYPE TO logging.DEBUG when debugging or finding faults\n",
    "logging.basicConfig(filename='log_'+namecode+'.log',\n",
    "                            filemode='a',\n",
    "                            format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                            datefmt='%m/%d/%Y %I:%M:%S %p',\n",
    "                            level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger('./logs_'+namecode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveNN(nn.Module):\n",
    "    def __init__(self, word_embeddings, vocab, embedSize=300, numClasses=2, beta = 0.3, use_weight = True, non_trainable = non_trainable):\n",
    "        super(RecursiveNN, self).__init__()\n",
    "#             if (w2vec):\n",
    "#                 self.embedding = nn.Embedding(len(vocab), embedSize)\n",
    "#                 self.embedding.load_state_dict({'weight': w2vec_weights})\n",
    "        self.embedding = nn.Embedding.from_pretrained(word_embeddings)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        if non_trainable:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(len(vocab), embedSize)\n",
    "        self.embedding = nn.Embedding(len(vocab), embedSize)\n",
    "        self.W = nn.Linear(2*embedSize, embedSize, bias=True)\n",
    "        self.nonLinear = torch.tanh\n",
    "        self.projection = nn.Linear(embedSize, numClasses, bias=True)\n",
    "        self.nodeProbList = []\n",
    "        self.labelList = []\n",
    "        self.loss = Var(torch.FloatTensor([0]))\n",
    "        self.V = vocab\n",
    "        self.beta = beta\n",
    "        self.use_weight = use_weight\n",
    "        self.total_rep = None #\n",
    "        self.count_rep = 0 #\n",
    "        self.numClasses = numClasses\n",
    "\n",
    "    def traverse(self, node):\n",
    "        if node.isLeaf:\n",
    "            if node.getLeafWord() in self.V:  # check if right word is in vocabulary\n",
    "                word = node.getLeafWord()\n",
    "            else:  # otherwise use the unknown token\n",
    "                word = 'UNK'\n",
    "            # print(self.V[word],len(self.V),word,(torch.LongTensor([int(self.V[word])])))\n",
    "            currentNode = (self.embedding(Var(torch.LongTensor([int(self.V[word])]))))\n",
    "        else: currentNode = self.nonLinear(self.W(torch.cat((self.traverse(node.left),self.traverse(node.right)),1)))\n",
    "        currentNode = currentNode/(torch.norm(currentNode))\n",
    "\n",
    "        assert node.label!=None\n",
    "        self.nodeProbList.append(self.projection(currentNode))\n",
    "        # print (node.label)\n",
    "        self.labelList.append(torch.LongTensor([node.label]))\n",
    "        loss_weight = 1-self.beta if node.annotated else self.beta\n",
    "        self.loss += (loss_weight*F.cross_entropy(input=torch.cat([self.projection(currentNode)]),target=Var(torch.cat([torch.LongTensor([node.label])]))))\n",
    "\n",
    "        #\n",
    "        if not node.isRoot():\n",
    "            if self.total_rep is None:\n",
    "                self.total_rep = currentNode.data.clone()\n",
    "            else:\n",
    "                self.total_rep += currentNode.data.clone()\n",
    "            self.count_rep += 1\n",
    "        #\n",
    "\n",
    "        return currentNode        \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.nodeProbList = []\n",
    "        self.labelList = []\n",
    "        self.loss = Var(torch.FloatTensor([0]))\n",
    "        self.traverse(x)\n",
    "        self.labelList = Var(torch.cat(self.labelList))\n",
    "        return torch.cat(self.nodeProbList)\n",
    "\n",
    "    def getLoss(self, tree):\n",
    "        nodes = self.forward(tree)\n",
    "        predictions = nodes.max(dim=1)[1]\n",
    "        loss = self.loss\n",
    "        return predictions,loss\n",
    "\n",
    "    def getRep(self, tree):\n",
    "        self.count_rep = 0\n",
    "        self.total_rep = None\n",
    "        self.nodeProbList = []\n",
    "        self.labelList = []\n",
    "        self.loss = Var(torch.FloatTensor([0]))\n",
    "\n",
    "        root_rep = self.traverse(tree)\n",
    "\n",
    "        return (torch.cat((root_rep,self.total_rep/self.count_rep),1)).data.numpy().T.flatten()\n",
    "\n",
    "\n",
    "    def evaluate(self, trees):\n",
    "            pbar = progressbar.ProgressBar(widgets=widgets, maxval=len(trees)).start()\n",
    "            n = nAll = correctRoot = correctAll = 0.0\n",
    "            tp = [1e-2]*self.numClasses\n",
    "            fp = [1e-2]*self.numClasses\n",
    "            fn = [1e-2]*self.numClasses\n",
    "            f1 = [0.]*self.numClasses\n",
    "            for j, tree in enumerate(trees):\n",
    "                predictions,_ = self.getLoss(tree.root)\n",
    "#                     print((predictions.cpu().data).numpy(),(predictions.cpu().data).numpy().shape)\n",
    "#                     print((self.labelList.cpu().data).numpy(), (self.labelList.cpu().data).numpy().shape)\n",
    "                correct = ((predictions.cpu().data).numpy()==(self.labelList.cpu().data).numpy())\n",
    "#                     print(correct)\n",
    "                correctAll += correct.sum()\n",
    "                nAll += np.shape(correct.squeeze())[0] if np.size(correct)!=1 else 1 \n",
    "                correctRoot += correct.squeeze()[-1] if np.size(correct)!=1 else correct[-1]\n",
    "#                     print(correct.squeeze()[-1] if np.size(correct)!=1 else correct[-1])\n",
    "#                     print('actual: {}'.format(tree.root.label))\n",
    "                for i in range(self.numClasses):\n",
    "                    size = np.size((predictions.cpu().data).numpy())\n",
    "                    if size!=1:\n",
    "                        pred = (predictions.cpu().data).numpy().squeeze()[-1]\n",
    "                        actual = (self.labelList.cpu().data).numpy().squeeze()[-1]\n",
    "                    else:\n",
    "                        pred = (predictions.cpu().data).numpy()[-1]\n",
    "                        actual = (self.labelList.cpu().data).numpy()[-1]\n",
    "                    if pred==i and actual==i:\n",
    "                        tp[i]+=1\n",
    "                    elif pred==i and actual!=i:\n",
    "                        fn[i]+=1\n",
    "                    elif pred==i and actual!=i:\n",
    "                        fp[i]+=1\n",
    "                n += 1\n",
    "                pbar.update(j)\n",
    "#             print(tp,fp,fn)\n",
    "            for i in range(self.numClasses):\n",
    "                p =(1.0*tp[i]/(tp[i]+fp[i]))\n",
    "                r =(1.0*tp[i]/(tp[i]+fn[i]))\n",
    "                f1[i] = (2*p*r)/(p+r)\n",
    "            pbar.finish()\n",
    "            return correctRoot / n, correctAll/nAll, f1\n",
    "\n",
    "    def eval_sent_lvl(self,trees,clf):\n",
    "        pbar = progressbar.ProgressBar(widgets=widgets, maxval=len(trees)).start()\n",
    "        n = nAll = correctRoot = correctAll = 0.0\n",
    "        X_predict = []\n",
    "        Y_gold = []\n",
    "        for j, tree in enumerate(trees):\n",
    "            tree_rep = model.getRep(tree.root)\n",
    "            X_predict.append(tree_rep)\n",
    "            Y_gold.append(tree.root.label)\n",
    "        acc = clf.score(np.array(X_predict),np.array(Y_gold))\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the following:\n",
    "- file containing dependency trees of sentences to be classified (trees can be createdd using make-trees)\n",
    "- dataframe containing information about the text and label of the corresponding trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_trees = 'econ_step1_trees_U.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pickle.load(open(\"./trees/df_\"+file_trees, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[While, enlisting, the, measures, taken, by, h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[I, want, to, really, emphasise, that, Aadhar,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Todays, change, should, probably, bring, it, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[This, scheme, has, been, made, foolproof, and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[We, have, introduced, the, GST, bill, in, par...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  target\n",
       "0  [While, enlisting, the, measures, taken, by, h...       0\n",
       "1  [I, want, to, really, emphasise, that, Aadhar,...       1\n",
       "2  [Todays, change, should, probably, bring, it, ...       1\n",
       "3  [This, scheme, has, been, made, foolproof, and...       1\n",
       "4  [We, have, introduced, the, GST, bill, in, par...       1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionary for words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = set(sum(df_train['tokens'],[]))\n",
    "\n",
    "word2idx = {}\n",
    "word2idx['UNK']=0\n",
    "i = 1\n",
    "for token in list(all_tokens):\n",
    "    word2idx[token] = i\n",
    "    i+=1\n",
    "\n",
    "idx2word = {v: k for k, v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: 'hopes',\n",
       " 2: 'men',\n",
       " 3: 'boost',\n",
       " 4: 'bank',\n",
       " 5: 'natural',\n",
       " 6: 'wo',\n",
       " 7: 'stop',\n",
       " 8: 'Parchas',\n",
       " 9: 'sow',\n",
       " 10: 'deceased',\n",
       " 11: 'Bathinda',\n",
       " 12: 'Karan',\n",
       " 13: 'doubt',\n",
       " 14: 'isolation',\n",
       " 15: 'vowed',\n",
       " 16: 'branch',\n",
       " 17: 'do',\n",
       " 18: 'cement',\n",
       " 19: 'continuous',\n",
       " 20: 'generations',\n",
       " 21: 'Ba',\n",
       " 22: 'Party',\n",
       " 23: 'draft',\n",
       " 24: 'happiness',\n",
       " 25: 'KBK',\n",
       " 26: 'willingly',\n",
       " 27: 'deposit',\n",
       " 28: 'remains',\n",
       " 29: 'tonnes',\n",
       " 30: 'renowned',\n",
       " 31: 'Dmks',\n",
       " 32: 'happen',\n",
       " 33: 'area',\n",
       " 34: 'Reacting',\n",
       " 35: 'Sharma',\n",
       " 36: 'disappointed',\n",
       " 37: 'man',\n",
       " 38: 'log',\n",
       " 39: 'construction',\n",
       " 40: 'tenure',\n",
       " 41: 'Bhim',\n",
       " 42: 'nurture',\n",
       " 43: 'present',\n",
       " 44: 'healthcare',\n",
       " 45: 'linked',\n",
       " 46: 'begun',\n",
       " 47: 'Nandan',\n",
       " 48: 'Hope',\n",
       " 49: 'Pushing',\n",
       " 50: 'introduced',\n",
       " 51: 'send',\n",
       " 52: 'tonight',\n",
       " 53: 'comprehensive',\n",
       " 54: 'Badly',\n",
       " 55: 'methods',\n",
       " 56: 'World',\n",
       " 57: 'Magisterial',\n",
       " 58: 'treat',\n",
       " 59: 'finished',\n",
       " 60: 'Twitter',\n",
       " 61: 'Delhi',\n",
       " 62: 'analytics',\n",
       " 63: 'welcomed',\n",
       " 64: 'separate',\n",
       " 65: 'nor',\n",
       " 66: 'invalid',\n",
       " 67: 'invited',\n",
       " 68: 'immense',\n",
       " 69: 'Pralhad',\n",
       " 70: 'juvenile',\n",
       " 71: 'direction',\n",
       " 72: 'extent',\n",
       " 73: 'deals',\n",
       " 74: 'inability',\n",
       " 75: 'breaking',\n",
       " 76: 'letting',\n",
       " 77: 'schools',\n",
       " 78: 'legislations',\n",
       " 79: 'allegations',\n",
       " 80: 'sheer',\n",
       " 81: 'dependent',\n",
       " 82: 'chance',\n",
       " 83: 'Visiting',\n",
       " 84: 'It',\n",
       " 85: 'biometric',\n",
       " 86: 'cashless',\n",
       " 87: 'afresh',\n",
       " 88: 'review',\n",
       " 89: 'solution',\n",
       " 90: 'institution',\n",
       " 91: 'gunny',\n",
       " 92: 'yes',\n",
       " 93: 'expect',\n",
       " 94: 'SCS',\n",
       " 95: 'subject',\n",
       " 96: 'Citing',\n",
       " 97: 'dry',\n",
       " 98: 'Kishan',\n",
       " 99: 'noticed',\n",
       " 100: 'crippled',\n",
       " 101: 'bringing',\n",
       " 102: 'Misuse',\n",
       " 103: 'intention',\n",
       " 104: 'Loans',\n",
       " 105: 'pieces',\n",
       " 106: 'integration',\n",
       " 107: 'Additionally',\n",
       " 108: 'Fiji',\n",
       " 109: 'bridging',\n",
       " 110: 'restored',\n",
       " 111: 'Bmr',\n",
       " 112: 'summit',\n",
       " 113: 'Ltd',\n",
       " 114: 'customer',\n",
       " 115: 'Maidan',\n",
       " 116: 'fully',\n",
       " 117: 'secretaries',\n",
       " 118: 'training',\n",
       " 119: 'gold',\n",
       " 120: 'markets',\n",
       " 121: 'proposition',\n",
       " 122: 'mood',\n",
       " 123: 'Telegraph',\n",
       " 124: 'Evms',\n",
       " 125: 'databases',\n",
       " 126: 'Mandakalli',\n",
       " 127: 'adhere',\n",
       " 128: 'pillars',\n",
       " 129: 'coming',\n",
       " 130: 'definitely',\n",
       " 131: 'consultants',\n",
       " 132: 'possible',\n",
       " 133: 'Ugrahan',\n",
       " 134: 'mammoth',\n",
       " 135: 'decade',\n",
       " 136: 'highways',\n",
       " 137: 'student',\n",
       " 138: 'Government',\n",
       " 139: 'Mafia',\n",
       " 140: 'jewellery',\n",
       " 141: 'column',\n",
       " 142: 'peppering',\n",
       " 143: 'few',\n",
       " 144: 'often',\n",
       " 145: 'apex',\n",
       " 146: 'erring',\n",
       " 147: 'finance',\n",
       " 148: 'caste',\n",
       " 149: 'purposes',\n",
       " 150: 'young',\n",
       " 151: 'Demonetisation',\n",
       " 152: 'widespread',\n",
       " 153: 'room',\n",
       " 154: 'lived',\n",
       " 155: 'PE',\n",
       " 156: 'defence',\n",
       " 157: 'central',\n",
       " 158: 'UpTo',\n",
       " 159: 'Latehar',\n",
       " 160: 'scams',\n",
       " 161: 'convention',\n",
       " 162: 'pressure',\n",
       " 163: 'Pro',\n",
       " 164: 'percentage',\n",
       " 165: 'sideline',\n",
       " 166: 'royal',\n",
       " 167: 'redress',\n",
       " 168: 'countries',\n",
       " 169: 'Emphasising',\n",
       " 170: 'partisan',\n",
       " 171: 'impact',\n",
       " 172: 'pink',\n",
       " 173: 'Gopalganj',\n",
       " 174: 'queues',\n",
       " 175: 'struggle',\n",
       " 176: 'restaurants',\n",
       " 177: 'Levy',\n",
       " 178: 'same',\n",
       " 179: 'marriage',\n",
       " 180: 'achieved',\n",
       " 181: 'miles',\n",
       " 182: 'rectify',\n",
       " 183: 'Upas',\n",
       " 184: 'marks',\n",
       " 185: 'liberated',\n",
       " 186: 'Twitterati',\n",
       " 187: 'Market',\n",
       " 188: 'tried',\n",
       " 189: 'houses',\n",
       " 190: 'karat',\n",
       " 191: 'allotted',\n",
       " 192: 'employed',\n",
       " 193: 'with',\n",
       " 194: 'stolen',\n",
       " 195: 'brought',\n",
       " 196: 'Rajshakti',\n",
       " 197: 'Partys',\n",
       " 198: 'scientist',\n",
       " 199: 'Jakhar',\n",
       " 200: 'round',\n",
       " 201: 'proudly',\n",
       " 202: 'Ghazipur',\n",
       " 203: 'Ap',\n",
       " 204: 'Dhns',\n",
       " 205: 'idea',\n",
       " 206: 'firing',\n",
       " 207: 'rainy',\n",
       " 208: 'inch',\n",
       " 209: 'Urging',\n",
       " 210: 'hardship',\n",
       " 211: 'word',\n",
       " 212: 'investigation',\n",
       " 213: 'IDs',\n",
       " 214: 'exchange',\n",
       " 215: 'entered',\n",
       " 216: 'family',\n",
       " 217: 'spared',\n",
       " 218: 'Vasantrao',\n",
       " 219: 'later',\n",
       " 220: 'earn',\n",
       " 221: 'counters',\n",
       " 222: 'accordance',\n",
       " 223: 'difficult',\n",
       " 224: 'Service',\n",
       " 225: 'mix',\n",
       " 226: 'Mehbooba',\n",
       " 227: 'Kolar',\n",
       " 228: 'suggested',\n",
       " 229: 'husbandry',\n",
       " 230: 'revolutionary',\n",
       " 231: 'although',\n",
       " 232: 'deposited',\n",
       " 233: 'facilitate',\n",
       " 234: 'Hugely',\n",
       " 235: 'contracting',\n",
       " 236: 'Kerala',\n",
       " 237: 'talks',\n",
       " 238: 'experts',\n",
       " 239: 'soil',\n",
       " 240: 'lenders',\n",
       " 241: 'significant',\n",
       " 242: 'explained',\n",
       " 243: 'decisions',\n",
       " 244: 'path',\n",
       " 245: 'far',\n",
       " 246: 'challenges',\n",
       " 247: 'closer',\n",
       " 248: 'Coolies',\n",
       " 249: 'calibration',\n",
       " 250: 'Destroyed',\n",
       " 251: 'Thanks',\n",
       " 252: 'relaxed',\n",
       " 253: 'Bank',\n",
       " 254: 'appeared',\n",
       " 255: 'vis',\n",
       " 256: 'share',\n",
       " 257: 'corporation',\n",
       " 258: 'Challan',\n",
       " 259: 'prohibition',\n",
       " 260: 'intensive',\n",
       " 261: 'straight',\n",
       " 262: 'back',\n",
       " 263: 'focusing',\n",
       " 264: 'Micro',\n",
       " 265: 'Now',\n",
       " 266: 'feet',\n",
       " 267: 'at',\n",
       " 268: 'modern',\n",
       " 269: 'expense',\n",
       " 270: 'obtain',\n",
       " 271: 'facilitating',\n",
       " 272: 'her',\n",
       " 273: 'mines',\n",
       " 274: 'go',\n",
       " 275: 'agricultural',\n",
       " 276: 'earthquake',\n",
       " 277: 'together',\n",
       " 278: 'economist',\n",
       " 279: 'antagonistic',\n",
       " 280: 'moved',\n",
       " 281: 'Sey',\n",
       " 282: 'zone',\n",
       " 283: 'disappointment',\n",
       " 284: 'women',\n",
       " 285: 'final',\n",
       " 286: 'Bjps',\n",
       " 287: 'objections',\n",
       " 288: 'broadly',\n",
       " 289: 'Regularise',\n",
       " 290: 'survey',\n",
       " 291: 'advice',\n",
       " 292: 'recovery',\n",
       " 293: 'scrapped',\n",
       " 294: 'Ever',\n",
       " 295: 'wings',\n",
       " 296: 'contesting',\n",
       " 297: 'mark',\n",
       " 298: 'harm',\n",
       " 299: 'gods',\n",
       " 300: 'Lucknow',\n",
       " 301: 'F',\n",
       " 302: 'operation',\n",
       " 303: 'doesnt',\n",
       " 304: 'ensured',\n",
       " 305: 'hampering',\n",
       " 306: 'opinion',\n",
       " 307: 'starved',\n",
       " 308: 'J',\n",
       " 309: 'businesses',\n",
       " 310: 'Yesterday',\n",
       " 311: 'nominations',\n",
       " 312: 'preparing',\n",
       " 313: 'approach',\n",
       " 314: 'weaker',\n",
       " 315: 'Nilekani',\n",
       " 316: 'U',\n",
       " 317: 'lot',\n",
       " 318: 'reporters',\n",
       " 319: 'hectare',\n",
       " 320: 'well',\n",
       " 321: 'firmly',\n",
       " 322: 'crime',\n",
       " 323: 'undisclosed',\n",
       " 324: 'dealer',\n",
       " 325: 'create',\n",
       " 326: 'served',\n",
       " 327: 'slew',\n",
       " 328: 'park',\n",
       " 329: 'revenues',\n",
       " 330: 'augmented',\n",
       " 331: 'rally',\n",
       " 332: 'exceeding',\n",
       " 333: 'nationality',\n",
       " 334: 'transfer',\n",
       " 335: 'US',\n",
       " 336: 'civil',\n",
       " 337: 'circulating',\n",
       " 338: 'washed',\n",
       " 339: 'among',\n",
       " 340: 'Tuggali',\n",
       " 341: 'Highlights',\n",
       " 342: 'call',\n",
       " 343: 'Tv',\n",
       " 344: 'its',\n",
       " 345: 'special',\n",
       " 346: 'illegal',\n",
       " 347: 'Trinamul',\n",
       " 348: 'indicated',\n",
       " 349: 'centres',\n",
       " 350: 'stage',\n",
       " 351: 'schemes',\n",
       " 352: 'based',\n",
       " 353: 'traditional',\n",
       " 354: 'et',\n",
       " 355: 'success',\n",
       " 356: 'adopted',\n",
       " 357: 'voted',\n",
       " 358: 'Introducing',\n",
       " 359: 'incorporated',\n",
       " 360: 'Palwal',\n",
       " 361: 'lives',\n",
       " 362: 'maintained',\n",
       " 363: 'record',\n",
       " 364: 'cooperation',\n",
       " 365: 'Mega',\n",
       " 366: 'reduced',\n",
       " 367: 'game',\n",
       " 368: 'corrections',\n",
       " 369: 'component',\n",
       " 370: 'solely',\n",
       " 371: 'finally',\n",
       " 372: 'assailing',\n",
       " 373: 'picture',\n",
       " 374: 'postpone',\n",
       " 375: 'edition',\n",
       " 376: 'delayed',\n",
       " 377: 'Photo',\n",
       " 378: 'Like',\n",
       " 379: 'privilege',\n",
       " 380: 'mushroom',\n",
       " 381: 'SBI',\n",
       " 382: 'why',\n",
       " 383: 'Attempts',\n",
       " 384: 'onboard',\n",
       " 385: 'objectives',\n",
       " 386: 'ink',\n",
       " 387: 'versus',\n",
       " 388: 'journey',\n",
       " 389: 'benefiting',\n",
       " 390: 'candidates',\n",
       " 391: 'camera',\n",
       " 392: 'Successive',\n",
       " 393: 'adjusted',\n",
       " 394: 'allegation',\n",
       " 395: 'agency',\n",
       " 396: 'outward',\n",
       " 397: 'perform',\n",
       " 398: 'certificate',\n",
       " 399: 'absolve',\n",
       " 400: 'counter',\n",
       " 401: 'experience',\n",
       " 402: 'matter',\n",
       " 403: 'urge',\n",
       " 404: 'full',\n",
       " 405: 'legend',\n",
       " 406: 'link',\n",
       " 407: 'country',\n",
       " 408: 'submit',\n",
       " 409: 'portfolio',\n",
       " 410: 'Rain',\n",
       " 411: 'Iststate',\n",
       " 412: 'Uninew',\n",
       " 413: 'minister',\n",
       " 414: 'headway',\n",
       " 415: 'detection',\n",
       " 416: 'Initiativehyderabad',\n",
       " 417: 'MMF',\n",
       " 418: 'revolving',\n",
       " 419: 'section',\n",
       " 420: 'Uttar',\n",
       " 421: 'roses',\n",
       " 422: 'TS',\n",
       " 423: 'activity',\n",
       " 424: 'proportion',\n",
       " 425: 'Palaniswami',\n",
       " 426: 'samples',\n",
       " 427: 'BIMA',\n",
       " 428: 'Jayalalithaa',\n",
       " 429: 'island',\n",
       " 430: 'Ichchapruam',\n",
       " 431: 'deprived',\n",
       " 432: 'lines',\n",
       " 433: 'Istwith',\n",
       " 434: 'lots',\n",
       " 435: 'achievement',\n",
       " 436: 'All',\n",
       " 437: 'joining',\n",
       " 438: 'Felicitate',\n",
       " 439: 'cancelled',\n",
       " 440: 'coordination',\n",
       " 441: 'judiciously',\n",
       " 442: 'predominantly',\n",
       " 443: 'fixing',\n",
       " 444: 'item',\n",
       " 445: 'medium',\n",
       " 446: 'blunder',\n",
       " 447: 'targeting',\n",
       " 448: 'CSIR',\n",
       " 449: 'department',\n",
       " 450: 'Tweet',\n",
       " 451: 'Earlier',\n",
       " 452: 'Ahluwalia',\n",
       " 453: 'corruption',\n",
       " 454: 'intervention',\n",
       " 455: 'entity',\n",
       " 456: 'Bhel',\n",
       " 457: 'mufti',\n",
       " 458: 'gardens',\n",
       " 459: 'reason',\n",
       " 460: 'pace',\n",
       " 461: 'overwhelming',\n",
       " 462: 'oblige',\n",
       " 463: 'basically',\n",
       " 464: 'reservation',\n",
       " 465: 'e',\n",
       " 466: 'Cdms',\n",
       " 467: 'Re',\n",
       " 468: 'implementing',\n",
       " 469: 'saved',\n",
       " 470: 'functional',\n",
       " 471: 'battling',\n",
       " 472: 'cue',\n",
       " 473: 'knows',\n",
       " 474: 'cinema',\n",
       " 475: 'website',\n",
       " 476: 'monitoring',\n",
       " 477: 'unstoppable',\n",
       " 478: 'Through',\n",
       " 479: 'plant',\n",
       " 480: 'attention',\n",
       " 481: 'Akkayyapalem',\n",
       " 482: 'narrated',\n",
       " 483: 'Crz',\n",
       " 484: 'top',\n",
       " 485: 'PAC',\n",
       " 486: 'threat',\n",
       " 487: 'licensed',\n",
       " 488: 'logistic',\n",
       " 489: 'constitutional',\n",
       " 490: 'Nsap',\n",
       " 491: 'GST',\n",
       " 492: 'pledged',\n",
       " 493: 'advocating',\n",
       " 494: 'trial',\n",
       " 495: 'find',\n",
       " 496: 'tackle',\n",
       " 497: 'injuries',\n",
       " 498: 'complex',\n",
       " 499: 'Guwahati',\n",
       " 500: 'response',\n",
       " 501: 'provided',\n",
       " 502: 'committees',\n",
       " 503: 'border',\n",
       " 504: 'figures',\n",
       " 505: 'staged',\n",
       " 506: 'provision',\n",
       " 507: 'show',\n",
       " 508: 'debts',\n",
       " 509: 'Everyone',\n",
       " 510: 'irrigation',\n",
       " 511: 'Ventures',\n",
       " 512: 'Cultivators',\n",
       " 513: 'sounding',\n",
       " 514: 'embezzlement',\n",
       " 515: 'Akhileshlucknow',\n",
       " 516: 'goals',\n",
       " 517: 'demand',\n",
       " 518: 'Via',\n",
       " 519: 'migrating',\n",
       " 520: 'cabinets',\n",
       " 521: 'gas',\n",
       " 522: 'Business',\n",
       " 523: 'onion',\n",
       " 524: 'Saath',\n",
       " 525: 'Once',\n",
       " 526: 'Update',\n",
       " 527: 'gap',\n",
       " 528: 'managing',\n",
       " 529: 'workable',\n",
       " 530: 'regarded',\n",
       " 531: 'arising',\n",
       " 532: 'German',\n",
       " 533: 'drink',\n",
       " 534: 'time',\n",
       " 535: 'Akhilesh',\n",
       " 536: 'availability',\n",
       " 537: 'stay',\n",
       " 538: 'July',\n",
       " 539: 'aimed',\n",
       " 540: 'movement',\n",
       " 541: 'remove',\n",
       " 542: 'Contrary',\n",
       " 543: 'political',\n",
       " 544: 'locked',\n",
       " 545: 'associations',\n",
       " 546: 'menace',\n",
       " 547: 'worst',\n",
       " 548: 'democratic',\n",
       " 549: 'contingency',\n",
       " 550: 'Langar',\n",
       " 551: 'repository',\n",
       " 552: 'Ekta',\n",
       " 553: 'download',\n",
       " 554: 'facing',\n",
       " 555: 'Brihanmumbai',\n",
       " 556: 'delighted',\n",
       " 557: 'equal',\n",
       " 558: 'instances',\n",
       " 559: 'doubtless',\n",
       " 560: 'flustered',\n",
       " 561: 'Institutionalising',\n",
       " 562: 'shameless',\n",
       " 563: 'everybody',\n",
       " 564: 'organizations',\n",
       " 565: 'guarantee',\n",
       " 566: 'smoking',\n",
       " 567: 'on',\n",
       " 568: 'Oct',\n",
       " 569: 'new',\n",
       " 570: 'independence',\n",
       " 571: 'compliant',\n",
       " 572: 'Yavatmal',\n",
       " 573: 'collection',\n",
       " 574: 'regime',\n",
       " 575: 'Uday',\n",
       " 576: 'aspirations',\n",
       " 577: 'neglect',\n",
       " 578: 'inflated',\n",
       " 579: 'holistic',\n",
       " 580: 'Karnal',\n",
       " 581: 'disaster',\n",
       " 582: 'PERSON',\n",
       " 583: 'example',\n",
       " 584: 'daughters',\n",
       " 585: 'asked',\n",
       " 586: 'Asked',\n",
       " 587: 'building',\n",
       " 588: 'consideration',\n",
       " 589: 'designers',\n",
       " 590: 'discuss',\n",
       " 591: 'whether',\n",
       " 592: 'responsibilities',\n",
       " 593: 'consultant',\n",
       " 594: 'assembly',\n",
       " 595: 'plan',\n",
       " 596: 'Nafed',\n",
       " 597: 'promoting',\n",
       " 598: 'Assam',\n",
       " 599: 'criticism',\n",
       " 600: 'law',\n",
       " 601: 'PSU',\n",
       " 602: 'memorial',\n",
       " 603: 'six',\n",
       " 604: 'discusses',\n",
       " 605: 'into',\n",
       " 606: 'assistance',\n",
       " 607: 'Rampant',\n",
       " 608: 'simply',\n",
       " 609: 'regular',\n",
       " 610: 'shops',\n",
       " 611: 'designing',\n",
       " 612: 'wheat',\n",
       " 613: 'away',\n",
       " 614: 'consumption',\n",
       " 615: 'convince',\n",
       " 616: 'AAP',\n",
       " 617: 'certify',\n",
       " 618: 'Amid',\n",
       " 619: 'hopeful',\n",
       " 620: 'target',\n",
       " 621: 'Telugu',\n",
       " 622: 'derail',\n",
       " 623: 'dollars',\n",
       " 624: 'big',\n",
       " 625: 'culling',\n",
       " 626: 'shown',\n",
       " 627: 'Hindu',\n",
       " 628: 'platform',\n",
       " 629: 'Captain',\n",
       " 630: 'distort',\n",
       " 631: 'cheating',\n",
       " 632: 'veiled',\n",
       " 633: 'Indirect',\n",
       " 634: 'Thakur',\n",
       " 635: 'moderate',\n",
       " 636: 'chopper',\n",
       " 637: 'PwC',\n",
       " 638: 'impression',\n",
       " 639: 'Edappadik',\n",
       " 640: 'organisation',\n",
       " 641: 'using',\n",
       " 642: 'fair',\n",
       " 643: 'misleading',\n",
       " 644: 'tribes',\n",
       " 645: 'veteran',\n",
       " 646: 'Maharashtra',\n",
       " 647: 'guesthouse',\n",
       " 648: 'agitation',\n",
       " 649: 'educating',\n",
       " 650: 'finalise',\n",
       " 651: 'violent',\n",
       " 652: 'motivated',\n",
       " 653: 'Africa',\n",
       " 654: 'expedite',\n",
       " 655: 'chairing',\n",
       " 656: 'apprise',\n",
       " 657: 'discontent',\n",
       " 658: 'discussed',\n",
       " 659: 'observe',\n",
       " 660: 'cutoff',\n",
       " 661: 'especially',\n",
       " 662: 'tweets',\n",
       " 663: 'fingerprint',\n",
       " 664: 'newer',\n",
       " 665: 'eases',\n",
       " 666: 'valedictory',\n",
       " 667: 'challenging',\n",
       " 668: 'fingerprints',\n",
       " 669: 'everyone',\n",
       " 670: 'released',\n",
       " 671: 'KP',\n",
       " 672: 'introduce',\n",
       " 673: 'elaborating',\n",
       " 674: 'ignore',\n",
       " 675: 'He',\n",
       " 676: 'soon',\n",
       " 677: 'calculated',\n",
       " 678: 'amount',\n",
       " 679: 'opening',\n",
       " 680: 'realise',\n",
       " 681: 'lists',\n",
       " 682: 'rope',\n",
       " 683: 'means',\n",
       " 684: 'Lane',\n",
       " 685: 'behalf',\n",
       " 686: 'Ruled',\n",
       " 687: 'cola',\n",
       " 688: 'manufacturers',\n",
       " 689: 'clearly',\n",
       " 690: 'completes',\n",
       " 691: 'territory',\n",
       " 692: 'holder',\n",
       " 693: 'considering',\n",
       " 694: 'Pending',\n",
       " 695: 'laminated',\n",
       " 696: 'week',\n",
       " 697: 'leakages',\n",
       " 698: 'biometrics',\n",
       " 699: 'Rainfed',\n",
       " 700: 'Responding',\n",
       " 701: 'source',\n",
       " 702: 'millions',\n",
       " 703: 'Janshakti',\n",
       " 704: 'lose',\n",
       " 705: 'cell',\n",
       " 706: 'name',\n",
       " 707: 'Make',\n",
       " 708: 'along',\n",
       " 709: 'nation',\n",
       " 710: 'linking',\n",
       " 711: 'resulting',\n",
       " 712: 'Hitting',\n",
       " 713: 'Minister',\n",
       " 714: 'cancelling',\n",
       " 715: 'fourth',\n",
       " 716: 'Calibrating',\n",
       " 717: 'transferred',\n",
       " 718: 'scale',\n",
       " 719: 'Identities',\n",
       " 720: 'enlist',\n",
       " 721: 'us',\n",
       " 722: 'soul',\n",
       " 723: 'Delhidays',\n",
       " 724: 'seats',\n",
       " 725: 'Afghan',\n",
       " 726: 'hugely',\n",
       " 727: 'visits',\n",
       " 728: 'felt',\n",
       " 729: 'protocols',\n",
       " 730: 'foremost',\n",
       " 731: 'book',\n",
       " 732: 'elaborate',\n",
       " 733: 'overseas',\n",
       " 734: 'Yogendra',\n",
       " 735: 'whopping',\n",
       " 736: 'hailstorm',\n",
       " 737: 'his',\n",
       " 738: 'consumers',\n",
       " 739: 'Remembrancer',\n",
       " 740: 'curb',\n",
       " 741: 'and',\n",
       " 742: 'deeper',\n",
       " 743: 'Bangladesh',\n",
       " 744: 'tabling',\n",
       " 745: 'cotton',\n",
       " 746: 'maintenance',\n",
       " 747: 'welcome',\n",
       " 748: 'paddy',\n",
       " 749: 'Mitras',\n",
       " 750: 'Ganesh',\n",
       " 751: 'aware',\n",
       " 752: 'went',\n",
       " 753: 'dream',\n",
       " 754: 'who',\n",
       " 755: 'boys',\n",
       " 756: 'Bharuch',\n",
       " 757: 'Assessees',\n",
       " 758: 'Horata',\n",
       " 759: 'infrastructural',\n",
       " 760: 'czar',\n",
       " 761: 'drive',\n",
       " 762: 'them',\n",
       " 763: 'DH',\n",
       " 764: 'Amarindersingh',\n",
       " 765: 'incidents',\n",
       " 766: 'auction',\n",
       " 767: 'wriggle',\n",
       " 768: 'crunch',\n",
       " 769: 'confidence',\n",
       " 770: 'revamped',\n",
       " 771: 'Ccpa',\n",
       " 772: 'Of',\n",
       " 773: 'supporters',\n",
       " 774: 'clinched',\n",
       " 775: 'waters',\n",
       " 776: 'Cash',\n",
       " 777: 'arise',\n",
       " 778: 'maintaining',\n",
       " 779: 'The',\n",
       " 780: 'reshuffled',\n",
       " 781: 'reconciliations',\n",
       " 782: 'demanding',\n",
       " 783: 'reform',\n",
       " 784: 'statutory',\n",
       " 785: 'vs',\n",
       " 786: 'season',\n",
       " 787: 'prescribed',\n",
       " 788: 'sufficiency',\n",
       " 789: 'announcements',\n",
       " 790: 'Referring',\n",
       " 791: 'cause',\n",
       " 792: 'point',\n",
       " 793: 'CAG',\n",
       " 794: 'hesitate',\n",
       " 795: 'Benefit',\n",
       " 796: 'bothered',\n",
       " 797: 'airport',\n",
       " 798: 'MSP',\n",
       " 799: 'push',\n",
       " 800: 'chalking',\n",
       " 801: 'across',\n",
       " 802: 'channel',\n",
       " 803: 'security',\n",
       " 804: 'northern',\n",
       " 805: 'multinational',\n",
       " 806: 'informal',\n",
       " 807: 'purchasing',\n",
       " 808: 'dark',\n",
       " 809: 'specified',\n",
       " 810: 'seems',\n",
       " 811: 'Gandhinagar',\n",
       " 812: 'sit',\n",
       " 813: 'opt',\n",
       " 814: 'owner',\n",
       " 815: 'loaded',\n",
       " 816: 'discount',\n",
       " 817: 'Utsav',\n",
       " 818: 'difficulties',\n",
       " 819: 'upgrade',\n",
       " 820: 'strain',\n",
       " 821: 'arrest',\n",
       " 822: 'launch',\n",
       " 823: 'Seized',\n",
       " 824: 'g',\n",
       " 825: 'woes',\n",
       " 826: 'hurdles',\n",
       " 827: 'mixture',\n",
       " 828: 'resolved',\n",
       " 829: 'thereof',\n",
       " 830: 'purpose',\n",
       " 831: 'accusing',\n",
       " 832: 'withdrawls',\n",
       " 833: 'justified',\n",
       " 834: 'write',\n",
       " 835: 'refinance',\n",
       " 836: 'initiative',\n",
       " 837: 'repeated',\n",
       " 838: 'ruling',\n",
       " 839: 'bond',\n",
       " 840: 'Express',\n",
       " 841: 'AC',\n",
       " 842: 'Pointing',\n",
       " 843: 'Friendly',\n",
       " 844: 'Tuesday',\n",
       " 845: 'kin',\n",
       " 846: 'Athrady',\n",
       " 847: 'civic',\n",
       " 848: 'Home',\n",
       " 849: 'Ragi',\n",
       " 850: 'sudden',\n",
       " 851: 'changes',\n",
       " 852: 'Hisotry',\n",
       " 853: 'Yadav',\n",
       " 854: 'announcement',\n",
       " 855: 'sack',\n",
       " 856: 'historic',\n",
       " 857: 'transmission',\n",
       " 858: 'recorded',\n",
       " 859: 'downs',\n",
       " 860: 'outreach',\n",
       " 861: 'increased',\n",
       " 862: 'delegation',\n",
       " 863: 'upon',\n",
       " 864: 'repaying',\n",
       " 865: 'suggest',\n",
       " 866: 'policies',\n",
       " 867: 'untreated',\n",
       " 868: 'powers',\n",
       " 869: 'territories',\n",
       " 870: 'Sunday',\n",
       " 871: 'Talking',\n",
       " 872: 'Tinsukia',\n",
       " 873: 'queries',\n",
       " 874: 'inspecting',\n",
       " 875: 'panel',\n",
       " 876: 'applied',\n",
       " 877: 'hide',\n",
       " 878: 'cars',\n",
       " 879: 'wrongful',\n",
       " 880: 'exchanging',\n",
       " 881: 'staging',\n",
       " 882: 'till',\n",
       " 883: 'decline',\n",
       " 884: 'literacy',\n",
       " 885: 'jump',\n",
       " 886: 'Counting',\n",
       " 887: 'job',\n",
       " 888: 'Soon',\n",
       " 889: 'company',\n",
       " 890: 'beds',\n",
       " 891: 'rd',\n",
       " 892: 'exempt',\n",
       " 893: 'identification',\n",
       " 894: 'behind',\n",
       " 895: 'submerged',\n",
       " 896: 'harvest',\n",
       " 897: 'Aadhar',\n",
       " 898: 'via',\n",
       " 899: 'home',\n",
       " 900: 'Andhra',\n",
       " 901: 'mode',\n",
       " 902: 'save',\n",
       " 903: 'KYC',\n",
       " 904: 'phase',\n",
       " 905: 'alleges',\n",
       " 906: 'holders',\n",
       " 907: 'granted',\n",
       " 908: 'Anandapuram',\n",
       " 909: 'credited',\n",
       " 910: 'continued',\n",
       " 911: 'apprehended',\n",
       " 912: 'ration',\n",
       " 913: 'attracting',\n",
       " 914: 'tomorrow',\n",
       " 915: 'overall',\n",
       " 916: 'Jan',\n",
       " 917: 'redefined',\n",
       " 918: 'information',\n",
       " 919: 'groom',\n",
       " 920: 'bathrooms',\n",
       " 921: 'gathering',\n",
       " 922: 'Sahyadri',\n",
       " 923: 'these',\n",
       " 924: 'cyberspace',\n",
       " 925: 'enhance',\n",
       " 926: 'targeted',\n",
       " 927: 'destroying',\n",
       " 928: 'picking',\n",
       " 929: 'almost',\n",
       " 930: 'emphasise',\n",
       " 931: 'seminar',\n",
       " 932: 'entertainment',\n",
       " 933: 'exchangers',\n",
       " 934: 'division',\n",
       " 935: 'introducing',\n",
       " 936: 'courts',\n",
       " 937: 'firm',\n",
       " 938: 'priority',\n",
       " 939: 'authorised',\n",
       " 940: 'window',\n",
       " 941: 'shortage',\n",
       " 942: 'industrialists',\n",
       " 943: 'charging',\n",
       " 944: 'Disruptionist',\n",
       " 945: 'phases',\n",
       " 946: 'Devasthanams',\n",
       " 947: 'police',\n",
       " 948: 'Demonetising',\n",
       " 949: 'intelligence',\n",
       " 950: 'bit',\n",
       " 951: 'Pawar',\n",
       " 952: 'Besides',\n",
       " 953: 'violence',\n",
       " 954: 'spurt',\n",
       " 955: 'strengthening',\n",
       " 956: 'processing',\n",
       " 957: 'Anroid',\n",
       " 958: 'unified',\n",
       " 959: 'Sonowal',\n",
       " 960: 'around',\n",
       " 961: 'Uidai',\n",
       " 962: 'farm',\n",
       " 963: 'fan',\n",
       " 964: 'workers',\n",
       " 965: 'Called',\n",
       " 966: 'Director',\n",
       " 967: 'overhauling',\n",
       " 968: 'Istbenefiting',\n",
       " 969: 'vigilantism',\n",
       " 970: 'provoke',\n",
       " 971: 'aeronautics',\n",
       " 972: 'alternative',\n",
       " 973: 'figured',\n",
       " 974: 'divert',\n",
       " 975: 'select',\n",
       " 976: 'dreams',\n",
       " 977: 'lead',\n",
       " 978: 'bind',\n",
       " 979: 'elders',\n",
       " 980: 'duty',\n",
       " 981: 'feed',\n",
       " 982: 'lawlessness',\n",
       " 983: 'strong',\n",
       " 984: 'technical',\n",
       " 985: 'sops',\n",
       " 986: 'STS',\n",
       " 987: 'communicating',\n",
       " 988: 'Politicise',\n",
       " 989: 'curbing',\n",
       " 990: 'ways',\n",
       " 991: 'process',\n",
       " 992: 'Whether',\n",
       " 993: 'opened',\n",
       " 994: 'considers',\n",
       " 995: 'stress',\n",
       " 996: 'chickens',\n",
       " 997: 'want',\n",
       " 998: 'representation',\n",
       " 999: 'skip',\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fine tuned . .\n",
      "Loading economic-word2vec-ner\n"
     ]
    }
   ],
   "source": [
    "if not w2vec:\n",
    "    print(\"Using fine tuned . .\")\n",
    "    if ner:\n",
    "        print(\"Loading \"+policy+\"-word2vec-ner\")\n",
    "        unbiased_in_domain_model = KeyedVectors.load('/Users/navreetkaur/MTP/finetune-word2vec/w2v-models/'+policy+'-word2vec-ner')\n",
    "    elif blackout:\n",
    "        print(\"Loading \"+policy+\"-word2vec-blackout\")\n",
    "        unbiased_in_domain_model = KeyedVectors.load('/Users/navreetkaur/MTP/finetune-word2vec/w2v-models/'+policy+'-word2vec-blackout')\n",
    "    else:\n",
    "        print(\"Loading \"+policy+\"-word2vec\")\n",
    "        unbiased_in_domain_model = KeyedVectors.load('Users/navreetkaur/MTP/finetune-word2vec/w2v-models/'+policy+'-word2vec')\n",
    "    \n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('/Users/navreetkaur/MTP/finetune-word2vec/w2v-models/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set value of the first(embedding) layer of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[While, enlisting, the, measures, taken, by, h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[I, want, to, really, emphasise, that, Aadhar,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Todays, change, should, probably, bring, it, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[This, scheme, has, been, made, foolproof, and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[We, have, introduced, the, GST, bill, in, par...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  target\n",
       "0  [While, enlisting, the, measures, taken, by, h...       0\n",
       "1  [I, want, to, really, emphasise, that, Aadhar,...       1\n",
       "2  [Todays, change, should, probably, bring, it, ...       1\n",
       "3  [This, scheme, has, been, made, foolproof, and...       1\n",
       "4  [We, have, introduced, the, GST, bill, in, par...       1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = np.zeros((len(idx2word), word2vec_model.vector_size))\n",
    "\n",
    "for idx, word in idx2word.items():\n",
    "    if not w2vec:\n",
    "        try: \n",
    "            embed[idx] = unbiased_in_domain_model[word]\n",
    "        except KeyError:\n",
    "            try:\n",
    "                embed[idx] = word2vec_model[word]\n",
    "            except:\n",
    "                embed[idx] = np.random.normal(size=(word2vec_model.vector_size, ))\n",
    "    else:\n",
    "        try:\n",
    "            embed[idx] = word2vec_model[word]\n",
    "        except:\n",
    "            embed[idx] = np.random.normal(size=(word2vec_model.vector_size, ))\n",
    "        \n",
    "embed = torch.from_numpy(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    694\n",
       "1    694\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby('target').size() # size of dataset and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5296, 300]), 5296)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.shape, len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tree(text):\n",
    "#     print(text)\n",
    "    output = nlp.annotate(text, properties={\n",
    "        'annotators': 'tokenize,ssplit,pos,depparse,parse',\n",
    "        'outputFormat': 'json'\n",
    "    })\n",
    "    output = literal_eval(output)\n",
    "    try:\n",
    "        tree = str(output['sentences'][0]['parse'])\n",
    "    except:\n",
    "        print(output,text)\n",
    "        return\n",
    "    # print (tree)\n",
    "    parse_string = ' '.join(str(tree).split())\n",
    "    # print(parse_string)\n",
    "    # print (\"\\n\\n\")\n",
    "    tree = nltk.tree.Tree.fromstring(parse_string)\n",
    "    tree.chomsky_normal_form()\n",
    "    tree.collapse_unary(collapseRoot=True,collapsePOS=True)\n",
    "    nt = convertNLTK_tree(tree)\n",
    "    return nt\n",
    "\n",
    "def printLabelTree(tree):\n",
    "    def inorder(node,nnode):\n",
    "        if node.isLeaf:\n",
    "            newnode = pptree.Node('H',nnode)\n",
    "            wnode = pptree.Node(node.word,newnode)\n",
    "        elif nnode is not None:\n",
    "            newnode = pptree.Node('H',nnode)\n",
    "            inorder(node.left,newnode)\n",
    "            inorder(node.right,newnode)\n",
    "        elif node.isRoot():\n",
    "            newnode = pptree.Node('H')\n",
    "            inorder(node.left,newnode)\n",
    "            inorder(node.right,newnode)\n",
    "            return newnode\n",
    "        return None\n",
    "    pptree.print_tree(inorder(tree.root,None))\n",
    "\n",
    "def create_trees_using_df(df):\n",
    "    tree = []\n",
    "    for tokens in list(df['tokens']):\n",
    "        if len(tokens)==0:\n",
    "            continue\n",
    "        line = ' '.join(tokens)\n",
    "        line += '\\n'\n",
    "        tree.append(make_tree(line))\n",
    "    return tree\n",
    "\n",
    "def printlabel(root,l):\n",
    "    if root:\n",
    "        l.append(root.label)\n",
    "#         print(root.label)\n",
    "        if root.left:\n",
    "            l+=printlabel(root.left,[])\n",
    "#             print(printlabel(root.left))\n",
    "        if root.right:\n",
    "            l+=printlabel(root.right,[])\n",
    "#             print(printlabel(root.right))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = StanfordCoreNLP('http://localhost', port=9000,timeout=90000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neutral = create_trees_using_df(df_train[df_train.target == 0])\n",
    "# anti = create_trees_using_df(df_train[df_train.target == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_neutral = create_trees_using_df(df_train[df_train.target == 1])\n",
    "# pro = create_trees_using_df(df_train[df_train.target == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neutral_test = create_trees_using_df(df_test[df_test.target == 0])\n",
    "# non_neutral_test = create_trees_using_df(df_test[df_test.target == 1])\n",
    "# anti_test = create_trees_using_df(df_test[df_test.target == 0])\n",
    "# pro_test = create_trees_using_df(df_test[df_test.target == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fout = open(file_trees,'wb')\n",
    "# pickle.dump([pro, anti],fout)\n",
    "# fout.close()\n",
    "# fout = open(file_trees+\"_test\",'wb')\n",
    "# pickle.dump([pro_test, anti_test],fout)\n",
    "# fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "[pro,anti] = pickle.load(open(os.path.join('./trees',file_trees),'rb'))\n",
    "[pro_test,anti_test] = pickle.load(open(os.path.join('./trees',file_trees+\"_test\"),'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fout = open(file_trees,'wb')\n",
    "# pickle.dump([neutral, non_neutral],fout)\n",
    "# fout.close()\n",
    "# fout = open(file_trees+'_test','wb')\n",
    "# pickle.dump([neutral_test, non_neutral_test],fout)\n",
    "# fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA=False\n",
    "def Var(v):\n",
    "    if CUDA: return Variable(v.cuda())\n",
    "    else: return Variable(v)\n",
    "    \n",
    "trees = []\n",
    "raw_words = []\n",
    "vocab = []\n",
    "\n",
    "# print(\"Loading trees...\")\n",
    "# [neutral, non_neutral] = pickle.load(open(file_trees,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for neutral_tree in neutral:\n",
    "#     neutral_tree.root.set_label('neutral')\n",
    "# for non_neutral_tree in non_neutral:\n",
    "#     non_neutral_tree.root.set_label('non_neutral')\n",
    "    \n",
    "for pro_tree in pro:\n",
    "    pro_tree.root.set_label('pro')\n",
    "for anti_tree in anti:\n",
    "    anti_tree.root.set_label('anti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for neutral_tree in neutral_test:\n",
    "#     neutral_tree.root.set_label('neutral')\n",
    "# for non_neutral_tree in non_neutral_test:\n",
    "#     non_neutral_tree.root.set_label('non_neutral')\n",
    "    \n",
    "for pro_tree in pro_test:\n",
    "    pro_tree.root.set_label('pro')\n",
    "for anti_tree in anti_test:\n",
    "    anti_tree.root.set_label('anti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(neutral,non_neutral):\n",
    "    trees = []\n",
    "    trees.extend(neutral)\n",
    "    trees.extend(non_neutral)\n",
    "    random.shuffle(trees)\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mytree import *\n",
    "from treeUtil import *\n",
    "# from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "val = {'pro':0,'anti':1,'default':-1}\n",
    "\n",
    "# pro:1, anti:0, neutral:2\n",
    "val_all = {'pro':1,'anti':0,'default':-1,'neutral':2}\n",
    "# neutral:0, non-neutral:1\n",
    "val_neutral = {'neutral':0, 'non_neutral':1, 'default':-1}\n",
    "\n",
    "\n",
    "def convert(T):\n",
    "    label = val[T.label] if (hasattr(T,'label')) else None\n",
    "    print(label)\n",
    "    newTree = convert_primary_new(T,label)\n",
    "    annotate_all(newTree)\n",
    "\n",
    "    return newTree\n",
    "\n",
    "def convert_neutral(T):\n",
    "    label = val_neutral[T.label] if (hasattr(T,'label')) else None\n",
    "    print(label)\n",
    "    newTree = convert_primary_new(T,label)\n",
    "    annotate_all(newTree)\n",
    "\n",
    "    return newTree\n",
    "\n",
    "def convert_all(T):\n",
    "    label = val_all[T.label] if (hasattr(T,'label')) else None\n",
    "    print(label)\n",
    "    newTree = convert_primary_new(T,label)\n",
    "    annotate_all(newTree)\n",
    "\n",
    "    return newTree\n",
    "\n",
    "\n",
    "def convert_primary(T):\n",
    "    if (hasattr(T,'label')):\n",
    "        print(T.label) \n",
    "    label = val[T.label] if (hasattr(T,'label')) else None\n",
    "    # label = val[T.label] if (hasattr(T,'label') ) else None # changed for ignoring neutral\n",
    "\n",
    "    if isinstance(T,leafObj):\n",
    "        newTree = Node(label,T.word,T.pos)\n",
    "        newTree.isLeaf = True\n",
    "        return newTree\n",
    "    else:\n",
    "        newTree = Node(label)\n",
    "    \n",
    "    leftChild = convert_primary(T.c1)\n",
    "    rightChild = convert_primary(T.c2)\n",
    "    leftChild.parent = newTree\n",
    "    rightChild.parent = newTree\n",
    "\n",
    "    newTree.left = leftChild\n",
    "    newTree.right = rightChild\n",
    "\n",
    "    return newTree\n",
    "\n",
    "def convert_primary_new(T,label):\n",
    "    # from IPython import embed; embed()\n",
    "    if T is None:\n",
    "        return None\n",
    "    # label = val[T.label] if (hasattr(T,'label') ) else None # changed for ignoring neutral\n",
    "    T.set_label(label)\n",
    "    # if (T.isLeaf) : print (T.word)\n",
    "\n",
    "    T.left = convert_primary_new(T.left,label)\n",
    "    T.right = convert_primary_new(T.right,label)\n",
    "\n",
    "    return T\n",
    "\n",
    "    # if T.isLeaf:\n",
    "    #     newTree = Node(label,T.word,T.pos)\n",
    "    #     newTree.isLeaf = True\n",
    "    #     return newTree\n",
    "    # else:\n",
    "    #     newTree = Node(label)\n",
    "    \n",
    "    # leftChild = convert_primary_new(T.left)\n",
    "    # rightChild = convert_primary_new(T.right)\n",
    "    # leftChild.parent = newTree\n",
    "    # rightChild.parent = newTree\n",
    "\n",
    "    # newTree.left = leftChild\n",
    "    # newTree.right = rightChild\n",
    "\n",
    "    # return newTree\n",
    "\n",
    "def convertNLTK_tree_primary(tree):\n",
    "    if tree.height()==2:\n",
    "        newTree = Node('default',tree[0],None)\n",
    "        newTree.isLeaf = True\n",
    "        return newTree\n",
    "    newTree = Node('default')\n",
    "    leftChild = convertNLTK_tree_primary(tree[0])\n",
    "    rightChild = convertNLTK_tree_primary(tree[1])\n",
    "    \n",
    "    leftChild.parent = newTree\n",
    "    rightChild.parent = newTree\n",
    "\n",
    "    newTree.left = leftChild\n",
    "    newTree.right = rightChild\n",
    "\n",
    "    return newTree\n",
    "\n",
    "def convertNLTK_tree(tree):\n",
    "    return Tree(convertNLTK_tree_primary(tree))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def annotate_all(T):\n",
    "    if T == None: return\n",
    "    if T.label != None : \n",
    "        T.annotated = True\n",
    "    else:\n",
    "        T.annotated = False\n",
    "        T.set_label(T.parent.label)\n",
    "    annotate_all(T.left)\n",
    "    annotate_all(T.right)\n",
    "\n",
    "def buildBalTree(sent):\n",
    "    words = sent.split(' ')\n",
    "\n",
    "    nodes = words\n",
    "\n",
    "    while len(nodes)>1:\n",
    "        temp = []\n",
    "        for i in range(0,len(nodes),2):\n",
    "            lChild = Node(None,nodes[i],None) if isinstance(nodes[i],str) else nodes[i]\n",
    "            if i+1<len(nodes):\n",
    "                rChild = Node(None,nodes[i+1],None) if isinstance(nodes[i+1],str) else nodes[i+1]\n",
    "            else:\n",
    "                rChild = None\n",
    "            if isinstance(nodes[i],str):\n",
    "                lChild.isLeaf = True\n",
    "                if rChild is not None:\n",
    "                    rChild.isLeaf = True\n",
    "            newNode = Node(None)\n",
    "            lChild.parent = newNode\n",
    "            newNode.left = lChild\n",
    "            newNode.right = rChild\n",
    "            if rChild is not None:\n",
    "                rChild.parent = newNode\n",
    "            temp.append(newNode)\n",
    "        nodes=temp\n",
    "    return Tree(nodes[0])\n",
    "\n",
    "def readFile2Trees(filename):\n",
    "    trees = []\n",
    "    with open(filename,'r') as file:\n",
    "        for line in file:\n",
    "            if line=='\\n':\n",
    "                continue\n",
    "            else:\n",
    "                [labelname,sent] = line.split(': ',1)\n",
    "                tree = buildBalTree(sent)\n",
    "                tree.root.set_label(val[labelname])\n",
    "                if val[labelname]!=2:\n",
    "                    trees.append(tree)\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# trees = combine(neutral, non_neutral)\n",
    "trees = combine(pro,anti)\n",
    "data = []\n",
    "for i in range(len(trees)):\n",
    "    data.append(Tree(convert(trees[i].root)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# trees_test = combine(neutral_test, non_neutral_test)\n",
    "trees_test = combine(pro_test,anti_test)\n",
    "data_test = []\n",
    "for i in range(len(trees_test)):\n",
    "    data_test.append(Tree(convert(trees_test[i].root)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA: model = RecursiveNN(embed,word2idx).cuda()\n",
    "else: model = RecursiveNN(embed,word2idx)\n",
    "max_epochs = 50\n",
    "widgets = [progressbar.Percentage(), ' ', progressbar.Bar(), ' ', progressbar.ETA()]\n",
    "l2_reg = {  'embedding.weight' : 1e-6,'W.weight' : 1e-4,'W.bias' : 1e-4,'projection.weight' : 1e-3,'projection.bias' : 1e-3}\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trn,dev = data[:int((len(data)+1)*.85)],data[int(len(data)*.85+1):]\n",
    "# len(data), len(trn), len(dev)\n",
    "trn = data\n",
    "dev = data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1388, 562)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn), len(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Start logging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 0\n",
      "Loss =  tensor([28.6716])\n",
      "Loss =  tensor([29.2780])\n",
      "Loss =  tensor([30.9149])\n",
      "Loss =  tensor([31.2382])\n",
      "Loss =  tensor([30.9575])\n",
      "Loss =  tensor([31.2406])\n",
      "Loss =  tensor([31.1724])\n",
      "Loss =  tensor([30.5903])\n",
      "Loss =  tensor([29.1935])\n",
      "Loss =  tensor([31.8247])\n",
      "Loss =  tensor([32.0630])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:15\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.77(best:0.77)\n",
      "Validation Root accuracy:0.78(best:0.78)\n",
      "F1:[0.02, 0.88](best:0.02 , 0.88)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.52(best:0.52)\n",
      "Training Root// accuracy:0.5(best:0.5)\n",
      "Training F1:[0.5, 0.67](best:0.5 , 0.67)\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "Loss =  tensor([31.9350])\n",
      "Loss =  tensor([30.9949])\n",
      "Loss =  tensor([31.4554])\n",
      "Loss =  tensor([32.5858])\n",
      "Loss =  tensor([28.8906])\n",
      "Loss =  tensor([30.3444])\n",
      "Loss =  tensor([30.4793])\n",
      "Loss =  tensor([30.1810])\n",
      "Loss =  tensor([30.6528])\n",
      "Loss =  tensor([30.8606])\n",
      "Loss =  tensor([30.0837])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:21\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.76(best:0.77)\n",
      "Validation Root accuracy:0.75(best:0.78)\n",
      "F1:[0.29, 0.88](best:0.29 , 0.88)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.52(best:0.52)\n",
      "Training Root// accuracy:0.5(best:0.5)\n",
      "Training F1:[0.67, 0.67](best:0.67 , 0.67)\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Loss =  tensor([29.7305])\n",
      "Loss =  tensor([32.2203])\n",
      "Loss =  tensor([29.7224])\n",
      "Loss =  tensor([31.7084])\n",
      "Loss =  tensor([30.0188])\n",
      "Loss =  tensor([29.1022])\n",
      "Loss =  tensor([30.3519])\n",
      "Loss =  tensor([32.6170])\n",
      "Loss =  tensor([30.8478])\n",
      "Loss =  tensor([29.8499])\n",
      "Loss =  tensor([30.7843])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:13\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.73(best:0.77)\n",
      "Validation Root accuracy:0.77(best:0.78)\n",
      "F1:[0.17, 0.88](best:0.29 , 0.88)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.52(best:0.52)\n",
      "Training Root// accuracy:0.5(best:0.5)\n",
      "Training F1:[0.64, 0.67](best:0.67 , 0.67)\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "Loss =  tensor([29.5560])\n",
      "Loss =  tensor([30.4759])\n",
      "Loss =  tensor([29.9455])\n",
      "Loss =  tensor([31.1617])\n",
      "Loss =  tensor([31.9543])\n",
      "Loss =  tensor([31.4605])\n",
      "Loss =  tensor([31.5425])\n",
      "Loss =  tensor([31.7611])\n",
      "Loss =  tensor([30.4352])\n",
      "Loss =  tensor([29.2843])\n",
      "Loss =  tensor([29.3024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:16\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.76(best:0.77)\n",
      "Validation Root accuracy:0.78(best:0.78)\n",
      "F1:[0.5, 0.88](best:0.5 , 0.88)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.52(best:0.52)\n",
      "Training Root// accuracy:0.5(best:0.5)\n",
      "Training F1:[0.71, 0.67](best:0.71 , 0.67)\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "Loss =  tensor([30.2881])\n",
      "Loss =  tensor([29.8627])\n",
      "Loss =  tensor([30.6136])\n",
      "Loss =  tensor([31.9581])\n",
      "Loss =  tensor([30.1866])\n",
      "Loss =  tensor([32.4494])\n",
      "Loss =  tensor([31.9281])\n",
      "Loss =  tensor([29.7600])\n",
      "Loss =  tensor([29.1035])\n",
      "Loss =  tensor([31.5511])\n",
      "Loss =  tensor([29.9515])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:15\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.73(best:0.77)\n",
      "Validation Root accuracy:0.78(best:0.78)\n",
      "F1:[0.5, 0.88](best:0.5 , 0.88)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.53(best:0.53)\n",
      "Training Root// accuracy:0.5(best:0.5)\n",
      "Training F1:[0.86, 0.67](best:0.86 , 0.67)\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "Loss =  tensor([30.5620])\n",
      "Loss =  tensor([31.0664])\n",
      "Loss =  tensor([30.6819])\n",
      "Loss =  tensor([30.7774])\n",
      "Loss =  tensor([31.7701])\n",
      "Loss =  tensor([28.6111])\n",
      "Loss =  tensor([30.5603])\n",
      "Loss =  tensor([30.9353])\n",
      "Loss =  tensor([30.7574])\n",
      "Loss =  tensor([30.7177])\n",
      "Loss =  tensor([28.8924])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:19\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.7(best:0.77)\n",
      "Validation Root accuracy:0.69(best:0.78)\n",
      "F1:[0.38, 0.88](best:0.5 , 0.88)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.54(best:0.54)\n",
      "Training Root// accuracy:0.55(best:0.55)\n",
      "Training F1:[0.78, 0.69](best:0.86 , 0.69)\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "Loss =  tensor([32.1426])\n",
      "Loss =  tensor([29.8614])\n",
      "Loss =  tensor([30.1872])\n",
      "Loss =  tensor([29.0858])\n",
      "Loss =  tensor([30.0221])\n",
      "Loss =  tensor([31.5048])\n",
      "Loss =  tensor([29.8558])\n",
      "Loss =  tensor([30.1282])\n",
      "Loss =  tensor([30.6387])\n",
      "Loss =  tensor([29.8283])\n",
      "Loss =  tensor([30.8246])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:17\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.7(best:0.77)\n",
      "Validation Root accuracy:0.65(best:0.78)\n",
      "F1:[0.3, 0.87](best:0.5 , 0.88)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:01:02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.54(best:0.54)\n",
      "Training Root// accuracy:0.58(best:0.58)\n",
      "Training F1:[0.81, 0.71](best:0.86 , 0.71)\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "Loss =  tensor([31.4577])\n",
      "Loss =  tensor([31.0338])\n",
      "Loss =  tensor([31.6756])\n",
      "Loss =  tensor([30.2853])\n",
      "Loss =  tensor([30.9594])\n",
      "Loss =  tensor([29.6665])\n",
      "Loss =  tensor([30.7992])\n",
      "Loss =  tensor([31.0656])\n",
      "Loss =  tensor([31.8117])\n",
      "Loss =  tensor([29.8664])\n",
      "Loss =  tensor([29.3712])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:14\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.58(best:0.77)\n",
      "Validation Root accuracy:0.58(best:0.78)\n",
      "F1:[0.37, 0.88](best:0.5 , 0.88)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.55(best:0.55)\n",
      "Training Root// accuracy:0.66(best:0.66)\n",
      "Training F1:[0.82, 0.78](best:0.86 , 0.78)\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "Loss =  tensor([29.9110])\n",
      "Loss =  tensor([30.8053])\n",
      "Loss =  tensor([30.6434])\n",
      "Loss =  tensor([30.5690])\n",
      "Loss =  tensor([29.9043])\n",
      "Loss =  tensor([30.8628])\n",
      "Loss =  tensor([31.1178])\n",
      "Loss =  tensor([32.9930])\n",
      "Loss =  tensor([30.6801])\n",
      "Loss =  tensor([30.8235])\n",
      "Loss =  tensor([28.6539])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:12\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.7(best:0.77)\n",
      "Validation Root accuracy:0.58(best:0.78)\n",
      "F1:[0.37, 0.88](best:0.5 , 0.88)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.56(best:0.56)\n",
      "Training Root// accuracy:0.66(best:0.66)\n",
      "Training F1:[0.82, 0.78](best:0.86 , 0.78)\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "Loss =  tensor([29.7052])\n",
      "Loss =  tensor([31.3552])\n",
      "Loss =  tensor([30.8522])\n",
      "Loss =  tensor([29.7179])\n",
      "Loss =  tensor([29.2003])\n",
      "Loss =  tensor([30.5318])\n",
      "Loss =  tensor([31.7859])\n",
      "Loss =  tensor([31.5012])\n",
      "Loss =  tensor([29.2576])\n",
      "Loss =  tensor([30.1768])\n",
      "Loss =  tensor([29.5134])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:12\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.52(best:0.77)\n",
      "Validation Root accuracy:0.75(best:0.78)\n",
      "F1:[0.32, 0.88](best:0.5 , 0.88)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.54(best:0.56)\n",
      "Training Root// accuracy:0.53(best:0.66)\n",
      "Training F1:[0.87, 0.68](best:0.87 , 0.78)\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "Loss =  tensor([30.5054])\n",
      "Loss =  tensor([30.3131])\n",
      "Loss =  tensor([30.4620])\n",
      "Loss =  tensor([30.8231])\n",
      "Loss =  tensor([31.5782])\n",
      "Loss =  tensor([34.7108])\n",
      "Loss =  tensor([29.6426])\n",
      "Loss =  tensor([28.6987])\n",
      "Loss =  tensor([31.2402])\n",
      "Loss =  tensor([32.6767])\n",
      "Loss =  tensor([30.1637])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:12\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.67(best:0.77)\n",
      "Validation Root accuracy:0.28(best:0.78)\n",
      "F1:[0.35, 0.86](best:0.5 , 0.88)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.54(best:0.56)\n",
      "Training Root// accuracy:0.57(best:0.66)\n",
      "Training F1:[0.7, 0.89](best:0.87 , 0.89)\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "Loss =  tensor([29.5104])\n",
      "Loss =  tensor([32.7939])\n",
      "Loss =  tensor([26.9568])\n",
      "Loss =  tensor([31.2444])\n",
      "Loss =  tensor([30.4863])\n",
      "Loss =  tensor([31.2644])\n",
      "Loss =  tensor([30.7596])\n",
      "Loss =  tensor([31.2234])\n",
      "Loss =  tensor([33.9981])\n",
      "Loss =  tensor([29.3214])\n",
      "Loss =  tensor([33.1732])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:13\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.44(best:0.77)\n",
      "Validation Root accuracy:0.62(best:0.78)\n",
      "F1:[0.32, 0.87](best:0.5 , 0.88)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.55(best:0.56)\n",
      "Training Root// accuracy:0.62(best:0.66)\n",
      "Training F1:[0.82, 0.74](best:0.87 , 0.89)\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "Loss =  tensor([30.2230])\n",
      "Loss =  tensor([30.1532])\n",
      "Loss =  tensor([30.2865])\n",
      "Loss =  tensor([29.7318])\n",
      "Loss =  tensor([29.1873])\n",
      "Loss =  tensor([29.8660])\n",
      "Loss =  tensor([31.6002])\n",
      "Loss =  tensor([31.8430])\n",
      "Loss =  tensor([29.6279])\n",
      "Loss =  tensor([30.7128])\n",
      "Loss =  tensor([29.6423])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:11\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.75(best:0.77)\n",
      "Validation Root accuracy:0.65(best:0.78)\n",
      "F1:[0.38, 0.88](best:0.5 , 0.88)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.55(best:0.56)\n",
      "Training Root// accuracy:0.65(best:0.66)\n",
      "Training F1:[0.85, 0.75](best:0.87 , 0.89)\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "Loss =  tensor([29.2647])\n",
      "Loss =  tensor([29.1707])\n",
      "Loss =  tensor([28.9495])\n",
      "Loss =  tensor([30.4761])\n",
      "Loss =  tensor([29.4502])\n",
      "Loss =  tensor([30.0390])\n",
      "Loss =  tensor([31.2063])\n",
      "Loss =  tensor([31.6778])\n",
      "Loss =  tensor([30.3669])\n",
      "Loss =  tensor([31.8590])\n",
      "Loss =  tensor([29.5846])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:16\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.43(best:0.77)\n",
      "Validation Root accuracy:0.64(best:0.78)\n",
      "F1:[0.42, 0.89](best:0.5 , 0.89)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.56(best:0.56)\n",
      "Training Root// accuracy:0.66(best:0.66)\n",
      "Training F1:[0.86, 0.77](best:0.87 , 0.89)\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "Loss =  tensor([30.2731])\n",
      "Loss =  tensor([30.7343])\n",
      "Loss =  tensor([31.1379])\n",
      "Loss =  tensor([28.6218])\n",
      "Loss =  tensor([31.9297])\n",
      "Loss =  tensor([32.0074])\n",
      "Loss =  tensor([31.2709])\n",
      "Loss =  tensor([31.8476])\n",
      "Loss =  tensor([29.6815])\n",
      "Loss =  tensor([29.5697])\n",
      "Loss =  tensor([28.6328])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:11\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.7(best:0.77)\n",
      "Validation Root accuracy:0.52(best:0.78)\n",
      "F1:[0.4, 0.9](best:0.5 , 0.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.57(best:0.57)\n",
      "Training Root// accuracy:0.67(best:0.67)\n",
      "Training F1:[0.79, 0.82](best:0.87 , 0.89)\n",
      "\n",
      "\n",
      "Epoch 15\n",
      "Loss =  tensor([28.8783])\n",
      "Loss =  tensor([30.3527])\n",
      "Loss =  tensor([32.9141])\n",
      "Loss =  tensor([32.2846])\n",
      "Loss =  tensor([29.8383])\n",
      "Loss =  tensor([28.8091])\n",
      "Loss =  tensor([30.8561])\n",
      "Loss =  tensor([31.5244])\n",
      "Loss =  tensor([30.6570])\n",
      "Loss =  tensor([31.8025])\n",
      "Loss =  tensor([29.6714])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:11\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.61(best:0.77)\n",
      "Validation Root accuracy:0.22(best:0.78)\n",
      "F1:[0.36, 1.0](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.53(best:0.57)\n",
      "Training Root// accuracy:0.5(best:0.67)\n",
      "Training F1:[0.67, 1.0](best:0.87 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 16\n",
      "Loss =  tensor([30.3584])\n",
      "Loss =  tensor([30.5171])\n",
      "Loss =  tensor([30.7728])\n",
      "Loss =  tensor([29.7750])\n",
      "Loss =  tensor([33.4588])\n",
      "Loss =  tensor([31.1048])\n",
      "Loss =  tensor([29.4070])\n",
      "Loss =  tensor([29.8041])\n",
      "Loss =  tensor([30.1006])\n",
      "Loss =  tensor([30.3233])\n",
      "Loss =  tensor([30.3393])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:20\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.56(best:0.77)\n",
      "Validation Root accuracy:0.58(best:0.78)\n",
      "F1:[0.38, 0.89](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.55(best:0.57)\n",
      "Training Root// accuracy:0.62(best:0.67)\n",
      "Training F1:[0.77, 0.75](best:0.87 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "Loss =  tensor([28.9747])\n",
      "Loss =  tensor([30.3972])\n",
      "Loss =  tensor([30.5012])\n",
      "Loss =  tensor([29.7775])\n",
      "Loss =  tensor([31.6280])\n",
      "Loss =  tensor([29.0983])\n",
      "Loss =  tensor([29.4208])\n",
      "Loss =  tensor([29.3827])\n",
      "Loss =  tensor([30.2267])\n",
      "Loss =  tensor([31.5100])\n",
      "Loss =  tensor([29.9569])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:15\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.58(best:0.77)\n",
      "Validation Root accuracy:0.62(best:0.78)\n",
      "F1:[0.36, 0.88](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.58(best:0.58)\n",
      "Training Root// accuracy:0.67(best:0.67)\n",
      "Training F1:[0.85, 0.78](best:0.87 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 18\n",
      "Loss =  tensor([29.7583])\n",
      "Loss =  tensor([29.4827])\n",
      "Loss =  tensor([31.4880])\n",
      "Loss =  tensor([30.7047])\n",
      "Loss =  tensor([29.6110])\n",
      "Loss =  tensor([30.1158])\n",
      "Loss =  tensor([30.8015])\n",
      "Loss =  tensor([30.7423])\n",
      "Loss =  tensor([30.4121])\n",
      "Loss =  tensor([31.0433])\n",
      "Loss =  tensor([31.8205])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:20\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.43(best:0.77)\n",
      "Validation Root accuracy:0.52(best:0.78)\n",
      "F1:[0.4, 0.9](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.57(best:0.58)\n",
      "Training Root// accuracy:0.7(best:0.7)\n",
      "Training F1:[0.81, 0.84](best:0.87 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 19\n",
      "Loss =  tensor([29.6708])\n",
      "Loss =  tensor([29.3993])\n",
      "Loss =  tensor([31.0577])\n",
      "Loss =  tensor([29.3128])\n",
      "Loss =  tensor([28.3051])\n",
      "Loss =  tensor([29.9932])\n",
      "Loss =  tensor([30.4526])\n",
      "Loss =  tensor([29.4514])\n",
      "Loss =  tensor([29.7081])\n",
      "Loss =  tensor([32.6826])\n",
      "Loss =  tensor([29.3425])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:15\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.68(best:0.77)\n",
      "Validation Root accuracy:0.49(best:0.78)\n",
      "F1:[0.4, 0.91](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.58(best:0.58)\n",
      "Training Root// accuracy:0.71(best:0.71)\n",
      "Training F1:[0.8, 0.86](best:0.87 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 20\n",
      "Loss =  tensor([27.6453])\n",
      "Loss =  tensor([28.9557])\n",
      "Loss =  tensor([30.8848])\n",
      "Loss =  tensor([32.2997])\n",
      "Loss =  tensor([30.4899])\n",
      "Loss =  tensor([29.6553])\n",
      "Loss =  tensor([29.6922])\n",
      "Loss =  tensor([30.5472])\n",
      "Loss =  tensor([31.9622])\n",
      "Loss =  tensor([29.9269])\n",
      "Loss =  tensor([29.6387])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:18\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.73(best:0.77)\n",
      "Validation Root accuracy:0.64(best:0.78)\n",
      "F1:[0.39, 0.88](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.58(best:0.58)\n",
      "Training Root// accuracy:0.7(best:0.71)\n",
      "Training F1:[0.88, 0.79](best:0.88 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 21\n",
      "Loss =  tensor([28.2493])\n",
      "Loss =  tensor([29.2964])\n",
      "Loss =  tensor([31.2578])\n",
      "Loss =  tensor([30.1789])\n",
      "Loss =  tensor([29.5340])\n",
      "Loss =  tensor([29.8174])\n",
      "Loss =  tensor([31.3480])\n",
      "Loss =  tensor([29.5640])\n",
      "Loss =  tensor([29.3259])\n",
      "Loss =  tensor([28.9706])\n",
      "Loss =  tensor([30.4097])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:15\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.44(best:0.77)\n",
      "Validation Root accuracy:0.57(best:0.78)\n",
      "F1:[0.42, 0.9](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.57(best:0.58)\n",
      "Training Root// accuracy:0.72(best:0.72)\n",
      "Training F1:[0.84, 0.83](best:0.88 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 22\n",
      "Loss =  tensor([29.3024])\n",
      "Loss =  tensor([29.0825])\n",
      "Loss =  tensor([29.7413])\n",
      "Loss =  tensor([29.9919])\n",
      "Loss =  tensor([28.6828])\n",
      "Loss =  tensor([29.3832])\n",
      "Loss =  tensor([29.9161])\n",
      "Loss =  tensor([30.7830])\n",
      "Loss =  tensor([27.6140])\n",
      "Loss =  tensor([31.5166])\n",
      "Loss =  tensor([30.2131])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:21\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.6(best:0.77)\n",
      "Validation Root accuracy:0.43(best:0.78)\n",
      "F1:[0.38, 0.9](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.58(best:0.58)\n",
      "Training Root// accuracy:0.7(best:0.72)\n",
      "Training F1:[0.79, 0.89](best:0.88 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 23\n",
      "Loss =  tensor([30.4408])\n",
      "Loss =  tensor([28.7382])\n",
      "Loss =  tensor([28.5660])\n",
      "Loss =  tensor([30.3211])\n",
      "Loss =  tensor([29.9266])\n",
      "Loss =  tensor([29.1611])\n",
      "Loss =  tensor([29.8557])\n",
      "Loss =  tensor([27.4860])\n",
      "Loss =  tensor([29.6051])\n",
      "Loss =  tensor([29.2019])\n",
      "Loss =  tensor([29.1596])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:18\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.68(best:0.77)\n",
      "Validation Root accuracy:0.63(best:0.78)\n",
      "F1:[0.43, 0.9](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.6(best:0.6)\n",
      "Training Root// accuracy:0.76(best:0.76)\n",
      "Training F1:[0.88, 0.84](best:0.88 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 24\n",
      "Loss =  tensor([28.0574])\n",
      "Loss =  tensor([31.2826])\n",
      "Loss =  tensor([29.4036])\n",
      "Loss =  tensor([30.8378])\n",
      "Loss =  tensor([30.8319])\n",
      "Loss =  tensor([29.2779])\n",
      "Loss =  tensor([32.4795])\n",
      "Loss =  tensor([29.7350])\n",
      "Loss =  tensor([29.5115])\n",
      "Loss =  tensor([28.6686])\n",
      "Loss =  tensor([29.4324])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:19\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.58(best:0.77)\n",
      "Validation Root accuracy:0.53(best:0.78)\n",
      "F1:[0.4, 0.9](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.6(best:0.6)\n",
      "Training Root// accuracy:0.74(best:0.76)\n",
      "Training F1:[0.83, 0.88](best:0.88 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 25\n",
      "Loss =  tensor([26.3801])\n",
      "Loss =  tensor([30.3043])\n",
      "Loss =  tensor([31.8577])\n",
      "Loss =  tensor([29.6118])\n",
      "Loss =  tensor([27.5845])\n",
      "Loss =  tensor([31.8146])\n",
      "Loss =  tensor([29.0965])\n",
      "Loss =  tensor([32.1796])\n",
      "Loss =  tensor([31.0739])\n",
      "Loss =  tensor([32.2522])\n",
      "Loss =  tensor([33.0077])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:17\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.41(best:0.77)\n",
      "Validation Root accuracy:0.44(best:0.78)\n",
      "F1:[0.38, 0.9](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.58(best:0.6)\n",
      "Training Root// accuracy:0.73(best:0.76)\n",
      "Training F1:[0.81, 0.9](best:0.88 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 26\n",
      "Loss =  tensor([28.7157])\n",
      "Loss =  tensor([27.4883])\n",
      "Loss =  tensor([31.3935])\n",
      "Loss =  tensor([29.4358])\n",
      "Loss =  tensor([31.6555])\n",
      "Loss =  tensor([29.0328])\n",
      "Loss =  tensor([28.5141])\n",
      "Loss =  tensor([29.7313])\n",
      "Loss =  tensor([31.9095])\n",
      "Loss =  tensor([30.3579])\n",
      "Loss =  tensor([29.3754])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:22\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.63(best:0.77)\n",
      "Validation Root accuracy:0.67(best:0.78)\n",
      "F1:[0.43, 0.89](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.59(best:0.6)\n",
      "Training Root// accuracy:0.73(best:0.76)\n",
      "Training F1:[0.9, 0.81](best:0.9 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 27\n",
      "Loss =  tensor([28.4860])\n",
      "Loss =  tensor([28.7370])\n",
      "Loss =  tensor([33.3684])\n",
      "Loss =  tensor([30.5610])\n",
      "Loss =  tensor([30.7144])\n",
      "Loss =  tensor([28.5779])\n",
      "Loss =  tensor([31.8043])\n",
      "Loss =  tensor([31.8016])\n",
      "Loss =  tensor([29.1398])\n",
      "Loss =  tensor([30.1255])\n",
      "Loss =  tensor([30.8942])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:20\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.57(best:0.77)\n",
      "Validation Root accuracy:0.45(best:0.78)\n",
      "F1:[0.36, 0.88](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.59(best:0.6)\n",
      "Training Root// accuracy:0.71(best:0.76)\n",
      "Training F1:[0.8, 0.86](best:0.9 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 28\n",
      "Loss =  tensor([28.1749])\n",
      "Loss =  tensor([30.1172])\n",
      "Loss =  tensor([31.2338])\n",
      "Loss =  tensor([29.9892])\n",
      "Loss =  tensor([30.8931])\n",
      "Loss =  tensor([28.7016])\n",
      "Loss =  tensor([31.2012])\n",
      "Loss =  tensor([32.3561])\n",
      "Loss =  tensor([31.9238])\n",
      "Loss =  tensor([31.7006])\n",
      "Loss =  tensor([29.1660])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:23\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.68(best:0.77)\n",
      "Validation Root accuracy:0.47(best:0.78)\n",
      "F1:[0.37, 0.89](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.58(best:0.6)\n",
      "Training Root// accuracy:0.7(best:0.76)\n",
      "Training F1:[0.8, 0.86](best:0.9 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 29\n",
      "Loss =  tensor([28.9284])\n",
      "Loss =  tensor([29.9569])\n",
      "Loss =  tensor([29.4385])\n",
      "Loss =  tensor([28.9335])\n",
      "Loss =  tensor([30.0752])\n",
      "Loss =  tensor([27.5865])\n",
      "Loss =  tensor([30.0841])\n",
      "Loss =  tensor([29.9906])\n",
      "Loss =  tensor([29.0953])\n",
      "Loss =  tensor([29.3491])\n",
      "Loss =  tensor([30.6286])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:21\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.58(best:0.77)\n",
      "Validation Root accuracy:0.46(best:0.78)\n",
      "F1:[0.34, 0.87](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.6(best:0.6)\n",
      "Training Root// accuracy:0.75(best:0.76)\n",
      "Training F1:[0.83, 0.88](best:0.9 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 30\n",
      "Loss =  tensor([28.9441])\n",
      "Loss =  tensor([29.1358])\n",
      "Loss =  tensor([28.4401])\n",
      "Loss =  tensor([27.6652])\n",
      "Loss =  tensor([28.3792])\n",
      "Loss =  tensor([28.9831])\n",
      "Loss =  tensor([31.4927])\n",
      "Loss =  tensor([29.8336])\n",
      "Loss =  tensor([29.6033])\n",
      "Loss =  tensor([29.1369])\n",
      "Loss =  tensor([30.6922])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:19\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.61(best:0.77)\n",
      "Validation Root accuracy:0.59(best:0.78)\n",
      "F1:[0.41, 0.89](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.6(best:0.6)\n",
      "Training Root// accuracy:0.77(best:0.77)\n",
      "Training F1:[0.89, 0.86](best:0.9 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 31\n",
      "Loss =  tensor([29.1540])\n",
      "Loss =  tensor([28.0691])\n",
      "Loss =  tensor([27.7170])\n",
      "Loss =  tensor([30.2189])\n",
      "Loss =  tensor([29.6613])\n",
      "Loss =  tensor([30.5407])\n",
      "Loss =  tensor([29.3797])\n",
      "Loss =  tensor([28.1049])\n",
      "Loss =  tensor([30.7423])\n",
      "Loss =  tensor([32.1855])\n",
      "Loss =  tensor([27.9821])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:16\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.66(best:0.77)\n",
      "Validation Root accuracy:0.49(best:0.78)\n",
      "F1:[0.41, 0.91](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.6(best:0.6)\n",
      "Training Root// accuracy:0.76(best:0.77)\n",
      "Training F1:[0.84, 0.9](best:0.9 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 32\n",
      "Loss =  tensor([29.7569])\n",
      "Loss =  tensor([27.3488])\n",
      "Loss =  tensor([28.6101])\n",
      "Loss =  tensor([30.1521])\n",
      "Loss =  tensor([28.6746])\n",
      "Loss =  tensor([27.6200])\n",
      "Loss =  tensor([29.9742])\n",
      "Loss =  tensor([29.2358])\n",
      "Loss =  tensor([26.9532])\n",
      "Loss =  tensor([30.4245])\n",
      "Loss =  tensor([30.0829])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:12\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.6(best:0.77)\n",
      "Validation Root accuracy:0.55(best:0.78)\n",
      "F1:[0.39, 0.89](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.61(best:0.61)\n",
      "Training Root// accuracy:0.79(best:0.79)\n",
      "Training F1:[0.88, 0.88](best:0.9 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 33\n",
      "Loss =  tensor([30.3684])\n",
      "Loss =  tensor([29.3159])\n",
      "Loss =  tensor([27.9392])\n",
      "Loss =  tensor([27.5888])\n",
      "Loss =  tensor([29.1212])\n",
      "Loss =  tensor([27.8943])\n",
      "Loss =  tensor([29.3892])\n",
      "Loss =  tensor([28.6021])\n",
      "Loss =  tensor([28.7735])\n",
      "Loss =  tensor([29.1432])\n",
      "Loss =  tensor([26.9871])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:19\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.6(best:0.77)\n",
      "Validation Root accuracy:0.6(best:0.78)\n",
      "F1:[0.4, 0.89](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.61(best:0.61)\n",
      "Training Root// accuracy:0.81(best:0.81)\n",
      "Training F1:[0.91, 0.88](best:0.91 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 34\n",
      "Loss =  tensor([28.7655])\n",
      "Loss =  tensor([28.2295])\n",
      "Loss =  tensor([27.9324])\n",
      "Loss =  tensor([28.0886])\n",
      "Loss =  tensor([31.6698])\n",
      "Loss =  tensor([29.4936])\n",
      "Loss =  tensor([27.8284])\n",
      "Loss =  tensor([29.6566])\n",
      "Loss =  tensor([31.0566])\n",
      "Loss =  tensor([29.3976])\n",
      "Loss =  tensor([28.5533])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:13\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.59(best:0.77)\n",
      "Validation Root accuracy:0.53(best:0.78)\n",
      "F1:[0.4, 0.9](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.62(best:0.62)\n",
      "Training Root// accuracy:0.81(best:0.81)\n",
      "Training F1:[0.88, 0.91](best:0.91 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 35\n",
      "Loss =  tensor([27.5351])\n",
      "Loss =  tensor([31.4009])\n",
      "Loss =  tensor([32.3488])\n",
      "Loss =  tensor([36.2720])\n",
      "Loss =  tensor([32.1951])\n",
      "Loss =  tensor([31.4669])\n",
      "Loss =  tensor([31.8388])\n",
      "Loss =  tensor([29.2200])\n",
      "Loss =  tensor([32.2558])\n",
      "Loss =  tensor([35.6354])\n",
      "Loss =  tensor([34.2020])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:11\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.45(best:0.77)\n",
      "Validation Root accuracy:0.74(best:0.78)\n",
      "F1:[0.46, 0.88](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.56(best:0.62)\n",
      "Training Root// accuracy:0.65(best:0.81)\n",
      "Training F1:[0.96, 0.74](best:0.96 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 36\n",
      "Loss =  tensor([30.3047])\n",
      "Loss =  tensor([29.2080])\n",
      "Loss =  tensor([28.2432])\n",
      "Loss =  tensor([29.9271])\n",
      "Loss =  tensor([28.7343])\n",
      "Loss =  tensor([28.6055])\n",
      "Loss =  tensor([28.4247])\n",
      "Loss =  tensor([27.7521])\n",
      "Loss =  tensor([29.5405])\n",
      "Loss =  tensor([30.8476])\n",
      "Loss =  tensor([30.6322])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:13\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.53(best:0.77)\n",
      "Validation Root accuracy:0.48(best:0.78)\n",
      "F1:[0.4, 0.91](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.61(best:0.62)\n",
      "Training Root// accuracy:0.77(best:0.81)\n",
      "Training F1:[0.84, 0.92](best:0.96 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 37\n",
      "Loss =  tensor([29.5662])\n",
      "Loss =  tensor([27.8583])\n",
      "Loss =  tensor([29.5910])\n",
      "Loss =  tensor([28.1655])\n",
      "Loss =  tensor([28.4261])\n",
      "Loss =  tensor([29.3384])\n",
      "Loss =  tensor([27.6623])\n",
      "Loss =  tensor([27.8469])\n",
      "Loss =  tensor([27.6287])\n",
      "Loss =  tensor([31.0348])\n",
      "Loss =  tensor([30.8667])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:14\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.69(best:0.77)\n",
      "Validation Root accuracy:0.51(best:0.78)\n",
      "F1:[0.39, 0.89](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.61(best:0.62)\n",
      "Training Root// accuracy:0.8(best:0.81)\n",
      "Training F1:[0.87, 0.9](best:0.96 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 38\n",
      "Loss =  tensor([27.8714])\n",
      "Loss =  tensor([29.9594])\n",
      "Loss =  tensor([26.8772])\n",
      "Loss =  tensor([27.3848])\n",
      "Loss =  tensor([29.7803])\n",
      "Loss =  tensor([28.1521])\n",
      "Loss =  tensor([32.8900])\n",
      "Loss =  tensor([32.0825])\n",
      "Loss =  tensor([30.2403])\n",
      "Loss =  tensor([28.7894])\n",
      "Loss =  tensor([29.4988])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:33:45\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.41(best:0.77)\n",
      "Validation Root accuracy:0.54(best:0.78)\n",
      "F1:[0.41, 0.9](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:13:23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.6(best:0.62)\n",
      "Training Root// accuracy:0.78(best:0.81)\n",
      "Training F1:[0.87, 0.89](best:0.96 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 39\n",
      "Loss =  tensor([30.8827])\n",
      "Loss =  tensor([30.0037])\n",
      "Loss =  tensor([28.3955])\n",
      "Loss =  tensor([29.4984])\n",
      "Loss =  tensor([28.8689])\n",
      "Loss =  tensor([29.3626])\n",
      "Loss =  tensor([29.1488])\n",
      "Loss =  tensor([29.0303])\n",
      "Loss =  tensor([29.2025])\n",
      "Loss =  tensor([26.9314])\n",
      "Loss =  tensor([28.4830])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:10\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.68(best:0.77)\n",
      "Validation Root accuracy:0.5(best:0.78)\n",
      "F1:[0.37, 0.88](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.61(best:0.62)\n",
      "Training Root// accuracy:0.78(best:0.81)\n",
      "Training F1:[0.86, 0.9](best:0.96 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 40\n",
      "Loss =  tensor([27.1926])\n",
      "Loss =  tensor([28.2396])\n",
      "Loss =  tensor([27.5596])\n",
      "Loss =  tensor([28.8427])\n",
      "Loss =  tensor([29.6868])\n",
      "Loss =  tensor([29.3396])\n",
      "Loss =  tensor([32.6641])\n",
      "Loss =  tensor([29.1978])\n",
      "Loss =  tensor([29.1787])\n",
      "Loss =  tensor([28.5998])\n",
      "Loss =  tensor([30.0717])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:16\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.5(best:0.77)\n",
      "Validation Root accuracy:0.61(best:0.78)\n",
      "F1:[0.41, 0.89](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.6(best:0.62)\n",
      "Training Root// accuracy:0.77(best:0.81)\n",
      "Training F1:[0.91, 0.84](best:0.96 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 41\n",
      "Loss =  tensor([29.7735])\n",
      "Loss =  tensor([27.4031])\n",
      "Loss =  tensor([30.2164])\n",
      "Loss =  tensor([27.4870])\n",
      "Loss =  tensor([27.6631])\n",
      "Loss =  tensor([27.6486])\n",
      "Loss =  tensor([28.0017])\n",
      "Loss =  tensor([28.9390])\n",
      "Loss =  tensor([27.6674])\n",
      "Loss =  tensor([28.5983])\n",
      "Loss =  tensor([28.0370])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:19\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.59(best:0.77)\n",
      "Validation Root accuracy:0.55(best:0.78)\n",
      "F1:[0.4, 0.9](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.62(best:0.62)\n",
      "Training Root// accuracy:0.84(best:0.84)\n",
      "Training F1:[0.9, 0.93](best:0.96 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 42\n",
      "Loss =  tensor([29.6547])\n",
      "Loss =  tensor([30.5913])\n",
      "Loss =  tensor([30.5081])\n",
      "Loss =  tensor([30.7504])\n",
      "Loss =  tensor([27.8708])\n",
      "Loss =  tensor([29.1257])\n",
      "Loss =  tensor([30.9981])\n",
      "Loss =  tensor([30.7866])\n",
      "Loss =  tensor([28.4791])\n",
      "Loss =  tensor([26.9329])\n",
      "Loss =  tensor([30.4629])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:14\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.57(best:0.77)\n",
      "Validation Root accuracy:0.65(best:0.78)\n",
      "F1:[0.46, 0.9](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.61(best:0.62)\n",
      "Training Root// accuracy:0.81(best:0.84)\n",
      "Training F1:[0.93, 0.86](best:0.96 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 43\n",
      "Loss =  tensor([29.1785])\n",
      "Loss =  tensor([28.4971])\n",
      "Loss =  tensor([27.5059])\n",
      "Loss =  tensor([28.8906])\n",
      "Loss =  tensor([27.4105])\n",
      "Loss =  tensor([28.1059])\n",
      "Loss =  tensor([27.3996])\n",
      "Loss =  tensor([28.4359])\n",
      "Loss =  tensor([27.4336])\n",
      "Loss =  tensor([27.6583])\n",
      "Loss =  tensor([27.0662])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:09\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.58(best:0.77)\n",
      "Validation Root accuracy:0.58(best:0.78)\n",
      "F1:[0.41, 0.9](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.63(best:0.63)\n",
      "Training Root// accuracy:0.84(best:0.84)\n",
      "Training F1:[0.91, 0.92](best:0.96 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 44\n",
      "Loss =  tensor([26.9083])\n",
      "Loss =  tensor([27.8980])\n",
      "Loss =  tensor([26.0340])\n",
      "Loss =  tensor([28.4347])\n",
      "Loss =  tensor([28.4556])\n",
      "Loss =  tensor([27.9943])\n",
      "Loss =  tensor([30.0511])\n",
      "Loss =  tensor([28.9104])\n",
      "Loss =  tensor([28.2295])\n",
      "Loss =  tensor([27.3328])\n",
      "Loss =  tensor([26.7105])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:09\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.68(best:0.77)\n",
      "Validation Root accuracy:0.6(best:0.78)\n",
      "F1:[0.43, 0.9](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.63(best:0.63)\n",
      "Training Root// accuracy:0.86(best:0.86)\n",
      "Training F1:[0.93, 0.92](best:0.96 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 45\n",
      "Loss =  tensor([27.6980])\n",
      "Loss =  tensor([27.4828])\n",
      "Loss =  tensor([27.5667])\n",
      "Loss =  tensor([29.7414])\n",
      "Loss =  tensor([26.2411])\n",
      "Loss =  tensor([28.1258])\n",
      "Loss =  tensor([27.9198])\n",
      "Loss =  tensor([27.4428])\n",
      "Loss =  tensor([28.0690])\n",
      "Loss =  tensor([28.4525])\n",
      "Loss =  tensor([30.4255])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:10\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.52(best:0.77)\n",
      "Validation Root accuracy:0.53(best:0.78)\n",
      "F1:[0.4, 0.9](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.63(best:0.63)\n",
      "Training Root// accuracy:0.87(best:0.87)\n",
      "Training F1:[0.92, 0.94](best:0.96 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 46\n",
      "Loss =  tensor([27.9371])\n",
      "Loss =  tensor([28.3736])\n",
      "Loss =  tensor([28.4659])\n",
      "Loss =  tensor([29.1423])\n",
      "Loss =  tensor([28.0468])\n",
      "Loss =  tensor([28.7179])\n",
      "Loss =  tensor([28.2851])\n",
      "Loss =  tensor([29.1222])\n",
      "Loss =  tensor([28.7738])\n",
      "Loss =  tensor([27.2781])\n",
      "Loss =  tensor([28.4063])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:09\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.61(best:0.77)\n",
      "Validation Root accuracy:0.6(best:0.78)\n",
      "F1:[0.4, 0.89](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.63(best:0.63)\n",
      "Training Root// accuracy:0.86(best:0.87)\n",
      "Training F1:[0.93, 0.91](best:0.96 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 47\n",
      "Loss =  tensor([27.2105])\n",
      "Loss =  tensor([27.3292])\n",
      "Loss =  tensor([26.5560])\n",
      "Loss =  tensor([26.8812])\n",
      "Loss =  tensor([28.7834])\n",
      "Loss =  tensor([27.3479])\n",
      "Loss =  tensor([29.1220])\n",
      "Loss =  tensor([28.7296])\n",
      "Loss =  tensor([31.2144])\n",
      "Loss =  tensor([30.7208])\n",
      "Loss =  tensor([26.7258])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:09\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.52(best:0.77)\n",
      "Validation Root accuracy:0.55(best:0.78)\n",
      "F1:[0.41, 0.9](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.63(best:0.63)\n",
      "Training Root// accuracy:0.85(best:0.87)\n",
      "Training F1:[0.91, 0.93](best:0.96 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 48\n",
      "Loss =  tensor([28.7507])\n",
      "Loss =  tensor([27.1613])\n",
      "Loss =  tensor([28.1177])\n",
      "Loss =  tensor([26.4727])\n",
      "Loss =  tensor([28.4092])\n",
      "Loss =  tensor([28.7763])\n",
      "Loss =  tensor([28.6079])\n",
      "Loss =  tensor([26.3419])\n",
      "Loss =  tensor([26.8393])\n",
      "Loss =  tensor([29.6636])\n",
      "Loss =  tensor([28.5015])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:09\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.58(best:0.77)\n",
      "Validation Root accuracy:0.56(best:0.78)\n",
      "F1:[0.41, 0.9](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.63(best:0.63)\n",
      "Training Root// accuracy:0.86(best:0.87)\n",
      "Training F1:[0.93, 0.93](best:0.96 , 1.0)\n",
      "\n",
      "\n",
      "Epoch 49\n",
      "Loss =  tensor([27.9244])\n",
      "Loss =  tensor([28.2061])\n",
      "Loss =  tensor([27.1842])\n",
      "Loss =  tensor([27.0820])\n",
      "Loss =  tensor([27.8400])\n",
      "Loss =  tensor([25.2080])\n",
      "Loss =  tensor([28.6087])\n",
      "Loss =  tensor([25.6445])\n",
      "Loss =  tensor([27.6181])\n",
      "Loss =  tensor([28.4203])\n",
      "Loss =  tensor([27.8795])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:08\n",
      "  0% |                                                         | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation All-nodes accuracy:0.6(best:0.77)\n",
      "Validation Root accuracy:0.59(best:0.78)\n",
      "F1:[0.42, 0.9](best:0.5 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |##########################################################| Time: 0:00:22\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training All-nodes accuracy:0.64(best:0.64)\n",
      "Training Root// accuracy:0.88(best:0.88)\n",
      "Training F1:[0.94, 0.93](best:0.96 , 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, dampening=0.0)\n",
    "# bestAll=bestRoot=0.0\n",
    "global count\n",
    "count=0\n",
    "BATCH_SIZE = 128\n",
    "# optimizer = torch.optim.LBFGS(model.parameters(), lr=0.5, max_iter=10, history_size = 10)\n",
    "bestAll=bestRoot=0.0\n",
    "bestF1_0=bestF1_1=best_trn_F1_0=best_trn_F1_1=0.0\n",
    "best_trn_All = best_trn_Root = 0.0 \n",
    "for epoch in range(max_epochs):\n",
    "#     trn = get_trn()\n",
    "    print(\"\\n\\nEpoch %d\" % epoch)\n",
    "    logging.info('Epoch: '+str(epoch))\n",
    "#     pbar = progressbar.ProgressBar(widgets=widgets, maxval=len(trn)/BATCH_SIZE).start()\n",
    "    params = []\n",
    "    for i in range(0,len(trn),BATCH_SIZE):\n",
    "        count+=1\n",
    "        batch = trn[i:min(i+BATCH_SIZE,len(trn))]\n",
    "        def closure():\n",
    "            global count\n",
    "            optimizer.zero_grad()\n",
    "            _,total_loss = model.getLoss(trn[0].root)\n",
    "            for tree in batch:\n",
    "                _, loss = model.getLoss(tree.root)\n",
    "                total_loss += loss\n",
    "\n",
    "            total_loss = total_loss/len(batch)\n",
    "            #L2 reg\n",
    "            param_dict = dict()\n",
    "            for name, param in model.named_parameters():\n",
    "                param_dict[name] = param.data.clone()\n",
    "                if param.requires_grad:\n",
    "                        total_loss += 0.5*l2_reg[name]*(torch.norm(param)**2)\n",
    "            params.append(param_dict)\n",
    "            print('Loss = ',total_loss.data)\n",
    "            logging.info('Loss = '+str(total_loss.data))\n",
    "            logger.scalar_summary('loss', total_loss.data, count)\n",
    "            total_loss.backward()\n",
    "            clip_grad_norm_(model.parameters(),5,2)\n",
    "            return total_loss\n",
    "#         pbar.update(i/BATCH_SIZE)\n",
    "        optimizer.step(closure)\n",
    "\n",
    "#     pbar.finish()\n",
    "\n",
    "    avg_param = dict()\n",
    "    for name, param1 in model.named_parameters():\n",
    "            avg_param[name] = param1.data.clone()\n",
    "\n",
    "    for i in range(1,len(params)):\n",
    "        for name, param in params[i].items():\n",
    "            avg_param[name] += param.clone()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name == 'embedding.weight':\n",
    "            continue\n",
    "        param.data = avg_param[name]/len(params)\n",
    "\n",
    "    correctRoot, correctAll, f1 = model.evaluate(dev)\n",
    "    # correctRoot = model.eval_sent_lvl(dev,LR_clf)\n",
    "    if bestAll<correctAll: bestAll=correctAll\n",
    "    if bestRoot<correctRoot: bestRoot=correctRoot\n",
    "    if bestF1_0<f1[0]: bestF1_0=f1[0]\n",
    "    if bestF1_1<f1[1]: bestF1_1=f1[1]\n",
    "    print(\"\\nValidation All-nodes accuracy:\"+str(round(correctAll,2))+\"(best:\"+str(round(bestAll,2))+\")\")\n",
    "    print(\"Validation Root accuracy:\" + str(round(correctRoot,2))+\"(best:\"+str(round(bestRoot,2))+\")\")\n",
    "    print(\"F1:\"+str([round(x,2) for x in f1])+\"(best:\"+str(round(bestF1_0,2))+\" , \"+str(round(bestF1_1,2))+\")\")\n",
    "    logging.info(\"Validation All-nodes accuracy:\"+str(round(correctAll,2))+\"(best:\"+str(round(bestAll,2))+\")\")\n",
    "    logging.info(\"Validation Root accuracy:\" + str(round(correctRoot,2))+\"(best:\"+str(round(bestRoot,2))+\")\")\n",
    "    logging.info(\"F1:\"+str([round(x,2) for x in f1])+\"(best:\"+str(round(bestF1_0,2))+\" , \"+str(round(bestF1_1,2))+\")\")\n",
    "    correct_trn_Root, correct_trn_All, f1_trn = model.evaluate(trn)\n",
    "    # correctRoot = model.eval_sent_lvl(dev,LR_clf)\n",
    "    if best_trn_All<correct_trn_All: best_trn_All=correct_trn_All\n",
    "    if best_trn_Root<correct_trn_Root: best_trn_Root=correct_trn_Root\n",
    "    if best_trn_F1_0<f1_trn[0]: best_trn_F1_0=f1_trn[0]\n",
    "    if best_trn_F1_1<f1_trn[1]: best_trn_F1_1=f1_trn[1]\n",
    "    print(\"\\nTraining All-nodes accuracy:\"+str(round(correct_trn_All,2))+\"(best:\"+str(round(best_trn_All,2))+\")\")\n",
    "    print(\"Training Root// accuracy:\" + str(round(correct_trn_Root,2))+\"(best:\"+str(round(best_trn_Root,2))+\")\")\n",
    "    print(\"Training F1:\"+str([round(x,2) for x in f1_trn])+\"(best:\"+str(round(best_trn_F1_0,2))+\" , \"+str(round(best_trn_F1_1,2))+\")\")\n",
    "    logging.info(\"Training All-nodes accuracy:\"+str(round(correct_trn_All,2))+\"(best:\"+str(round(best_trn_All,2))+\")\")\n",
    "    logging.info(\"Training Root accuracy:\" + str(round(correct_trn_Root,2))+\"(best:\"+str(round(best_trn_Root,2))+\")\")\n",
    "    logging.info(\"Training F1:\"+str([round(x,2) for x in f1_trn])+\"(best:\"+str(round(best_trn_F1_0,2))+\" , \"+str(round(best_trn_F1_1,2))+\")\")\n",
    "    info = {'valid_root_acc':correctRoot, 'valid_tree_acc':correctAll, 'train_root_acc':correct_trn_Root, 'train_tree_acc':correct_trn_All}\n",
    "    for tag, value in info.items():\n",
    "        logger.scalar_summary(tag,value,epoch)\n",
    "    random.shuffle(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'economic_w2v_0ProAnti_0General_1NER_0Blackout_1Balance_1Undersample_1Fixed'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "namecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model,open(\"./models/\"+namecode+'.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
